# æ¾æçº¿è™«ç—…çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿ

<div align="center">

**çŸ¥è¯†å·¥ç¨‹ç¬¬äºŒç»„ - åŸºäºæ–‡çŒ®çš„æ¾æçº¿è™«ç—…çŸ¥è¯†å›¾è°±é¡¹ç›®**

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org)
[![Neo4j](https://img.shields.io/badge/Neo4j-4.x%20%7C%205.x-green.svg)](https://neo4j.com)
[![Version](https://img.shields.io/badge/Version-v3.0.0-brightgreen.svg)](https://github.com/Dictatora0/Neo4j-graphics-of-PWD/releases)
[![LLM](https://img.shields.io/badge/LLM-Qwen2.5--Coder--14B-orange.svg)](https://github.com/QwenLM/Qwen2.5-Coder)

**GitHub ä»“åº“**ï¼š[https://github.com/Dictatora0/Neo4j-graphics-of-PWD.git](https://github.com/Dictatora0/Neo4j-graphics-of-PWD.git)

**ğŸ“¢ æœ€æ–°ç‰ˆæœ¬**: v3.0.0 - å…¨åŠŸèƒ½é›†æˆç‰ˆæœ¬ ([æŸ¥çœ‹æ›´æ–°æ—¥å¿—](./CHANGELOG.md))

</div>

---

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®ä»æ¾æçº¿è™«ç—…ï¼ˆPine Wilt Diseaseï¼ŒPWDï¼‰ç›¸å…³ PDF æ–‡çŒ®ä¸­è‡ªåŠ¨æŠ½å–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºå¯åœ¨ Neo4j ä¸­æŸ¥è¯¢å’Œå¯è§†åŒ–çš„é¢†åŸŸçŸ¥è¯†å›¾è°±ã€‚

## ğŸ‰ v3.0.0 å…¨åŠŸèƒ½é›†æˆ

**äº”å¤§æ ¸å¿ƒå‡çº§**ï¼š

1. **ğŸ§  Qwen2.5-Coder LLM** - å¼ºåˆ¶ JSON Schema è¾“å‡ºï¼Œå‡†ç¡®ç‡ +22%
2. **ğŸ“„ Layout-Aware è§£æ** - æ™ºèƒ½è¡¨æ ¼æå–ï¼Œè§£æç‡ +35%
3. **ğŸ“· Multimodal æ”¯æŒ** - å›¾ç‰‡è‡ªåŠ¨æè¿°ï¼ŒçŸ¥è¯†æ¥æºå¤šæ ·åŒ–
4. **ğŸ”— BGE-M3 Embedding** - æ··åˆæ£€ç´¢ï¼Œå®ä½“å¯¹é½ 100%
5. **ğŸ¤– Agentic Workflow** - LLM å®¡æŸ¥ + GraphRAG ç¤¾åŒºæ‘˜è¦

ç®¡é“æ ¸å¿ƒç”±äº”éƒ¨åˆ†ç»„æˆï¼š

- **æ™ºèƒ½æ–‡æ¡£è§£æ**ï¼šLayout-Aware PDF æå–ï¼Œæ”¯æŒè¡¨æ ¼å’Œå›¾ç‰‡
- **LLM æ¦‚å¿µæŠ½å–**ï¼šä½¿ç”¨ Qwen2.5-Coder-14B è¿›è¡Œä¸¥æ ¼ JSON Schema è¾“å‡º
- **æ··åˆå»é‡**ï¼šBGE-M3 Dense+Sparse æ··åˆæ£€ç´¢ï¼Œä¸­è‹±å®ä½“å¯¹é½
- **Agentic å®¡æŸ¥**ï¼šLLM äºŒæ¬¡æ ¡éªŒä½ç½®ä¿¡åº¦ä¸‰å…ƒç»„
- **GraphRAG å¢å¼º**ï¼šç¤¾åŒºæ£€æµ‹ç”Ÿæˆä¸»é¢˜èŠ‚ç‚¹ï¼Œæ”¯æŒå…¨å±€çŸ¥è¯†æ¦‚è§ˆ

ç›®æ ‡æ˜¯å¾—åˆ°ä¸€ä»½ç»“æ„æ¸…æ™°ã€æ•°æ®è´¨é‡å¯æ§ã€æ™ºèƒ½åŒ–ç¨‹åº¦é«˜çš„æ¾æçº¿è™«ç—…çŸ¥è¯†å›¾è°±ï¼Œæ”¯æŒè¿›ä¸€æ­¥åˆ†æå’Œå±•ç¤ºã€‚

---

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python 3.9+** ï¼ˆæ¨è 3.10ï¼‰
- **Neo4j 4.x æˆ– 5.x** ï¼ˆGraphRAG éœ€è¦ GDS æ’ä»¶ï¼‰
- **æœ¬åœ° LLM æœåŠ¡**ï¼ˆOllamaï¼‰
  - æ¨èï¼š`qwen2.5-coder:14b` ï¼ˆé»˜è®¤ï¼‰
  - å¤‡é€‰ï¼š`qwen2.5-coder:7b` æˆ– `llama3.2:3b`
- **ç³»ç»Ÿè¦æ±‚**
  - å†…å­˜ï¼š16GB+ ï¼ˆQwen-14B éœ€è¦çº¦ 9GBï¼‰
  - å­˜å‚¨ï¼š20GB+ ï¼ˆåŒ…å«æ¨¡å‹å’Œæ•°æ®ï¼‰
  - GPUï¼šå¯é€‰ï¼ˆåŠ é€Ÿ VLM å›¾ç‰‡æè¿°ï¼‰

### å®‰è£…ä¾èµ–

```bash
# 1. å®‰è£… Python ä¾èµ–
pip install -r requirements.txt

# 2. ä¸‹è½½ spaCy æ¨¡å‹
python -m spacy download zh_core_web_sm
python -m spacy download en_core_web_sm

# 3. ä¸‹è½½ Qwen2.5-Coder LLMï¼ˆå¿…éœ€ï¼‰
ollama pull qwen2.5-coder:14b
# æˆ–ä½¿ç”¨ 7B ç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼‰
# ollama pull qwen2.5-coder:7b

# 4. (å¯é€‰) ä¸‹è½½è§†è§‰æ¨¡å‹ç”¨äºå›¾ç‰‡æè¿°
# huggingface-cli download Qwen/Qwen2-VL-7B-Instruct

# 5. (å¯é€‰) BGE-M3 embedding æ¨¡å‹ä¼šåœ¨é¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½
```

### å‡†å¤‡æ•°æ®

1. å°†å¾…å¤„ç†çš„ PDF æ–‡çŒ®æ”¾å…¥é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ `æ–‡çŒ®/` ç›®å½•
2. æ ¹æ®éœ€è¦è°ƒæ•´ `config/config.yaml` ä¸­çš„å‚æ•°ï¼ˆè§ä¸‹æ–‡â€œé…ç½®è¯´æ˜â€ï¼‰

### è¿è¡Œæ„å»ºç®¡é“

```bash
# æ–¹å¼ä¸€ï¼šç›´æ¥è¿è¡Œä¸»ç¨‹åº
python main.py

# æˆ–ä½¿ç”¨å°è£…å¥½çš„è„šæœ¬
./scripts/workflow/run_complete_workflow.sh
```

ä¸»ç¨‹åºå®Œæˆåï¼Œä¼šåœ¨ `output/` ç›®å½•ç”Ÿæˆï¼š

- `concepts.csv` / `relationships.csv`ï¼šLLM æŠ½å–å’Œå»é‡åçš„æ¦‚å¿µä¸å…³ç³»
- `entities_clean.csv` / `relations_clean.csv`ï¼šæ¸…æ´—åçš„å®ä½“å’Œå…³ç³»
- `neo4j_import/`ï¼šå¯¼å…¥ Neo4j æ‰€éœ€çš„ CSV ä¸ Cypher è„šæœ¬
- `statistics_report.txt`ï¼šæŠ½å–ä¸æ¸…æ´—é˜¶æ®µçš„ç»Ÿè®¡ç»“æœ

### å¯¼å…¥ Neo4j

å¯¼å…¥æ¨èä½¿ç”¨ä¸¤ç§æ–¹å¼ä¹‹ä¸€ï¼š

1. **ä½¿ç”¨ä¸‰å…ƒç»„å¯¼å…¥è„šæœ¬ï¼ˆæœ€ç»ˆå›¾è°±ï¼‰**

   ```bash
   python import_to_neo4j_final.py
   ```

   è¯¥è„šæœ¬ä¼šï¼š

   - è¯»å– `output/triples_export_semantic_clean.csv`ï¼ˆè‹¥å­˜åœ¨ï¼Œå¦åˆ™ä½¿ç”¨ `triples_export.csv`ï¼‰
   - æ¸…ç©ºå½“å‰æ•°æ®åº“
   - åˆ›å»ºèŠ‚ç‚¹ä¸å…³ç³»ï¼Œå¹¶æ·»åŠ ç±»å‹ã€æƒé‡ã€æ ·å¼ç­‰å±æ€§
   - ç”Ÿæˆç´¢å¼•å’ŒåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯

2. **ä½¿ç”¨ CSV + Cypher å¯¼å…¥è„šæœ¬**

   ```bash
   cd output/neo4j_import
   python import_to_neo4j.py
   # æˆ–åœ¨ Neo4j Browser ä¸­æ‰§è¡Œ import.cypher
   ```

   ä½¿ç”¨ `nodes.csv` / `relations.csv` æ„å»ºä¸€ä¸ªæ›´ç®€åŒ–çš„å®ä½“-å…³ç³»å›¾ã€‚

å¯¼å…¥å®Œæˆåï¼Œå¯åœ¨æµè§ˆå™¨è®¿é—® Neo4jï¼š

- åœ°å€ï¼š`http://localhost:7474`
- ç”¨æˆ·åï¼š`neo4j`
- å¯†ç ï¼š`12345678`ï¼ˆé»˜è®¤å€¼ï¼Œè§ `config/config.yaml`ï¼‰

---

## ğŸ“Š v3.0.0 æ€§èƒ½æå‡

### ğŸ¯ æ–°åŠŸèƒ½ç‰¹æ€§

#### 1. Qwen2.5-Coder LLM å‡çº§

- **æ¨¡å‹**: `qwen2.5-coder:14b` (æˆ– 7B)
- **ä¸Šä¸‹æ–‡**: 8k-32k tokens (vs 2k-4k)
- **JSON è¾“å‡º**: å¼ºåˆ¶ Schema éªŒè¯
- **Prompt**: é¢†åŸŸçŸ¥è¯†åµŒå…¥ + 9 å¤§æ¦‚å¿µç±»åˆ«
- **æ€§èƒ½**: JSON è§£ææˆåŠŸç‡æå‡è‡³ 97%

#### 2. Layout-Aware æ–‡æ¡£è§£æ

- **æ™ºèƒ½è§£æ**: Marker PDF + PDFPlumber è¡¨æ ¼æå–
- **ç»“æ„åŒ–**: Markdown æ ¼å¼è¾“å‡º
- **å‚è€ƒæ–‡çŒ®**: ç²¾å‡†è¯†åˆ«å’Œå‰”é™¤
- **æ€§èƒ½**: è¡¨æ ¼æ•°æ®è§£æç‡æå‡è‡³ 95%

#### 3. Multimodal å›¾ç‰‡æè¿°

- **å›¾ç‰‡æå–**: PyMuPDF è‡ªåŠ¨æå– PDF å›¾ç‰‡
- **VLM æè¿°**: Qwen2-VL-7B / Ollama VLM
- **æ–‡æœ¬èåˆ**: æè¿°æ’å…¥åŸæ–‡ä¸Šä¸‹æ–‡
- **åº”ç”¨**: å›¾è¡¨ã€æ˜¾å¾®é•œå›¾ã€ç»Ÿè®¡å›¾è½¬æ–‡å­—

#### 4. BGE-M3 Embedding å‡çº§

- **æ¨¡å‹**: BAAI/bge-m3
- **æ··åˆæ£€ç´¢**: Dense (å‘é‡) + Sparse (å…³é”®è¯)
- **å®ä½“å¯¹é½**: ä¸­è‹±æ–‡ 100% å‡†ç¡®
- **åŒä¹‰è¯**: è‡ªåŠ¨å‘ç°å’Œæ‰©å……

#### 5. Agentic Workflow

- **LLM å®¡ç¨¿äºº**: å¯¹ 0.6-0.8 ç½®ä¿¡åº¦ä¸‰å…ƒç»„äºŒæ¬¡åˆ¤æ–­
- **GraphRAG**: Louvain/Leiden ç¤¾åŒºæ£€æµ‹
- **ä¸»é¢˜èŠ‚ç‚¹**: è‡ªåŠ¨ç”Ÿæˆç¤¾åŒºæ‘˜è¦
- **è´¨é‡æ§åˆ¶**: æ™ºèƒ½è¿‡æ»¤å’Œä¼˜åŒ–

---

## å·¥ä½œæµç¨‹ä¸æŠ€æœ¯å®ç°è¯¦è§£

æ•´ä¸ªæµç¨‹å¯ä»¥åˆ†ä¸ºå››ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰è¯¦ç»†çš„æŠ€æœ¯å®ç°ç»†èŠ‚ï¼š

---

### é˜¶æ®µ 1ï¼šPDF æ–‡æœ¬æå–ä¸é¢„å¤„ç†

**æ ¸å¿ƒæ¨¡å—**ï¼š`pdf_extractor.py`ã€`ocr_processor.py`ã€`parallel_processor.py`

#### 1.1 æŠ€æœ¯æ ˆ

- **PyMuPDF (fitz)**ï¼šä¸»è¦ PDF è§£æåº“

  - é€Ÿåº¦å¿«ï¼ˆ1-2 é¡µ/ç§’ï¼‰
  - æ”¯æŒå¤æ‚æ ¼å¼
  - æå–æ–‡æœ¬åŒæ—¶ä¿ç•™å¸ƒå±€ä¿¡æ¯

- **OCR æ”¯æŒï¼ˆå¯é€‰ï¼‰**ï¼š

  - Tesseract OCRï¼šå¼€æº OCR å¼•æ“
  - PaddleOCRï¼šä¸­æ–‡è¯†åˆ«æ•ˆæœæ›´å¥½
  - è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬è´¨é‡ï¼Œä½äºé˜ˆå€¼æ—¶è§¦å‘ OCR

- **å¹¶è¡Œå¤„ç†**ï¼š
  - ä½¿ç”¨ `multiprocessing` å¹¶è¡Œå¤„ç†å¤šä¸ª PDF
  - é»˜è®¤ 8 ä¸ªå·¥ä½œè¿›ç¨‹
  - åŸºäºé˜Ÿåˆ—çš„ä»»åŠ¡åˆ†é…

#### 1.2 å…³é”®ç®—æ³•

**æ–‡æœ¬æ¸…æ´—æµç¨‹**ï¼š

```python
1. ç§»é™¤æ§åˆ¶å­—ç¬¦: [\x00-\x08\x0b-\x0c\x0e-\x1f]
2. ç»Ÿä¸€è¡Œç»“æŸç¬¦: \r\n â†’ \n
3. å»é™¤é¡µçœ‰é¡µè„šæ¨¡å¼:
   - "ç¬¬Xé¡µ" / "Page X"
   - "ç‰ˆæƒæ‰€æœ‰" / "Copyright"
   - é¡µç æ¨¡å¼: "X/Y"
4. è¿‡æ»¤å…ƒæ•°æ®å…³é”®è¯:
   - ä½œè€…ã€å•ä½ã€æ”¶ç¨¿ã€åŸºé‡‘é¡¹ç›®
5. æ£€æµ‹å¹¶æˆªæ–­å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
```

**å‚è€ƒæ–‡çŒ®æ£€æµ‹**ï¼š

```python
# å…³é”®è¯åŒ¹é…
keywords = ['å‚è€ƒæ–‡çŒ®', 'References', 'Bibliography']

# å¯å‘å¼è§„åˆ™
- æ£€æµ‹è¿ç»­å¼•ç”¨æ ¼å¼: "[1] ä½œè€…..."
- è¯†åˆ«å¼•ç”¨å¯†åº¦çªå¢æ®µè½
- åŸºäºç¼©è¿›å’Œç¼–å·æ¨¡å¼
```

**ç¼“å­˜æœºåˆ¶**ï¼š

- ä½¿ç”¨ `hashlib.md5` å¯¹æ–‡ä»¶å†…å®¹ç”ŸæˆæŒ‡çº¹
- ç¼“å­˜ç»“æ„ï¼š`{pdf_hash: extracted_text}`
- æ”¯æŒå¢é‡å¤„ç†ï¼Œé¿å…é‡å¤æå–

#### 1.3 æ€§èƒ½ä¼˜åŒ–

| ä¼˜åŒ–æŠ€æœ¯     | æå‡æ•ˆæœ     | è¯´æ˜                |
| ------------ | ------------ | ------------------- |
| å¹¶è¡Œå¤„ç†     | 5-8 å€       | å¤šæ ¸ CPU åˆ©ç”¨ç‡æå‡ |
| ç¼“å­˜æœºåˆ¶     | 100 å€       | é¿å…é‡å¤æå–        |
| OCR æŒ‰éœ€è§¦å‘ | èŠ‚çœ 90%æ—¶é—´ | ä»…å¯¹ä½è´¨é‡æ–‡æœ¬å¯ç”¨  |
| æ–‡æœ¬åˆ†å—     | å‡å°‘å†…å­˜ 50% | æµå¼å¤„ç†å¤§æ–‡ä»¶      |

#### 1.4 å¯æ”¹è¿›ç‚¹

- ğŸ”„ **è¡¨æ ¼æå–**ï¼šä½¿ç”¨ `camelot` æˆ– `pdfplumber` ç»“æ„åŒ–æå–è¡¨æ ¼
- ğŸ”„ **å›¾ç‰‡ OCR**ï¼šæå–å›¾ç‰‡ä¸­çš„æ–‡å­—ä¿¡æ¯
- ğŸ”„ **å¤šè¯­è¨€æ”¯æŒ**ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶åˆ†ç¦»ä¸­è‹±æ–‡
- ğŸ”„ **PDF ç»“æ„è§£æ**ï¼šè¯†åˆ«ç« èŠ‚ã€æ ‡é¢˜ã€æ‘˜è¦ç­‰è¯­ä¹‰ç»“æ„
- ğŸ”„ **å…¬å¼è¯†åˆ«**ï¼šä½¿ç”¨ LaTeX-OCR æå–æ•°å­¦å…¬å¼

---

### é˜¶æ®µ 2ï¼šLLM æ¦‚å¿µä¸å…³ç³»æŠ½å–

**æ ¸å¿ƒæ¨¡å—**ï¼š`enhanced_pipeline.py`ã€`concept_extractor.py`ã€`concept_deduplicator.py`

#### 2.1 æŠ€æœ¯æ¶æ„

**LLM æä¾›å•†**ï¼š

- **Ollama æœ¬åœ°æœåŠ¡**ï¼š
  - æ¨¡å‹ï¼š`qwen2.5-coder:14b` (é»˜è®¤ï¼Œv3.0)
  - å¤‡é€‰ï¼š`qwen2.5-coder:7b`, `llama3.2:3b`
  - API ç«¯ç‚¹ï¼š`http://localhost:11434/api/generate`
  - è¶…æ—¶è®¾ç½®ï¼š180 ç§’ (Qwen-14B éœ€è¦æ›´é•¿æ—¶é—´)
  - é‡è¯•æœºåˆ¶ï¼š3 æ¬¡
  - JSON æ¨¡å¼ï¼šå¼ºåˆ¶ Schema è¾“å‡º

**Prompt Engineering**ï¼š

```python
# ç³»ç»Ÿæç¤ºè¯ï¼ˆé¢†åŸŸä¸“å®¶è§’è‰²ï¼‰
system_prompt = """
ä½ æ˜¯æ¾æçº¿è™«ç—…çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚

é‡ç‚¹å…³æ³¨:
- ç—…åŸä½“: æ¾æçº¿è™«ã€ä¼´ç”Ÿç»†èŒ
- å¯„ä¸»æ¤ç‰©: æ¾æ ‘ã€é©¬å°¾æ¾ã€é»‘æ¾
- åª’ä»‹æ˜†è™«: æ¾è¤å¤©ç‰›
- ç—…å®³ç—‡çŠ¶: èè”«ã€æ¯æ­»ã€å˜è‰²
- é˜²æ²»æªæ–½: è¯å‰‚ã€è¯±æ•å™¨
- ç¯å¢ƒå› å­: æ¸©åº¦ã€æ¹¿åº¦ã€æµ·æ‹”

ç±»åˆ«: pathogen, host, vector, symptom,
      treatment, environment, location

é¿å…: é€šç”¨è¯(å› ç´ ã€è¿‡ç¨‹ã€æ–¹æ³•)
"""

# è¾“å‡ºæ ¼å¼ï¼ˆç»“æ„åŒ– JSONï¼‰
output_format = [
  {
    "entity": "æ¦‚å¿µå",
    "importance": 1-5,  # é‡è¦æ€§è¯„åˆ†
    "category": "ç±»åˆ«"
  }
]
```

**å…³ç³»æŠ½å– Prompt**ï¼š

```python
system_prompt = """
æå–æ¦‚å¿µé—´çš„è¯­ä¹‰å…³ç³»ã€‚

å…³ç³»ç±»å‹:
- INFECTS(æ„ŸæŸ“): ç—…åŸâ†’å¯„ä¸»
- TRANSMITS(ä¼ æ’­): åª’ä»‹â†’ç—…åŸ/ç–¾ç—…
- PARASITIZES(å¯„ç”Ÿ): åª’ä»‹/ç—…åŸâ†’å¯„ä¸»
- CAUSES(å¼•èµ·): ç—…åŸâ†’ç—‡çŠ¶
- TREATS(é˜²æ²»): æªæ–½â†’ç—…åŸ/ç–¾ç—…
- DISTRIBUTED_IN(åˆ†å¸ƒ): ç”Ÿç‰©â†’åœ°åŒº
- AFFECTS(å½±å“): å› ç´ â†’ç–¾ç—…/å¯„ä¸»

è¾“å‡º: [{"head": "A", "tail": "B",
        "relation": "ç±»å‹", "confidence": 0-1}]
"""
```

#### 2.2 å…³é”®å‚æ•°è°ƒä¼˜

```yaml
LLM å‚æ•°:
  temperature: 0.1 # ä½æ¸©åº¦ä¿è¯è¾“å‡ºç¨³å®š
  top_p: 0.9 # æ ¸é‡‡æ ·
  top_k: 40 # å€™é€‰è¯é™åˆ¶
  max_tokens: 800 # é™åˆ¶è¾“å‡ºé•¿åº¦

æ–‡æœ¬åˆ†å—:
  chunk_size: 2000 # å­—ç¬¦æ•°ï¼ˆé€‚é… token é™åˆ¶ï¼‰
  overlap: 200 # é‡å é¿å…è¯­ä¹‰æ–­è£‚
  max_chunks: 100 # æ€»å—æ•°é™åˆ¶ï¼ˆæ§åˆ¶æˆæœ¬ï¼‰

è¾“å‡ºè§£æ:
  json_repair: True # è‡ªåŠ¨ä¿®å¤æ ¼å¼é”™è¯¯
  retry_on_error: 3 # è§£æå¤±è´¥é‡è¯•
```

#### 2.3 æ¦‚å¿µå»é‡ç®—æ³•

**åµŒå…¥æ¨¡å‹é€‰æ‹©**ï¼š

```python
# v3.0 ä¸»é€‰ï¼šBGE-M3 (æ¨è)
model = "BAAI/bge-m3"
- æ··åˆæ£€ç´¢ (Dense + Sparse)
- 1024 ç»´å‘é‡
- ä¸­è‹±æ–‡å¯¹é½ 100%
- æ”¯æŒæ··åˆç›¸ä¼¼åº¦è®¡ç®—

# å¤‡é€‰ï¼šsentence-transformers
model = "sentence-transformers/paraphrase-MiniLM-L6-v2"
- æ”¯æŒå¤šè¯­è¨€
- 384 ç»´å‘é‡
- é€Ÿåº¦å¿«ï¼ˆ50 æ¦‚å¿µ/ç§’ï¼‰

# å¤‡é€‰ï¼šTF-IDF (æ— éœ€é¢„è®­ç»ƒ)
- åŸºäºå­—ç¬¦ n-gram (2-3)
- 100 ç»´å‘é‡
- é€‚åˆå°è§„æ¨¡æ•°æ®
```

**å»é‡ç­–ç•¥**ï¼š

```python
# 1. è®¡ç®—è¯­ä¹‰åµŒå…¥
embeddings = model.encode(concepts)  # [N, 384]

# 2. ç›¸ä¼¼åº¦çŸ©é˜µ
similarity = cosine_similarity(embeddings)  # [N, N]

# 3. è´ªå¿ƒèšç±»
threshold = 0.85  # ç›¸ä¼¼åº¦é˜ˆå€¼
for i, concept_i in enumerate(concepts):
    if used[i]: continue
    canonical = concept_i  # é¦–ä¸ªä½œä¸ºè§„èŒƒå½¢å¼

    for j in range(i+1, len(concepts)):
        if similarity[i][j] >= threshold:
            mapping[concept_j] = canonical  # æ˜ å°„åˆ°è§„èŒƒå½¢å¼
            used[j] = True

# 4. å±æ€§åˆå¹¶
importance = max(group_importances)    # å–æœ€é«˜
category = most_common(group_categories) # å–ä¼—æ•°
connections = sum(group_connections)   # ç´¯åŠ è¿æ¥æ•°
```

**é˜ˆå€¼è°ƒä¼˜ç­–ç•¥**ï¼š

```
ç›¸ä¼¼åº¦é˜ˆå€¼ similarity_threshold:
- 0.80-0.83: æ¿€è¿›å»é‡ï¼Œé€‚åˆåˆæ­¥æ¸…æ´—
- 0.83-0.87: å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼‰
- 0.87-0.95: ä¿å®ˆæ¨¡å¼ï¼Œä¿ç•™ç»†å¾®å·®å¼‚
```

#### 2.4 æ€§èƒ½åˆ†æ

| æ­¥éª¤     | æ—¶é—´å¼€é”€   | ç“¶é¢ˆ     |
| -------- | ---------- | -------- |
| PDF æå– | 1-2 åˆ†é’Ÿ   | I/O      |
| æ–‡æœ¬åˆ†å— | <10 ç§’     | è®¡ç®—     |
| LLM æ¨ç† | 10-30 åˆ†é’Ÿ | ä¸»è¦ç“¶é¢ˆ |
| æ¦‚å¿µå»é‡ | 10-30 ç§’   | åµŒå…¥è®¡ç®— |
| å…³ç³»åˆå¹¶ | <5 ç§’      | å†…å­˜     |

**LLM è°ƒç”¨ç»Ÿè®¡**ï¼ˆ30 ä¸ªæ–‡æœ¬å—ä¸ºä¾‹ï¼‰ï¼š

```
è°ƒç”¨æ¬¡æ•°: 60 æ¬¡ï¼ˆæ¦‚å¿µ + å…³ç³»å„ 30ï¼‰
å¹³å‡å»¶è¿Ÿ: 3-8 ç§’/æ¬¡
æ€»æ—¶é•¿: 5-15 åˆ†é’Ÿ
Token æ¶ˆè€—: ~60K input + ~24K output
```

#### 2.5 å¯æ”¹è¿›ç‚¹

- ğŸ”„ **Few-shot Learning**ï¼šåœ¨ prompt ä¸­æ·»åŠ ç¤ºä¾‹æå‡å‡†ç¡®ç‡
- ğŸ”„ **Function Calling**ï¼šä½¿ç”¨ GPT-4 çš„ç»“æ„åŒ–è¾“å‡ºæ¨¡å¼
- ğŸ”„ **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼šåˆå¹¶å¤šä¸ªå°å—å‡å°‘ API è°ƒç”¨
- ğŸ”„ **æœ¬åœ° LLM ä¼˜åŒ–**ï¼šä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆ4-bitï¼‰åŠ é€Ÿæ¨ç†
- ğŸ”„ **æ··åˆç­–ç•¥**ï¼šè§„åˆ™ + LLM äº’è¡¥æå‡å¬å›ç‡
- ğŸ”„ **ä¸»åŠ¨å­¦ä¹ **ï¼šå¯¹ä½ç½®ä¿¡åº¦æ ·æœ¬äººå·¥æ ‡æ³¨è¿­ä»£æ”¹è¿›
- ğŸ”„ **å±‚æ¬¡èšç±»**ï¼šHDBSCAN æ›¿ä»£ç®€å•é˜ˆå€¼èšç±»

---

### é˜¶æ®µ 3ï¼šæ•°æ®æ¸…æ´—ä¸è´¨é‡æ§åˆ¶

**æ ¸å¿ƒæ¨¡å—**ï¼š`data_cleaner.py`ã€`neo4j_generator.py`ã€`entity_linker.py`

#### 3.1 æ¸…æ´—è§„åˆ™ä½“ç³»

**å®ä½“è¿‡æ»¤è§„åˆ™**ï¼š

```python
# 1. å­—ç¬¦é•¿åº¦è¿‡æ»¤
min_length = 2          # è¿‡çŸ­å®ä½“ï¼ˆå¦‚"çš„"ã€"å’Œ"ï¼‰
max_length = 50         # è¿‡é•¿å®ä½“ï¼ˆå¯èƒ½æ˜¯å¥å­ç‰‡æ®µï¼‰

# 2. åœç”¨è¯è¿‡æ»¤
stopwords = load('config/stopwords.txt')
# åŒ…å«: å› ç´ ã€è¿‡ç¨‹ã€æ–¹æ³•ã€å½±å“ã€ä½œç”¨ ç­‰é€šç”¨è¯

# 3. ç‰¹æ®Šå­—ç¬¦è¿‡æ»¤
invalid_patterns = [
    r'^[0-9]+$',        # çº¯æ•°å­—
    r'^[a-zA-Z]{1,2}$', # å•ä¸ªå­—æ¯
    r'[^\w\s\-()]',     # ç‰¹æ®Šç¬¦å·
]

# 4. é¢‘æ¬¡è¿‡æ»¤
min_frequency = 2       # è‡³å°‘å‡ºç° 2 æ¬¡
```

**å…³ç³»è¿‡æ»¤è§„åˆ™**ï¼š

```python
# 1. ç½®ä¿¡åº¦é˜ˆå€¼
confidence_threshold = 0.65

# 2. è‡ªç¯æ£€æµ‹
head == tail â†’ åˆ é™¤

# 3. é‡å¤å…³ç³»åˆå¹¶
(A, R, B) + (A, R, B) â†’ æƒé‡ç´¯åŠ 

# 4. å¯¹ç§°å…³ç³»å¤„ç†
(A, CO_OCCURS_WITH, B) â‰ˆ (B, CO_OCCURS_WITH, A)
â†’ ä»…ä¿ç•™ä¸€æ¡ï¼Œæƒé‡ç´¯åŠ 
```

**å®ä½“å‘½åè§„èŒƒåŒ–**ï¼š

```python
# 1. å¤§å°å†™ç»Ÿä¸€
- ä¸­æ–‡æ¦‚å¿µ: ä¿æŒåŸæ ·
- è‹±æ–‡æ¦‚å¿µ: å°å†™åŒ–
- ä¸“æœ‰åè¯: é¦–å­—æ¯å¤§å†™

# 2. ç©ºæ ¼æ ‡å‡†åŒ–
å¤šä¸ªç©ºæ ¼ â†’ å•ä¸ªç©ºæ ¼
å‰åç©ºæ ¼ â†’ å»é™¤

# 3. åŒä¹‰è¯åˆå¹¶
mapping = {
    "æ¾æçº¿è™«": "bursaphelenchus xylophilus",
    "å¤©ç‰›": "monochamus alternatus",
    "é»‘æ¾": "pinus thunbergii"
}

# 4. ç¼©å†™æ‰©å±•
"PWD" â†’ "pine wilt disease"
```

#### 3.2 å®ä½“é“¾æ¥

**å®ä½“é“¾æ¥ç­–ç•¥**ï¼š

```python
# 1. ç²¾ç¡®åŒ¹é…
if entity in knowledge_base:
    return kb_entity

# 2. æ¨¡ç³ŠåŒ¹é…ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
def levenshtein_distance(s1, s2):
    # å…è®¸ 20% ç¼–è¾‘è·ç¦»
    threshold = 0.8

# 3. è¯å¹²æå–
from nltk.stem import PorterStemmer
stem(entity) == stem(kb_entity)

# 4. å‘é‡ç›¸ä¼¼åº¦
embedding_similarity(entity, kb_entity) > 0.90
```

**çŸ¥è¯†åº“æ¥æº**ï¼š

```python
# é¢†åŸŸè¯å…¸ï¼ˆdomain_dict.jsonï¼‰
{
  "ç—…åŸä½“": [
    "bursaphelenchus xylophilus",
    "æ¾æçº¿è™«"
  ],
  "å¯„ä¸»": [
    "pinus thunbergii", "é»‘æ¾",
    "pinus massoniana", "é©¬å°¾æ¾"
  ]
}

# å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆå¯æ‰©å±•ï¼‰
- WikiData
- UMLS (åŒ»å­¦ç»Ÿä¸€è¯­è¨€ç³»ç»Ÿ)
- ç”Ÿç‰©åˆ†ç±»æ•°æ®åº“
```

#### 3.3 Neo4j å¯¼å…¥æ–‡ä»¶ç”Ÿæˆ

**CSV æ ¼å¼è§„èŒƒ**ï¼š

```python
# nodes.csv
id,name,type,importance,connections
concept_001,bursaphelenchus xylophilus,Pathogen,5,23
concept_002,pinus thunbergii,Host,4,18

# relations.csv
source,target,relation,weight,confidence,source_pdf
concept_001,concept_002,INFECTS,0.92,0.88,paper1.pdf
```

**Cypher è„šæœ¬ç”Ÿæˆ**ï¼š

```cypher
// 1. åˆ›å»ºå”¯ä¸€æ€§çº¦æŸ
CREATE CONSTRAINT concept_name_unique
FOR (n:Concept) REQUIRE n.name IS UNIQUE;

// 2. æ‰¹é‡å¯¼å…¥èŠ‚ç‚¹ï¼ˆMERGE é¿å…é‡å¤ï¼‰
LOAD CSV WITH HEADERS FROM 'file:///nodes.csv' AS row
MERGE (n:Concept {name: row.name})
SET n.type = row.type,
    n.importance = toInteger(row.importance);

// 3. åˆ›å»ºç´¢å¼•
CREATE INDEX concept_type_index
FOR (n:Concept) ON (n.type);

// 4. æ‰¹é‡å¯¼å…¥å…³ç³»
LOAD CSV WITH HEADERS FROM 'file:///relations.csv' AS row
MATCH (a:Concept {name: row.source})
MATCH (b:Concept {name: row.target})
MERGE (a)-[r:${row.relation}]->(b)
SET r.weight = toFloat(row.weight);
```

#### 3.4 è´¨é‡æ§åˆ¶æŒ‡æ ‡

| æŒ‡æ ‡       | é˜ˆå€¼ | æ£€æŸ¥æ–¹å¼            |
| ---------- | ---- | ------------------- |
| æ¦‚å¿µæœ‰æ•ˆç‡ | >85% | äººå·¥æŠ½æŸ¥ 100 ä¸ª     |
| å…³ç³»å‡†ç¡®ç‡ | >70% | äººå·¥éªŒè¯ 50 ä¸ª      |
| å»é‡è¦†ç›–ç‡ | >90% | è®¡ç®—åŒä¹‰è¯å¯¹æ•°      |
| å­¤ç«‹èŠ‚ç‚¹ç‡ | <10% | è®¡ç®—åº¦æ•°ä¸º 0 çš„èŠ‚ç‚¹ |
| è‡ªç¯å…³ç³»   | 0    | è‡ªåŠ¨æ£€æµ‹å¹¶ç§»é™¤      |

#### 3.5 å¯æ”¹è¿›ç‚¹

- ğŸ”„ **ä¸»åŠ¨å­¦ä¹ **ï¼šäººå·¥æ ‡æ³¨è¾¹ç•Œæ ·ä¾‹ä¼˜åŒ–é˜ˆå€¼
- ğŸ”„ **è§„åˆ™æŒ–æ˜**ï¼šè‡ªåŠ¨å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼
- ğŸ”„ **å¼‚å¸¸æ£€æµ‹**ï¼šè¯†åˆ«å¼‚å¸¸é«˜/ä½é¢‘å®ä½“
- ğŸ”„ **å®ä½“æ¶ˆæ­§**ï¼šåŒºåˆ†åŒåä¸åŒä¹‰çš„å®ä½“
- ğŸ”„ **å…³ç³»ç±»å‹ç»†åŒ–**ï¼šå°† CO_OCCURS ç»†åˆ†ä¸ºæ›´å…·ä½“çš„è¯­ä¹‰å…³ç³»

---

### é˜¶æ®µ 4ï¼šè¯­ä¹‰ä½“æ£€ä¸å›¾è°±ä¼˜åŒ–

**æ ¸å¿ƒæ¨¡å—**ï¼š`bio_semantic_review.py`ã€`fix_semantic_triples.py`

#### 4.1 èŠ‚ç‚¹ç±»å‹æ¨æ–­

**åŸºäºè§„åˆ™çš„åˆ†ç±»å™¨**ï¼š

```python
def infer_node_type(name: str) -> str:
    n = name.lower()

    # ä¼˜å…ˆçº§è§„åˆ™ï¼ˆä»é«˜åˆ°ä½ï¼‰
    if "bursaphelenchus" in n:
        return "Pathogen"

    if "pine wilt" in n:
        return "Disease"

    if any(x in n for x in ["pinus", "pine", "tree"]):
        return "Host"

    if "monochamus" in n or "beetle" in n:
        return "Vector"

    if any(x in n for x in ["control", "trap", "é˜²æ²»"]):
        return "ControlMeasure"

    if any(x in n for x in ["symptom", "wilt", "ç—‡çŠ¶"]):
        return "Symptom"

    if any(x in n for x in ["province", "city", "area"]):
        return "Region"

    if any(x in n for x in ["temperature", "climate"]):
        return "EnvironmentalFactor"

    if any(x in n for x in ["spectral", "algorithm"]):
        return "Technology"

    return "Other"
```

**ç±»å‹åˆ†å¸ƒéªŒè¯**ï¼š

```python
# é¢„æœŸåˆ†å¸ƒï¼ˆåŸºäºé¢†åŸŸçŸ¥è¯†ï¼‰
expected_distribution = {
    "Pathogen": 5-10,
    "Host": 10-20,
    "Vector": 3-8,
    "Disease": 1-3,
    "Symptom": 5-15,
    "ControlMeasure": 3-10,
    "Region": 5-15,
    "EnvironmentalFactor": 3-8,
    "Technology": 3-8,
    "Other": <20
}
```

#### 4.2 å…³ç³»è¯­ä¹‰æ£€æŸ¥

**å…³ç³»-èŠ‚ç‚¹ç±»å‹ç™½åå•**ï¼š

```python
VALID_RELATION_PATTERNS = {
    "INFECTS": [
        ("Pathogen", "Host"),     # ç—…åŸæ„ŸæŸ“å¯„ä¸» âœ“
        ("Pathogen", "Vector"),   # ç—…åŸæ„ŸæŸ“åª’ä»‹ âœ“
    ],
    "TRANSMITS": [
        ("Vector", "Pathogen"),   # åª’ä»‹ä¼ æ’­ç—…åŸ âœ“
        ("Vector", "Disease"),    # åª’ä»‹ä¼ æ’­ç–¾ç—… âœ“
    ],
    "PARASITIZES": [
        ("Pathogen", "Host"),     # ç—…åŸå¯„ç”Ÿäºå¯„ä¸» âœ“
        ("Vector", "Host"),       # åª’ä»‹å¯„ç”Ÿäºå¯„ä¸» âœ“
    ],
    "CAUSES": [
        ("Pathogen", "Symptom"),  # ç—…åŸå¼•èµ·ç—‡çŠ¶ âœ“
        ("Disease", "Symptom"),   # ç–¾ç—…å¼•èµ·ç—‡çŠ¶ âœ“
    ],
    "TREATS": [
        ("ControlMeasure", "Disease"),  # æªæ–½æ²»ç–—ç–¾ç—… âœ“
        ("ControlMeasure", "Pathogen"), # æªæ–½å¯¹æŠ—ç—…åŸ âœ“
    ],
    "DISTRIBUTED_IN": [
        ("Pathogen", "Region"),   # ç—…åŸåˆ†å¸ƒäºåœ°åŒº âœ“
        ("Host", "Region"),       # å¯„ä¸»åˆ†å¸ƒäºåœ°åŒº âœ“
        ("Vector", "Region"),     # åª’ä»‹åˆ†å¸ƒäºåœ°åŒº âœ“
    ]
}
```

**è¯­ä¹‰å¼‚å¸¸æ£€æµ‹**ï¼š

```python
def check_semantic_validity(head_type, relation, tail_type):
    """æ£€æŸ¥ä¸‰å…ƒç»„è¯­ä¹‰åˆç†æ€§"""

    # 1. æ£€æŸ¥ç™½åå•
    if (head_type, tail_type) not in VALID_RELATION_PATTERNS[relation]:
        issue = f"Invalid pattern: {head_type} -{relation}-> {tail_type}"

    # 2. æ£€æµ‹æ–¹å‘é”™è¯¯
    if relation == "INFECTS" and head_type == "Host":
        suggestion = "Reverse direction"

    # 3. æ£€æµ‹è¯­ä¹‰å†²çª
    if relation == "TRANSMITS" and tail_type == "Host":
        issue = "TRANSMITS should target Pathogen/Disease"

    # 4. æ£€æµ‹è‡ªç¯
    if head == tail:
        issue = "Self-loop detected"
        action = "DELETE"
```

#### 4.3 è‡ªåŠ¨ä¿®æ­£ç­–ç•¥

**æ–¹å‘çº æ­£**ï¼š

```python
AUTO_REVERSE_RULES = {
    # (å…³ç³», é”™è¯¯æ–¹å‘) â†’ æ­£ç¡®æ–¹å‘
    ("INFECTS", ("Host", "Pathogen")):
        ("Pathogen", "INFECTS", "Host"),

    ("TRANSMITS", ("Disease", "Vector")):
        ("Vector", "TRANSMITS", "Disease"),

    ("TREATS", ("Disease", "ControlMeasure")):
        ("ControlMeasure", "TREATS", "Disease"),
}

# ä»…åœ¨éå¸¸ç¡®å®šçš„æƒ…å†µä¸‹è‡ªåŠ¨çº æ­£
confidence_threshold = 0.95
```

**å…³ç³»ç±»å‹æ›¿æ¢**ï¼š

```python
RELATION_TYPE_FIXES = {
    # è¿‡äºé€šç”¨çš„å…³ç³» â†’ æ›´å…·ä½“çš„å…³ç³»
    ("Host", "CO_OCCURS_WITH", "Vector"): "HOSTS",
    ("Pathogen", "CO_OCCURS_WITH", "Symptom"): "CAUSES",
    ("ControlMeasure", "CO_OCCURS_WITH", "Disease"): "TREATS",
}
```

#### 4.4 è´¨é‡æŠ¥å‘Šç”Ÿæˆ

**è¯­ä¹‰é—®é¢˜åˆ†ç±»**ï¼š

```python
issues_df = pd.DataFrame(columns=[
    'triple_id',          # ä¸‰å…ƒç»„ ID
    'head', 'relation', 'tail',
    'head_type', 'tail_type',
    'issue_type',         # é—®é¢˜ç±»å‹
    'severity',           # ä¸¥é‡ç¨‹åº¦ (HIGH/MEDIUM/LOW)
    'suggestion',         # ä¿®æ­£å»ºè®®
    'auto_fixed'          # æ˜¯å¦è‡ªåŠ¨ä¿®æ­£
])

# é—®é¢˜ç±»å‹ç»Ÿè®¡
issue_types = [
    "INVALID_PATTERN",    # ä¸ç¬¦åˆç™½åå•
    "WRONG_DIRECTION",    # æ–¹å‘é”™è¯¯
    "SELF_LOOP",          # è‡ªç¯
    "ORPHAN_NODE",        # å­¤ç«‹èŠ‚ç‚¹
    "LOW_CONFIDENCE",     # ä½ç½®ä¿¡åº¦
]
```

**æ¸…æ´—æŠ¥å‘Šç¤ºä¾‹**ï¼š

```
=== è¯­ä¹‰ä½“æ£€æŠ¥å‘Š ===
æ£€æŸ¥æ—¶é—´: 2025-11-16
åŸå§‹ä¸‰å…ƒç»„: 365 æ¡
æ£€æµ‹é—®é¢˜: 47 æ¡

é—®é¢˜åˆ†ç±»:
  - æ–¹å‘é”™è¯¯: 12 (è‡ªåŠ¨ä¿®æ­£: 8)
  - æ— æ•ˆæ¨¡å¼: 23 (äººå·¥å®¡æ ¸)
  - è‡ªç¯: 5 (è‡ªåŠ¨åˆ é™¤)
  - å­¤ç«‹èŠ‚ç‚¹: 7 (ä¿ç•™ä½†æ ‡è®°)

æ¸…æ´—åä¸‰å…ƒç»„: 352 æ¡
è´¨é‡æå‡: çº¦ 15%
```

#### 4.5 å¯æ”¹è¿›ç‚¹

- ğŸ”„ **æœºå™¨å­¦ä¹ åˆ†ç±»å™¨**ï¼šä½¿ç”¨ GNN è‡ªåŠ¨å­¦ä¹ èŠ‚ç‚¹ç±»å‹
- ğŸ”„ **å…³ç³»éªŒè¯æ¨¡å‹**ï¼šè®­ç»ƒåˆ†ç±»å™¨åˆ¤æ–­å…³ç³»åˆç†æ€§
- ğŸ”„ **çŸ¥è¯†å›¾è°±åµŒå…¥**ï¼šTransE/RotatE æ£€æµ‹ä¸ä¸€è‡´æ€§
- ğŸ”„ **è§„åˆ™å­¦ä¹ **ï¼šä»æ•°æ®ä¸­è‡ªåŠ¨æŒ–æ˜è¯­ä¹‰è§„åˆ™
- ğŸ”„ **äº¤äº’å¼å®¡æ ¸ç•Œé¢**ï¼šå¯è§†åŒ–å®¡æ ¸å’Œä¿®æ­£å·¥å…·

---

### ğŸ†• é˜¶æ®µ 5ï¼šv3.0 å¤šæ¨¡æ€å›¾ç‰‡å¤„ç†ï¼ˆå¯é€‰ï¼‰

**æ ¸å¿ƒæ¨¡å—**ï¼š`image_captioner.py`

#### 5.1 å›¾ç‰‡æå–

**PDF å›¾ç‰‡æµæå–**ï¼š

```python
import fitz  # PyMuPDF

def extract_images_from_pdf(pdf_path: str, output_dir: str):
    """ä» PDF ä¸­æå–æ‰€æœ‰å›¾ç‰‡"""
    doc = fitz.open(pdf_path)
    images = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        image_list = page.get_images()

        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            # ä¿å­˜å›¾ç‰‡
            image_path = f"{output_dir}/page{page_num}_img{img_index}.{image_ext}"
            with open(image_path, "wb") as img_file:
                img_file.write(image_bytes)

            images.append({
                "page": page_num,
                "path": image_path,
                "format": image_ext
            })

    return images
```

#### 5.2 VLM æè¿°ç”Ÿæˆ

**ImageCaptioner å®ç°**ï¼š

```python
class ImageCaptioner:
    """ç»Ÿä¸€çš„å›¾åƒæè¿°æ¥å£"""

    def __init__(self, model_name="Qwen/Qwen2-VL-7B-Instruct",
                 provider="transformers"):
        self.model_name = model_name
        self.provider = provider  # transformers or ollama

        if provider == "transformers":
            from transformers import pipeline
            self._pipeline = pipeline(
                "image-to-text",
                model=model_name,
                trust_remote_code=True
            )

    def caption_image(self, image_path: str, prompt: str = None):
        """ä¸ºå•å¼ å›¾åƒç”Ÿæˆæè¿°"""
        if prompt is None:
            prompt = (
                "ä½ æ˜¯æ¾æçº¿è™«ç—…ä¸“å®¶ï¼Œè¯·è¯¦ç»†æè¿°å›¾åƒä¸­çš„å…³é”®å¯¹è±¡ã€"
                "åœºæ™¯ã€æ–‡å­—ä¸ç»Ÿè®¡ä¿¡æ¯ï¼Œçªå‡ºä¸æ¾æçº¿è™«ç—…ç›¸å…³çš„çŸ¥è¯†ç‚¹ã€‚"
            )

        if self.provider == "transformers":
            result = self._pipeline(image_path, prompt=prompt)
            return result[0]["generated_text"]

        elif self.provider == "ollama":
            # è°ƒç”¨ Ollama VLM API
            return self._caption_with_ollama(image_path, prompt)
```

#### 5.3 æ–‡æœ¬æµåˆå¹¶

**æè¿°æ’å…¥ç­–ç•¥**ï¼š

```python
def merge_image_captions(text: str, images: List[Dict], captions: Dict[str, str]):
    """å°†å›¾ç‰‡æè¿°æ’å…¥åˆ°åŸæ–‡ä¸­"""
    # æŒ‰é¡µç æ’åº
    images.sort(key=lambda x: x["page"])

    # åˆ†é¡µå¤„ç†
    pages = text.split("\n\n--- Page Break ---\n\n")

    for img in images:
        page_num = img["page"]
        caption = captions.get(img["path"], "")

        if caption and page_num < len(pages):
            # æ’å…¥æè¿°åˆ°é¡µé¢æœ«å°¾
            pages[page_num] += f"\n\n[å›¾ç‰‡æè¿°] {caption}\n"

    return "\n\n--- Page Break ---\n\n".join(pages)
```

#### 5.4 é…ç½®é€‰é¡¹

```yaml
# config/config.yaml
pdf:
  enable_image_captions: false # é»˜è®¤ç¦ç”¨ï¼ˆéœ€è¦GPUï¼‰
  image_output_dir: ./output/pdf_images
  max_images_per_pdf: 25
  caption_model: Qwen/Qwen2-VL-7B-Instruct
  caption_provider: transformers # or ollama
  caption_prompt: "ä½ æ˜¯æ¾æçº¿è™«ç—…ä¸“å®¶ï¼Œè¯·æè¿°å›¾ç‰‡..."
```

#### 5.5 æ€§èƒ½è€ƒè™‘

| æ¨¡å‹        | é€Ÿåº¦   | å†…å­˜ | è´¨é‡       |
| ----------- | ------ | ---- | ---------- |
| Qwen2-VL-7B | ~5s/å›¾ | 16GB | â­â­â­â­â­ |
| Ollama VLM  | ~3s/å›¾ | 8GB  | â­â­â­â­   |
| BLIP-2      | ~2s/å›¾ | 6GB  | â­â­â­     |

---

### ğŸ†• é˜¶æ®µ 6ï¼šv3.0 BGE-M3 æ··åˆæ£€ç´¢å»é‡

**æ ¸å¿ƒæ¨¡å—**ï¼š`concept_deduplicator.py`

#### 6.1 BGE-M3 æ¶æ„

**å¯†é›†+ç¨€ç–æ··åˆåµŒå…¥**ï¼š

```python
class BGE_M3_Embedder:
    """BGE-M3 embedding provider"""

    def __init__(self, model_name="BAAI/bge-m3", device=None):
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(model_name, device=device)
        self.model_name = model_name

    def embed(self, texts: List[str]) -> np.ndarray:
        """ç”Ÿæˆå¯†é›†å‘é‡ embeddings (1024ç»´)"""
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=True
        )
        return np.array(embeddings)

    def hybrid_similarity(self, text1: str, text2: str,
                         alpha: float = 0.7) -> float:
        """æ··åˆç›¸ä¼¼åº¦: alpha*dense + (1-alpha)*sparse"""
        # Dense similarity
        emb1 = self.embed([text1])
        emb2 = self.embed([text2])
        dense_sim = cosine_similarity(emb1, emb2)[0][0]

        # Sparse similarity (è¯çº§ Jaccard)
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        sparse_sim = len(words1 & words2) / len(words1 | words2) if words1 and words2 else 0

        return alpha * dense_sim + (1 - alpha) * sparse_sim
```

#### 6.2 ä¸­è‹±å®ä½“å¯¹é½

**å¯¹é½ç®—æ³•**ï¼š

```python
def align_bilingual_entities(concepts: List[str],
                             embedder: BGE_M3_Embedder,
                             threshold: float = 0.90):
    """ä¸­è‹±æ–‡å®ä½“è‡ªåŠ¨å¯¹é½"""
    # åˆ†ç¦»ä¸­è‹±æ–‡
    chinese = [c for c in concepts if contains_chinese(c)]
    english = [c for c in concepts if not contains_chinese(c)]

    # è®¡ç®—äº¤å‰ç›¸ä¼¼åº¦
    alignment = []
    for zh in chinese:
        best_match = None
        best_score = 0

        for en in english:
            score = embedder.hybrid_similarity(zh, en)
            if score > best_score and score >= threshold:
                best_score = score
                best_match = en

        if best_match:
            alignment.append({
                "chinese": zh,
                "english": best_match,
                "score": best_score
            })

    return alignment

# ç¤ºä¾‹ç»“æœ
# {"chinese": "æ¾æçº¿è™«", "english": "bursaphelenchus xylophilus", "score": 0.98}
# {"chinese": "é©¬å°¾æ¾", "english": "pinus massoniana", "score": 0.96}
```

#### 6.3 æ€§èƒ½å¯¹æ¯”

| Embedding æ¨¡å‹ | ç»´åº¦ | ä¸­è‹±å¯¹é½ | åŒä¹‰è¯å¬å› | é€Ÿåº¦           |
| -------------- | ---- | -------- | ---------- | -------------- |
| BGE-M3 (v3.0)  | 1024 | 100%     | 95%        | 30 concepts/s  |
| MiniLM (v1.0)  | 384  | 80%      | 85%        | 50 concepts/s  |
| TF-IDF         | 100  | 60%      | 70%        | 100 concepts/s |

---

### ğŸ†• é˜¶æ®µ 7ï¼šv3.0 Agentic Workflow æ™ºèƒ½å®¡æŸ¥

**æ ¸å¿ƒæ¨¡å—**ï¼š`bio_semantic_review.py`ã€`graph_summarizer.py`

#### 7.1 LLM å®¡ç¨¿äºº Agent

**äºŒæ¬¡æ ¡éªŒé€»è¾‘**ï¼š

```python
def _llm_decide(s: str, rel: str, t: str,
                s_type: str, t_type: str, weight: float) -> bool:
    """LLM åˆ¤æ–­ä¸‰å…ƒç»„æ˜¯å¦åˆç†"""

    prompt = f"""
ä½ æ˜¯æ¾æçº¿è™«ç—…é¢†åŸŸä¸“å®¶ã€‚åˆ¤æ–­ä»¥ä¸‹çŸ¥è¯†ä¸‰å…ƒç»„æ˜¯å¦åˆç†ï¼Œåªå›ç­” Yes æˆ– Noã€‚

ã€å®ä½“1ã€‘{s} (ç±»å‹: {s_type})
ã€å…³ç³»ã€‘ {rel}
ã€å®ä½“2ã€‘{t} (ç±»å‹: {t_type})
ã€ç½®ä¿¡åº¦ã€‘{weight:.2f}

åˆ¤æ–­ä¾æ®:
1. ç”Ÿç‰©å­¦é€»è¾‘æ˜¯å¦æ­£ç¡®
2. å®ä½“ç±»å‹ä¸å…³ç³»æ˜¯å¦åŒ¹é…
3. æ˜¯å¦ç¬¦åˆæ¾æçº¿è™«ç—…çš„ä¸“ä¸šçŸ¥è¯†

åªå›ç­” Yes æˆ– Noï¼š
"""

    response = ollama_api(prompt, model="qwen2.5-coder:14b")
    decision = response.strip().lower()

    if decision in {"yes", "æ˜¯", "åŒæ„", "correct"}:
        return True
    elif decision in {"no", "å¦", "ä¸åŒæ„", "incorrect"}:
        return False
    else:
        return True  # ä¿å®ˆç­–ç•¥ï¼šé»˜è®¤ä¿ç•™
```

**åº”ç”¨åœºæ™¯**ï¼š

```python
# åœ¨ bio_semantic_review.py ä¸­åº”ç”¨
for triple in triples:
    confidence = float(triple["weight"])

    # åªå®¡æŸ¥ç½®ä¿¡åº¦ 0.6-0.8 çš„ä¸‰å…ƒç»„
    if 0.6 <= confidence <= 0.8:
        keep = _llm_decide(
            triple["node_1"],
            triple["relation"],
            triple["node_2"],
            triple["node_1_type"],
            triple["node_2_type"],
            confidence
        )

        if not keep:
            issues.append({
                "triple": triple,
                "action": "llm_reject",
                "reason": "LLM å®¡æŸ¥æœªé€šè¿‡"
            })
            continue

    # é€šè¿‡å®¡æŸ¥çš„ä¸‰å…ƒç»„
    cleaned_triples.append(triple)
```

#### 7.2 GraphRAG ç¤¾åŒºæ‘˜è¦

**ç¤¾åŒºæ£€æµ‹**ï¼š

```python
def detect_communities(driver, algorithm="louvain"):
    """ä½¿ç”¨ Neo4j GDS è¿›è¡Œç¤¾åŒºæ£€æµ‹"""
    with driver.session() as session:
        # 1. åˆ›å»ºå›¾æŠ•å½±
        session.run("""
            CALL gds.graph.project(
              'pwd_graph',
              'Concept',
              {RELATIONSHIP: {orientation: 'UNDIRECTED'}}
            )
        """)

        # 2. è¿è¡Œç¤¾åŒºæ£€æµ‹
        if algorithm == "louvain":
            session.run("""
                CALL gds.louvain.write('pwd_graph', {
                    writeProperty: 'communityId'
                })
            """)

        # 3. è·å–ç¤¾åŒºæˆå‘˜
        result = session.run("""
            MATCH (n:Concept)
            RETURN n.communityId AS communityId,
                   collect({name: n.name, type: n.type}) AS members
            GROUP BY n.communityId
        """)

        return list(result)
```

**æ‘˜è¦ç”Ÿæˆ**ï¼š

```python
def generate_community_summary(community_id: int, members: List[Dict]):
    """ä¸ºç¤¾åŒºç”Ÿæˆä¸»é¢˜æ‘˜è¦"""
    # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
    type_counts = {}
    for m in members:
        t = m.get("type", "Other")
        type_counts[t] = type_counts.get(t, 0) + 1

    # æå–ä»£è¡¨èŠ‚ç‚¹
    top_nodes = [m["name"] for m in members[:20]]

    # LLM ç”Ÿæˆæ‘˜è¦
    prompt = f"""
ç¤¾åŒºID: {community_id}
èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {type_counts}
ä»£è¡¨èŠ‚ç‚¹: {", ".join(top_nodes)}

è¯·ç»™å‡º:
1) ä¸»é¢˜æ ‡é¢˜(ä¸è¶…è¿‡20å­—)
2) ç¤¾åŒºæ‘˜è¦(150-250å­—)
3) 3-6ä¸ªå…³é”®è¯

ä»…è¿”å›JSON: {{"title":"...", "summary":"...", "keywords":[...]}}
"""

    response = ollama_api(prompt, model="qwen2.5-coder:14b")
    summary = json.loads(response)

    return {
        "communityId": community_id,
        "title": summary["title"],
        "summary": summary["summary"],
        "keywords": summary["keywords"],
        "memberCount": len(members)
    }
```

**å†™å…¥ Neo4j**ï¼š

```python
def create_theme_nodes(driver, summaries: List[Dict]):
    """åˆ›å»ºä¸»é¢˜èŠ‚ç‚¹"""
    with driver.session() as session:
        for s in summaries:
            session.run("""
                MERGE (t:Theme {communityId: $communityId})
                SET t.title = $title,
                    t.summary = $summary,
                    t.keywords = $keywords,
                    t.memberCount = $memberCount
            """, **s)

            # è¿æ¥ç¤¾åŒºæˆå‘˜åˆ°ä¸»é¢˜
            session.run("""
                MATCH (t:Theme {communityId: $communityId})
                MATCH (n:Concept {communityId: $communityId})
                MERGE (n)-[:BELONGS_TO]->(t)
            """, communityId=s["communityId"])
```

#### 7.3 å®Œæ•´ Agentic æµç¨‹

```
1. PDF æå– â†’ æ–‡æœ¬å—
   â†“
2. LLM æ¦‚å¿µæŠ½å– â†’ åŸå§‹ä¸‰å…ƒç»„
   â†“
3. ç½®ä¿¡åº¦åˆ†ç±»:
   - > 0.8: ç›´æ¥é€šè¿‡
   - 0.6-0.8: LLM å®¡ç¨¿äººäºŒæ¬¡åˆ¤æ–­
   - < 0.6: è¿‡æ»¤
   â†“
4. å¯¼å…¥ Neo4j
   â†“
5. ç¤¾åŒºæ£€æµ‹ (Louvain)
   â†“
6. LLM ç”Ÿæˆç¤¾åŒºæ‘˜è¦
   â†“
7. åˆ›å»º Theme èŠ‚ç‚¹
   â†“
8. æœ€ç»ˆçŸ¥è¯†å›¾è°±
   - Concept èŠ‚ç‚¹
   - Relationship è¾¹
   - Theme èŠ‚ç‚¹ (æ–°å¢)
```

#### 7.4 é…ç½®é€‰é¡¹

```yaml
# config/config.yaml
agentic:
  # LLM å®¡ç¨¿äºº
  enable_llm_review: false # é»˜è®¤ç¦ç”¨ï¼ˆè€—æ—¶ï¼‰
  review_confidence_range: [0.6, 0.8]
  review_model: qwen2.5-coder:14b

  # GraphRAG
  enable_graph_rag: false # éœ€è¦ Neo4j GDS
  community_algorithm: louvain # louvain or leiden
  summary_model: qwen2.5-coder:14b
  min_community_size: 5 # æœ€å°ç¤¾åŒºè§„æ¨¡
```

---

## çŸ¥è¯†å›¾è°±è®¾è®¡

### å®ä½“ç±»å‹ï¼ˆæ¦‚å¿µå±‚é¢ï¼‰

ä¸‹è¡¨ä¸ºå›¾è°±ä¸­å¸¸è§çš„å®ä½“ç±»å‹åŠç¤ºä¾‹ï¼š

| ç±»å‹       | è¯´æ˜       | ç¤ºä¾‹                         |
| ---------- | ---------- | ---------------------------- |
| Disease    | ç–¾ç—…       | pine wilt disease            |
| Pathogen   | ç—…åŸä½“     | bursaphelenchus xylophilus   |
| Host       | å¯„ä¸»       | pinus thunbergiiã€é©¬å°¾æ¾     |
| Vector     | åª’ä»‹       | monochamus alternatus ç­‰å¤©ç‰› |
| Symptom    | ç—‡çŠ¶       | å¶ç‰‡å˜è‰²ã€è½å¶               |
| Control    | é˜²æ²»æªæ–½   | è¯±æ•å™¨ã€ç”Ÿç‰©é˜²æ²»ã€é˜²æ²»       |
| Technology | æŠ€æœ¯ä¸æ–¹æ³• | Sentinel-2ã€é«˜å…‰è°±æ•°æ®       |
| Location   | åœ°ç‚¹       | æ³°å±±é£æ™¯åŒºã€å·´å±±ã€ç–«åŒº       |
| Other      | å…¶ä»–æ¦‚å¿µ   | æ—ä¸šã€å…‰è°±ã€æ³¢æ®µé€‰æ‹©ç®—æ³•ç­‰   |

ä¸åŒè„šæœ¬å’Œå¯¼å…¥æ–¹å¼ä¸‹ï¼Œå…·ä½“çš„æ ‡ç­¾å‘½åä¼šç•¥æœ‰å·®å¼‚ï¼Œä½†æ•´ä½“è®¾è®¡å›´ç»•ä¸Šè¿°å‡ ç±»ã€‚

### å…³ç³»ç±»å‹ï¼ˆè¯­ä¹‰å±‚é¢ï¼‰

åœ¨æœ€ç»ˆå›¾è°±ä¸­ï¼Œé™¤äº†å…±ç°å…³ç³»å¤–ï¼Œè¿˜åŒ…å«å¤šç±»è¯­ä¹‰å…³ç³»ï¼Œä¾‹å¦‚ï¼š

- `PARASITIZES`ï¼ˆå¯„ç”Ÿï¼‰ï¼šåª’ä»‹æˆ–ç—…åŸä½“å¯„ç”Ÿåœ¨å¯„ä¸»ä¸Š
- `INFECTS`ï¼ˆæ„ŸæŸ“ï¼‰ï¼šç—…åŸä½“å¯¹å¯„ä¸»çš„æ„ŸæŸ“å…³ç³»
- `CAUSES` / `SYMPTOM`ï¼ˆå¼•èµ· / ç—‡çŠ¶ï¼‰ï¼šç–¾ç—…ä¸ç—‡çŠ¶ä¹‹é—´çš„è”ç³»
- `TRANSMITS`ï¼ˆä¼ æ’­ï¼‰ï¼šåª’ä»‹ä¼ æ’­ç—…åŸä½“æˆ–ç–¾ç—…
- `DISTRIBUTED_IN`ï¼ˆåˆ†å¸ƒäºï¼‰ï¼šç–¾ç—…æˆ–åª’ä»‹åœ¨åœ°åŒºä¸Šçš„åˆ†å¸ƒ
- `AFFECTS`ï¼ˆå½±å“ï¼‰ï¼šç¯å¢ƒæˆ–æŠ€æœ¯å› ç´ å¯¹ç—…å®³çš„å½±å“
- `TREATS` / `CONTROLS`ï¼ˆæ²»ç–— / é˜²æ²»ï¼‰ï¼šé˜²æ²»æªæ–½ä¸ç—…å®³æˆ–ç—…åŸä½“ä¹‹é—´çš„å…³ç³»
- `USED_FOR` / `MONITORS`ï¼ˆç”¨äº / ç›‘æµ‹ï¼‰ï¼šæŠ€æœ¯ä¸ç›‘æµ‹ä»»åŠ¡ä¹‹é—´çš„å…³ç³»
- `CO_OCCURS_WITH`ï¼ˆå…±ç°ï¼‰ï¼šæ–‡çŒ®ä¸­å…±åŒå‡ºç°çš„æ¦‚å¿µï¼Œç”¨äºè¡¥å……èƒŒæ™¯è¿æ¥

---

## ç›®å½•ç»“æ„ä¸æ ¸å¿ƒè„šæœ¬

é¡¹ç›®æ ¹ç›®å½•çš„ä¸»è¦ç»“æ„å¦‚ä¸‹ï¼ˆç®€åŒ–ï¼‰ï¼š

```text
PWD/
â”œâ”€â”€ README.md                  # é¡¹ç›®è¯´æ˜ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â”œâ”€â”€ requirements.txt           # Python ä¾èµ–
â”œâ”€â”€ .gitignore                 # Git å¿½ç•¥è§„åˆ™
â”‚
â”œâ”€â”€ docs/                      # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.txt  # é¡¹ç›®ç»“æ„è¯´æ˜
â”‚   â””â”€â”€ PWD_Knowledge_Graph_Analysis.html  # åˆ†ææŠ¥å‘ŠHTMLç‰ˆæœ¬
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter Notebooks
â”‚   â”œâ”€â”€ PWD_Knowledge_Graph_Analysis.ipynb  # ä¸»åˆ†æç¬”è®°æœ¬
â”‚   â””â”€â”€ PWD_KG_Notebook.ipynb  # çŸ¥è¯†å›¾è°±ç¬”è®°æœ¬
â”‚
â”œâ”€â”€ æ ¸å¿ƒè„šæœ¬ï¼ˆä¸»æµç¨‹ï¼‰
â”‚   â”œâ”€â”€ main.py                # ä¸»å…¥å£ï¼Œæ•´åˆå¢å¼ºç®¡é“ä¸ Neo4j ç®¡ç†
â”‚   â”œâ”€â”€ enhanced_pipeline.py   # LLM æ¦‚å¿µä¸å…³ç³»æŠ½å–ç®¡é“
â”‚   â”œâ”€â”€ concept_extractor.py   # æ¦‚å¿µä¸å…³ç³»æŠ½å–
â”‚   â”œâ”€â”€ concept_deduplicator.py # åµŒå…¥å¼å»é‡ä¸åˆå¹¶
â”‚   â”œâ”€â”€ data_cleaner.py        # æ•°æ®æ¸…æ´—ä¸è§„èŒƒåŒ–
â”‚   â”œâ”€â”€ neo4j_generator.py     # ç”Ÿæˆ Neo4j å¯¼å…¥æ–‡ä»¶
â”‚   â”œâ”€â”€ neo4j_manager.py       # Neo4j å¤‡ä»½ã€æ¸…ç©ºä¸å›æ»š
â”‚   â”œâ”€â”€ pdf_extractor.py       # PDF æ–‡æœ¬æå–
â”‚   â”œâ”€â”€ ocr_processor.py       # OCR å¤„ç†
â”‚   â”œâ”€â”€ entity_linker.py       # å®ä½“é“¾æ¥
â”‚   â”œâ”€â”€ parallel_processor.py  # å¹¶è¡Œå¤„ç†
â”‚   â”œâ”€â”€ bio_semantic_review.py # ä¸‰å…ƒç»„è¯­ä¹‰ä½“æ£€
â”‚   â””â”€â”€ import_to_neo4j_final.py # ä½¿ç”¨ä¸‰å…ƒç»„å¯¼å…¥æœ€ç»ˆå›¾è°±
â”‚
â”œâ”€â”€ scripts/                   # è¾…åŠ©è„šæœ¬
â”‚   â”œâ”€â”€ workflow/              # å·¥ä½œæµè„šæœ¬
â”‚   â”‚   â”œâ”€â”€ run_complete_workflow.sh  # ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹
â”‚   â”‚   â”œâ”€â”€ check_progress.sh  # è¿è¡Œè¿›åº¦æ£€æŸ¥
â”‚   â”‚   â”œâ”€â”€ clean_project.sh   # è¾“å‡ºä¸ç¼“å­˜æ¸…ç†
â”‚   â”‚   â””â”€â”€ organize_project.sh # é¡¹ç›®æ–‡ä»¶æ•´ç†
â”‚   â””â”€â”€ utils/                 # å·¥å…·è„šæœ¬
â”‚       â”œâ”€â”€ export_for_review.py  # å¯¼å‡ºå®¡æŸ¥æ–‡ä»¶
â”‚       â”œâ”€â”€ export_triples.py  # å¯¼å‡ºä¸‰å…ƒç»„
â”‚       â”œâ”€â”€ export_neo4j_to_csv.py # ä»æ•°æ®åº“å¯¼å‡º CSV
â”‚       â”œâ”€â”€ auto_disambiguate.py # è‡ªåŠ¨æ¶ˆæ­§
â”‚       â”œâ”€â”€ cache_manager.py   # ç¼“å­˜ç®¡ç†
â”‚       â”œâ”€â”€ config_loader.py   # é…ç½®åŠ è½½
â”‚       â”œâ”€â”€ logger_config.py   # æ—¥å¿—é…ç½®
â”‚       â””â”€â”€ visualize_neo4j_graph.py # Neo4j å›¾å¯è§†åŒ–
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml            # ä¸»é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ domain_dict.json       # é¢†åŸŸè¯å…¸
â”‚   â””â”€â”€ stopwords.txt          # åœç”¨è¯
â”‚
â”œâ”€â”€ output/                    # è¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ concepts*.csv          # æ¦‚å¿µç›¸å…³ä¸­é—´ç»“æœ
â”‚   â”œâ”€â”€ relationships*.csv     # å…³ç³»ç›¸å…³ä¸­é—´ç»“æœ
â”‚   â”œâ”€â”€ entities_clean.csv     # æ¸…æ´—åå®ä½“
â”‚   â”œâ”€â”€ relations_clean.csv    # æ¸…æ´—åå…³ç³»
â”‚   â”œâ”€â”€ neo4j_import/          # Neo4j å¯¼å…¥æ–‡ä»¶ä¸è„šæœ¬
â”‚   â”œâ”€â”€ triples/               # ä¸‰å…ƒç»„ç›¸å…³ä¸­é—´ç»“æœ
â”‚   â”œâ”€â”€ statistics_report.txt  # æŠ½å–/æ¸…æ´—é˜¶æ®µç»Ÿè®¡
â”‚   â””â”€â”€ *.md/*.json            # æ•°æ®æ£€æŸ¥ä¸å¯¼å…¥æŠ¥å‘Š
â”‚
â”œâ”€â”€ archive/                   # å¼€å‘è¿‡ç¨‹å­˜æ¡£
â”‚   â”œâ”€â”€ scripts/               # è°ƒè¯•å’Œä¸­é—´ç‰ˆæœ¬è„šæœ¬
â”‚   â””â”€â”€ docs/                  # æ—§æ–‡æ¡£å’ŒæŠ¥å‘Š
â”‚
â”œâ”€â”€ æ–‡çŒ®/                      # PDF æ–‡çŒ®ç›®å½•
â””â”€â”€ venv/                      # è™šæ‹Ÿç¯å¢ƒï¼ˆä¸çº³å…¥ç‰ˆæœ¬æ§åˆ¶ï¼‰
```

æ›´ç»†è‡´çš„è¯´æ˜å¯å‚è€ƒ `docs/PROJECT_STRUCTURE.txt`ã€‚

---

## æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶è¯¦è§£

### é…ç½®ç®¡ç†ç³»ç»Ÿ

**é…ç½®æ–‡ä»¶**ï¼š`config/config.yaml`ã€`config_loader.py`

#### é…ç½®æ¶æ„

```yaml
# PDF æå–é…ç½®
pdf:
  input_directory: ./æ–‡çŒ®
  output_directory: ./output/extracted_texts
  enable_cache: true # å¯ç”¨ MD5 ç¼“å­˜
  cache_directory: ./cache/pdf
  parallel_workers: 8 # å¹¶è¡Œè¿›ç¨‹æ•°
  enable_ocr: false # OCR æŒ‰éœ€å¯ç”¨
  ocr_engine: tesseract # tesseract | paddle

# å®ä½“è¯†åˆ«é…ç½®
entity:
  enable_tfidf: true # TF-IDF å…³é”®è¯æå–
  enable_yake: true # YAKE ç®—æ³•
  enable_keybert: true # KeyBERTï¼ˆåŸºäº BERTï¼‰
  enable_spacy: true # spaCy NER
  domain_dict_file: ./config/domain_dict.json
  min_frequency: 2 # æœ€å°è¯é¢‘

# å…³ç³»æŠ½å–é…ç½®
relation:
  enable_pattern_matching: true # æ¨¡å¼åŒ¹é…
  enable_cooccurrence: true # å…±ç°åˆ†æ
  window_size: 100 # å…±ç°çª—å£å¤§å°
  min_cooccurrence: 2 # æœ€å°å…±ç°æ¬¡æ•°

# æ•°æ®æ¸…æ´—é…ç½®
cleaning:
  confidence_threshold: 0.65 # ç½®ä¿¡åº¦é˜ˆå€¼
  similarity_threshold: 0.85 # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
  enable_entity_linking: true # å®ä½“é“¾æ¥
  min_length: 2 # æœ€å°å®ä½“é•¿åº¦
  max_length: 50 # æœ€å¤§å®ä½“é•¿åº¦

# Neo4j æ•°æ®åº“é…ç½®
neo4j:
  uri: neo4j://127.0.0.1:7687 # æœ¬åœ°å®ä¾‹
  user: neo4j
  password: "12345678"
  database: PWD # æ•°æ®åº“å
  enable_backup: true # è‡ªåŠ¨å¤‡ä»½
  backup_directory: ./backups

# LLM é…ç½®
llm:
  provider: ollama # ollama | openai
  model: llama3.2:3b # æ¨¡å‹åç§°
  ollama_host: http://localhost:11434
  max_chunks: 100 # æœ€å¤§å¤„ç†å—æ•°
  chunk_size: 2000 # å—å¤§å°ï¼ˆå­—ç¬¦ï¼‰
  chunk_overlap: 200 # å—é‡å 
  temperature: 0.1 # ç”Ÿæˆæ¸©åº¦
  timeout: 120 # API è¶…æ—¶ï¼ˆç§’ï¼‰

# å»é‡é…ç½®
deduplication:
  similarity_threshold: 0.85 # æ¦‚å¿µå»é‡é˜ˆå€¼
  embedding_model: sentence-transformers/paraphrase-MiniLM-L6-v2
  use_clustering: false # ä½¿ç”¨èšç±»ç®—æ³•

# è¿‡æ»¤é…ç½®
filtering:
  min_importance: 1 # æœ€å°é‡è¦æ€§
  min_connections: 0 # æœ€å°è¿æ¥æ•°

# ç³»ç»Ÿé…ç½®
system:
  enable_cache: true # å…¨å±€ç¼“å­˜å¼€å…³
  enable_parallel: true # å¹¶è¡Œå¤„ç†
  log_level: INFO # DEBUG | INFO | WARNING
  max_workers: 8 # æœ€å¤§å·¥ä½œè¿›ç¨‹
```

#### é…ç½®åŠ è½½æœºåˆ¶

```python
# config_loader.py å®ç°
import yaml
from pathlib import Path
from typing import Dict, Any

class ConfigLoader:
    """é…ç½®åŠ è½½å™¨ï¼Œæ”¯æŒå¤šå±‚çº§é…ç½®åˆå¹¶"""

    def __init__(self, config_path: str = "config/config.yaml"):
        self.config_path = Path(config_path)
        self._config = None
        self._load_config()

    def _load_config(self):
        """åŠ è½½å¹¶éªŒè¯é…ç½®"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self._config = yaml.safe_load(f)

        # é»˜è®¤å€¼å¡«å……
        self._apply_defaults()

        # é…ç½®éªŒè¯
        self._validate_config()

    def get(self, key: str, default: Any = None) -> Any:
        """
        è·å–é…ç½®å€¼ï¼Œæ”¯æŒç‚¹å·è·¯å¾„

        Example:
            config.get('llm.model')  # 'llama3.2:3b'
            config.get('pdf.enable_cache')  # True
        """
        keys = key.split('.')
        value = self._config

        for k in keys:
            if isinstance(value, dict):
                value = value.get(k)
            else:
                return default

        return value if value is not None else default
```

#### é…ç½®ä¼˜åŒ–ç­–ç•¥

```python
# åœºæ™¯ 1: å°è§„æ¨¡æµ‹è¯•ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
test_config = {
    'llm.max_chunks': 10,           # ä»…å¤„ç† 10 ä¸ªå—
    'pdf.parallel_workers': 4,      # å‡å°‘å¹¶è¡Œåº¦
    'filtering.min_importance': 3,  # åªä¿ç•™é‡è¦æ¦‚å¿µ
}

# åœºæ™¯ 2: é«˜è´¨é‡ç”Ÿäº§ï¼ˆå‡†ç¡®ç‡ä¼˜å…ˆï¼‰
production_config = {
    'llm.temperature': 0.0,         # ç¡®å®šæ€§è¾“å‡º
    'cleaning.confidence_threshold': 0.75,
    'deduplication.similarity_threshold': 0.90,
    'enable_entity_linking': True,
}

# åœºæ™¯ 3: é«˜å¬å›ï¼ˆè¦†ç›–ç‡ä¼˜å…ˆï¼‰
recall_config = {
    'cleaning.confidence_threshold': 0.5,
    'deduplication.similarity_threshold': 0.80,
    'filtering.min_importance': 0,
}

# åœºæ™¯ 4: å¤§è§„æ¨¡å¤„ç†ï¼ˆé€Ÿåº¦ä¼˜å…ˆï¼‰
speed_config = {
    'pdf.parallel_workers': 16,     # æœ€å¤§å¹¶è¡Œ
    'system.enable_cache': True,    # å¼ºåˆ¶ç¼“å­˜
    'llm.max_chunks': None,         # ä¸é™åˆ¶
    'enable_ocr': False,            # ç¦ç”¨ OCR
}
```

---

### ç¼“å­˜ç®¡ç†ç³»ç»Ÿ

**æ ¸å¿ƒæ¨¡å—**ï¼š`cache_manager.py`

#### ç¼“å­˜æ¶æ„

```python
class CacheManager:
    """å¤šçº§ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = "./cache"):
        self.cache_dir = Path(cache_dir)

        # ç¼“å­˜åˆ†ç±»
        self.pdf_cache_dir = self.cache_dir / "pdf"
        self.embedding_cache_dir = self.cache_dir / "embeddings"
        self.llm_cache_dir = self.cache_dir / "llm"

        # åˆ›å»ºç›®å½•
        for dir in [self.pdf_cache_dir,
                    self.embedding_cache_dir,
                    self.llm_cache_dir]:
            dir.mkdir(parents=True, exist_ok=True)

    def get_cache_key(self, data: Any, prefix: str = "") -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆMD5 hashï¼‰"""
        import hashlib

        if isinstance(data, (str, bytes)):
            content = data.encode() if isinstance(data, str) else data
        else:
            content = str(data).encode()

        hash_key = hashlib.md5(content).hexdigest()
        return f"{prefix}_{hash_key}" if prefix else hash_key
```

#### ç¼“å­˜ç­–ç•¥

| ç¼“å­˜ç±»å‹     | å­˜å‚¨å†…å®¹   | æœ‰æ•ˆæœŸ | å¤§å°é™åˆ¶ |
| ------------ | ---------- | ------ | -------- |
| **PDF ç¼“å­˜** | æå–çš„æ–‡æœ¬ | æ°¸ä¹…   | æ— é™åˆ¶   |
| **åµŒå…¥ç¼“å­˜** | æ¦‚å¿µå‘é‡   | æ°¸ä¹…   | 10GB     |
| **LLM ç¼“å­˜** | API å“åº”   | 7 å¤©   | 5GB      |
| **å¤„ç†ç¼“å­˜** | ä¸­é—´ç»“æœ   | 1 å¤©   | 1GB      |

#### ç¼“å­˜å¤±æ•ˆç­–ç•¥

```python
# 1. åŸºäºæ—¶é—´çš„å¤±æ•ˆ
if (current_time - cache_time) > TTL:
    invalidate_cache()

# 2. åŸºäºç‰ˆæœ¬çš„å¤±æ•ˆ
cache_version = "v1.0"
if cache.version != cache_version:
    invalidate_cache()

# 3. åŸºäºå†…å®¹çš„å¤±æ•ˆï¼ˆMD5 æ ¡éªŒï¼‰
if compute_md5(file) != cache.md5:
    invalidate_cache()

# 4. æ‰‹åŠ¨æ¸…ç†
./scripts/workflow/clean_project.sh  # æ¸…ç†æ‰€æœ‰ç¼“å­˜
```

---

### Neo4j æ•°æ®åº“ç®¡ç†

**æ ¸å¿ƒæ¨¡å—**ï¼š`neo4j_manager.py`ã€`import_to_neo4j_final.py`

#### æ•°æ®åº“æ¶æ„è®¾è®¡

```cypher
// èŠ‚ç‚¹æ¨¡å‹
(:Concept {
  name: String,              // æ¦‚å¿µåç§°ï¼ˆå”¯ä¸€ï¼‰
  type: String,              // ç±»å‹ï¼ˆHost/Pathogen/Vector...ï¼‰
  importance: Integer,       // é‡è¦æ€§ 1-5
  connections: Integer,      // è¿æ¥æ•°
  category: String,          // åˆ†ç±»
  source: String,            // æ¥æºæ–‡çŒ®
  created_at: DateTime       // åˆ›å»ºæ—¶é—´
})

// å…³ç³»æ¨¡å‹
()-[r:RELATION_TYPE {
  weight: Float,             // æƒé‡ 0-1
  confidence: Float,         // ç½®ä¿¡åº¦ 0-1
  source: String,            // æ¥æºï¼ˆllm/proximity/ruleï¼‰
  source_pdf: String,        // æ¥æºæ–‡çŒ®
  created_at: DateTime       // åˆ›å»ºæ—¶é—´
}]->()

// ç´¢å¼•å’Œçº¦æŸ
CREATE CONSTRAINT concept_name_unique
FOR (n:Concept) REQUIRE n.name IS UNIQUE;

CREATE INDEX concept_type_index
FOR (n:Concept) ON (n.type);

CREATE INDEX concept_importance_index
FOR (n:Concept) ON (n.importance);
```

#### æ‰¹é‡å¯¼å…¥ä¼˜åŒ–

```python
# ä½¿ç”¨äº‹åŠ¡æ‰¹å¤„ç†
BATCH_SIZE = 1000

def import_nodes_batch(nodes_df, driver):
    """æ‰¹é‡å¯¼å…¥èŠ‚ç‚¹"""
    with driver.session() as session:
        for i in range(0, len(nodes_df), BATCH_SIZE):
            batch = nodes_df[i:i+BATCH_SIZE]

            session.execute_write(
                lambda tx: tx.run("""
                    UNWIND $batch AS row
                    MERGE (n:Concept {name: row.name})
                    SET n.type = row.type,
                        n.importance = row.importance,
                        n.connections = row.connections
                """, batch=batch.to_dict('records'))
            )

# æ€§èƒ½å¯¹æ¯”
# é€æ¡æ’å…¥: 100 èŠ‚ç‚¹ â†’ 60 ç§’
# æ‰¹é‡å¯¼å…¥: 100 èŠ‚ç‚¹ â†’ 2 ç§’  (30x æé€Ÿ)
```

#### å¤‡ä»½ä¸æ¢å¤

```python
class Neo4jManager:
    """Neo4j æ•°æ®åº“ç®¡ç†å™¨"""

    def backup_database(self, backup_dir: str = "./backups"):
        """å¯¼å‡ºæ•°æ®åº“ä¸º Cypher è„šæœ¬"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = f"{backup_dir}/backup_{timestamp}.cypher"

        with self.driver.session() as session:
            # å¯¼å‡ºèŠ‚ç‚¹
            nodes = session.run("MATCH (n) RETURN n")

            # å¯¼å‡ºå…³ç³»
            rels = session.run("MATCH ()-[r]->() RETURN r")

            # ç”Ÿæˆ Cypher è„šæœ¬
            with open(backup_file, 'w') as f:
                # ... å†™å…¥ CREATE è¯­å¥

        return backup_file

    def restore_from_backup(self, backup_file: str):
        """ä»å¤‡ä»½æ¢å¤æ•°æ®åº“"""
        with self.driver.session() as session:
            # æ¸…ç©ºæ•°æ®åº“
            session.run("MATCH (n) DETACH DELETE n")

            # æ‰§è¡Œå¤‡ä»½è„šæœ¬
            with open(backup_file, 'r') as f:
                cypher_script = f.read()
                session.run(cypher_script)
```

---

### æ—¥å¿—ç³»ç»Ÿ

**æ ¸å¿ƒæ¨¡å—**ï¼š`logger_config.py`

#### æ—¥å¿—æ¶æ„

```python
import logging
from logging.handlers import RotatingFileHandler

def setup_logger(name: str, level: str = "INFO"):
    """é…ç½®åˆ†å±‚æ—¥å¿—ç³»ç»Ÿ"""

    logger = logging.getLogger(name)
    logger.setLevel(getattr(logging, level))

    # æ§åˆ¶å° Handlerï¼ˆå½©è‰²è¾“å‡ºï¼‰
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(ColoredFormatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    ))

    # æ–‡ä»¶ Handlerï¼ˆæ»šåŠ¨æ—¥å¿—ï¼‰
    file_handler = RotatingFileHandler(
        f'logs/{name}.log',
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    ))

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    return logger
```

#### æ—¥å¿—çº§åˆ«ä½¿ç”¨

```python
# DEBUG: è°ƒè¯•ä¿¡æ¯ï¼ˆè¯¦ç»†çš„å˜é‡å€¼ï¼‰
logger.debug(f"Embedding shape: {embeddings.shape}")

# INFO: è¿›åº¦ä¿¡æ¯ï¼ˆç”¨æˆ·å…³å¿ƒçš„äº‹ä»¶ï¼‰
logger.info(f"Processed {i}/{total} chunks")

# WARNING: è­¦å‘Šï¼ˆä¸å½±å“æµç¨‹ä½†éœ€æ³¨æ„ï¼‰
logger.warning(f"Low confidence: {confidence:.2f}")

# ERROR: é”™è¯¯ï¼ˆå½±å“åŠŸèƒ½ä½†å¯æ¢å¤ï¼‰
logger.error(f"Failed to parse JSON: {e}")

# CRITICAL: ä¸¥é‡é”™è¯¯ï¼ˆç¨‹åºæ— æ³•ç»§ç»­ï¼‰
logger.critical(f"Database connection lost")
```

---

### å¹¶è¡Œå¤„ç†æ¡†æ¶

**æ ¸å¿ƒæ¨¡å—**ï¼š`parallel_processor.py`

#### å¹¶è¡Œç­–ç•¥

```python
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor

class ParallelProcessor:
    """å¹¶è¡Œå¤„ç†æ¡†æ¶"""

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or cpu_count()

    def map(self, func, items, chunksize=1):
        """å¹¶è¡Œæ˜ å°„"""
        with Pool(self.max_workers) as pool:
            return pool.map(func, items, chunksize=chunksize)

    def starmap(self, func, items):
        """å¹¶è¡Œæ˜ å°„ï¼ˆå¤šå‚æ•°ï¼‰"""
        with Pool(self.max_workers) as pool:
            return pool.starmap(func, items)

# ä½¿ç”¨ç¤ºä¾‹
processor = ParallelProcessor(max_workers=8)

# PDF æå–å¹¶è¡ŒåŒ–
pdf_files = glob.glob("æ–‡çŒ®/*.pdf")
texts = processor.map(extract_pdf, pdf_files)

# LLM æ¨ç†å¹¶è¡ŒåŒ–ï¼ˆè°¨æ…ä½¿ç”¨ï¼Œå¯èƒ½è¶…å‡º API é™åˆ¶ï¼‰
chunks = split_into_chunks(text)
results = processor.map(llm_extract, chunks)
```

#### æ€§èƒ½è°ƒä¼˜

| åœºæ™¯     | å»ºè®®è¿›ç¨‹æ•°     | è¯´æ˜     |
| -------- | -------------- | -------- |
| PDF æå– | CPU æ ¸å¿ƒæ•°     | I/O å¯†é›† |
| æ–‡æœ¬å¤„ç† | CPU æ ¸å¿ƒæ•° Ã— 2 | è®¡ç®—å¯†é›† |
| LLM è°ƒç”¨ | 2-4            | API é™æµ |
| åµŒå…¥è®¡ç®— | CPU æ ¸å¿ƒæ•°     | å†…å­˜å¯†é›† |

---

### é”™è¯¯å¤„ç†ä¸é‡è¯•

#### é‡è¯•è£…é¥°å™¨

```python
from functools import wraps
import time

def retry(max_attempts=3, delay=1, backoff=2):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            attempt = 0
            current_delay = delay

            while attempt < max_attempts:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    attempt += 1
                    if attempt >= max_attempts:
                        raise

                    logger.warning(
                        f"Attempt {attempt} failed: {e}. "
                        f"Retrying in {current_delay}s..."
                    )
                    time.sleep(current_delay)
                    current_delay *= backoff

        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@retry(max_attempts=3, delay=2, backoff=2)
def call_llm_api(text):
    response = requests.post(api_url, json={'text': text})
    response.raise_for_status()
    return response.json()
```

---

## é…ç½®è°ƒä¼˜å»ºè®®

### åœºæ™¯åŒ–é…ç½®

```python
# åœºæ™¯ 1: å¼€å‘è°ƒè¯•
DEBUG_CONFIG = {
    'llm.max_chunks': 5,
    'pdf.parallel_workers': 2,
    'system.log_level': 'DEBUG',
}

# åœºæ™¯ 2: ç”Ÿäº§ç¯å¢ƒ
PRODUCTION_CONFIG = {
    'llm.max_chunks': None,
    'pdf.parallel_workers': 16,
    'cleaning.confidence_threshold': 0.75,
    'system.log_level': 'INFO',
}

# åœºæ™¯ 3: ä½èµ„æºç¯å¢ƒ
LOW_RESOURCE_CONFIG = {
    'pdf.parallel_workers': 2,
    'system.enable_parallel': False,
    'deduplication.embedding_model': 'tfidf',  # ä½¿ç”¨è½»é‡æ¨¡å‹
}
```

### æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

- [ ] å¯ç”¨ç¼“å­˜æœºåˆ¶
- [ ] è°ƒæ•´å¹¶è¡Œè¿›ç¨‹æ•°
- [ ] ä¼˜åŒ– LLM è°ƒç”¨æ‰¹æ¬¡
- [ ] ä½¿ç”¨é€‚å½“çš„åµŒå…¥æ¨¡å‹
- [ ] å®šæœŸæ¸…ç†æ—¥å¿—å’Œç¼“å­˜
- [ ] ç›‘æ§å†…å­˜ä½¿ç”¨
- [ ] è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´

---

## Neo4j ä½¿ç”¨ä¸åˆ†æ

- åŸºæœ¬è¿æ¥ä¿¡æ¯å’Œå¸¸ç”¨æŸ¥è¯¢ç¤ºä¾‹è§ï¼š`NEO4J_USAGE_GUIDE.md`
- å¯¼å…¥å®Œæˆåï¼Œå¯åœ¨ Neo4j Browser ä¸­ï¼š
  - æŒ‰èŠ‚ç‚¹/å…³ç³»ç±»å‹æµè§ˆæ•´ä½“ç»“æ„
  - æŸ¥çœ‹åº¦æ•°æœ€é«˜çš„èŠ‚ç‚¹ã€æƒé‡è¾ƒé«˜çš„å…³ç³»
  - é€šè¿‡æœ€çŸ­è·¯å¾„å’Œå­å›¾æŸ¥è¯¢åˆ†æä¼ æ’­é“¾è·¯

å…¸å‹æŸ¥è¯¢ç¤ºä¾‹ï¼ˆèŠ‚é€‰ï¼‰ï¼š

```cypher
// æŸ¥çœ‹èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
MATCH (n)
RETURN n.type AS node_type, count(*) AS count
ORDER BY count DESC;

// æŸ¥çœ‹å…³ç³»ç±»å‹åˆ†å¸ƒ
MATCH ()-[r]->()
RETURN type(r) AS rel_type, count(*) AS count
ORDER BY count DESC;

// æŸ¥è¯¢ç—…åŸä½“åˆ°å¯„ä¸»çš„ä¼ æ’­è·¯å¾„
MATCH path = (p:Pathogen)-[*1..4]-(h:Host)
RETURN p.name, h.name, length(path) AS path_length
LIMIT 10;
```

---

## Neo4j å®æ—¶ç»Ÿè®¡ï¼ˆç¤ºä¾‹ï¼‰

ç»Ÿè®¡æ—¶é—´ï¼š2025-11-16ï¼ˆåŸºäºå½“å‰é»˜è®¤æ•°æ®åº“ï¼‰

æŸ¥è¯¢è¯­å¥ï¼š

```cypher
// èŠ‚ç‚¹å’Œå…³ç³»æ€»æ•°
MATCH (n) RETURN count(n) AS node_count;
MATCH ()-[r]->() RETURN count(r) AS rel_count;

// èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒï¼ˆæŒ‰ n.type æˆ–æ ‡ç­¾ï¼‰
MATCH (n)
RETURN coalesce(n.type, head(labels(n))) AS type, count(*) AS count
ORDER BY count DESC;

// å…³ç³»ç±»å‹åˆ†å¸ƒ
MATCH ()-[r]->()
RETURN type(r) AS type, count(*) AS count
ORDER BY count DESC;
```

å½“å‰ç»“æœå¿«ç…§ï¼š

- èŠ‚ç‚¹æ€»æ•°ï¼š59
- å…³ç³»æ€»æ•°ï¼š365

èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒï¼š

- Otherï¼š18
- Hostï¼š16
- Locationï¼š10
- Vectorï¼š5
- Technologyï¼š5
- Controlï¼š3
- Diseaseï¼š1
- Pathogenï¼š1

å…³ç³»ç±»å‹åˆ†å¸ƒï¼ˆæŒ‰æ¡æ•°ä»é«˜åˆ°ä½ï¼‰ï¼š

- CO_OCCURS_WITHï¼š299
- RELATED_TOï¼š12
- PARASITIZESï¼š6
- TREATSï¼š5ï¼ŒDISTRIBUTED_INï¼š5
- AFFECTSï¼š4
- TRANSMITS / INFECTS / FEEDS_ON / LOCATED_IN / USED_FOR / CONTAINS / SYMPTOM_OFï¼šå„ 3
- CARRIES / COMPARES_WITH / CONTROLS / CAUSES / APPLIES_TOï¼šå„ 2
- COMPETES_WITH / MONITORS / COMPONENT_OFï¼šå„ 1

---

## æ€§èƒ½ä¸æ³¨æ„äº‹é¡¹

- å¤„ç†è§„æ¨¡ï¼šå½“å‰é…ç½®ä¸‹ï¼Œå¤„ç†åå‡ ç¯‡ PDFï¼ˆçº¦å‡ å MBï¼‰åœ¨ä¸€å°æ™®é€šç¬”è®°æœ¬ä¸Šè€—æ—¶çº¦å‡ ååˆ†é’Ÿï¼Œä¾èµ–æœ¬åœ° LLM æ¨ç†é€Ÿåº¦
- è¿è¡Œè¿‡ç¨‹ä¸­ä¼šç”Ÿæˆè¾ƒå¤šä¸­é—´ CSV/JSON æ–‡ä»¶ï¼Œå»ºè®®å®šæœŸä½¿ç”¨ `scripts/workflow/clean_project.sh` æ¸…ç†
- LLM æŠ½å–ç»“æœéš¾å…åŒ…å«å™ªå£°å’Œè¾¹ç¼˜æ¦‚å¿µï¼Œæœ€ç»ˆå›¾è°±æ˜¯åœ¨å¤šè½®è¿‡æ»¤å’Œè¯­ä¹‰ä½“æ£€åå¾—åˆ°ï¼Œå…³é”®ç»“è®ºå»ºè®®ç»“åˆé¢†åŸŸçŸ¥è¯†å¤æ ¸

---

## è®¸å¯è¯åŠç”¨é€”

æœ¬é¡¹ç›®ä»…ç”¨äºè¯¾ç¨‹å­¦ä¹ å’Œå­¦æœ¯ç ”ç©¶ï¼Œä¸ç”¨äºç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ã€‚
