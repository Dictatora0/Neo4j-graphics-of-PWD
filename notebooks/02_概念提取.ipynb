{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¤– æ­¥éª¤ 2ï¼šLLM æ¦‚å¿µæå–\n",
        "\n",
        "è¿™ä¸ª notebook è´Ÿè´£ï¼š\n",
        "1. ä½¿ç”¨ OpenAI GPT ä»æ–‡æœ¬ä¸­æå–æ¦‚å¿µ\n",
        "2. è¯†åˆ«æ¦‚å¿µä¹‹é—´çš„å…³ç³»\n",
        "3. è¯„ä¼°æ¦‚å¿µé‡è¦æ€§\n",
        "4. ä¿å­˜æå–ç»“æœ\n",
        "\n",
        "**âš ï¸ æ³¨æ„**ï¼šæ­¤æ­¥éª¤ä¼šè°ƒç”¨ OpenAI APIï¼Œäº§ç”Ÿè´¹ç”¨ï¼\n",
        "**â±ï¸ é¢„è®¡æ—¶é—´**ï¼šå–å†³äºå—æ•°é‡ï¼Œ30å—çº¦éœ€ 5-10 åˆ†é’Ÿ\n",
        "**ğŸ’° é¢„è®¡è´¹ç”¨**ï¼š30å—çº¦ $1-3ï¼ˆä½¿ç”¨ gpt-3.5-turboï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. åŠ è½½ç¯å¢ƒå’Œé…ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import yaml\n",
        "import logging\n",
        "from typing import Dict, List, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import openai\n",
        "\n",
        "# åŠ è½½ç¯å¢ƒå˜é‡\n",
        "if os.path.exists('/content/PWD_Project/.env'):\n",
        "    with open('/content/PWD_Project/.env', 'r') as f:\n",
        "        for line in f:\n",
        "            if '=' in line:\n",
        "                key, value = line.strip().split('=', 1)\n",
        "                os.environ[key] = value\n",
        "\n",
        "PROJECT_ROOT = os.environ.get('PROJECT_ROOT', '/content/PWD_Project')\n",
        "CONFIG_PATH = os.environ.get('CONFIG_PATH', f'{PROJECT_ROOT}/config/config_colab.yaml')\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, f'{PROJECT_ROOT}/modules')\n",
        "\n",
        "# åŠ è½½é…ç½®\n",
        "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
        "    CONFIG = yaml.safe_load(f)\n",
        "\n",
        "# é…ç½®æ—¥å¿—\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('ConceptExtractor')\n",
        "\n",
        "print(f\"âœ… é…ç½®å·²åŠ è½½\")\n",
        "print(f\"   LLM æ¨¡å‹: {CONFIG['llm']['model']}\")\n",
        "print(f\"   æœ€å¤§å¤„ç†å—æ•°: {CONFIG['llm']['max_chunks']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. åŠ è½½ä¸Šä¸€æ­¥çš„æ–‡æœ¬å—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½æ–‡æœ¬å—\n",
        "output_dir = CONFIG['output']['base_directory']\n",
        "chunks_file = f\"{output_dir}/text_chunks.json\"\n",
        "\n",
        "if not os.path.exists(chunks_file):\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ°æ–‡æœ¬å—æ–‡ä»¶: {chunks_file}\")\n",
        "    print(\"   è¯·å…ˆè¿è¡Œ 01_PDFæå–.ipynb\")\n",
        "    raise FileNotFoundError(chunks_file)\n",
        "\n",
        "with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "print(f\"âœ… åŠ è½½äº† {len(chunks)} ä¸ªæ–‡æœ¬å—\")\n",
        "print(f\"\\nâš ï¸  å°†å¤„ç†å‰ {CONFIG['llm']['max_chunks']} ä¸ªå—ï¼ˆé…ç½®é™åˆ¶ï¼‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. æ¦‚å¿µæå–å™¨ï¼ˆOpenAI ç‰ˆæœ¬ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConceptExtractorOpenAI:\n",
        "    \"\"\"ä½¿ç”¨ OpenAI API è¿›è¡Œæ¦‚å¿µæå–\"\"\"\n",
        "    \n",
        "    def __init__(self, api_key: str, model: str = 'gpt-3.5-turbo', \n",
        "                 temperature: float = 0.1, timeout: int = 120):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.timeout = timeout\n",
        "        openai.api_key = api_key\n",
        "        \n",
        "        # ç»Ÿè®¡ä¿¡æ¯\n",
        "        self.stats = {\n",
        "            'total_requests': 0,\n",
        "            'successful_requests': 0,\n",
        "            'failed_requests': 0,\n",
        "            'total_concepts': 0,\n",
        "            'total_relationships': 0\n",
        "        }\n",
        "    \n",
        "    def extract_from_chunks(self, chunks: List[Dict], max_chunks: int = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"ä»æ–‡æœ¬å—æ‰¹é‡æå–æ¦‚å¿µå’Œå…³ç³»\"\"\"\n",
        "        if max_chunks:\n",
        "            chunks = chunks[:max_chunks]\n",
        "            logger.info(f\"é™åˆ¶å¤„ç† {max_chunks} ä¸ªæ–‡æœ¬å—\")\n",
        "        \n",
        "        all_concepts = []\n",
        "        all_relationships = []\n",
        "        \n",
        "        for chunk in tqdm(chunks, desc=\"ğŸ¤– LLM æ¦‚å¿µæå–\"):\n",
        "            try:\n",
        "                result = self._extract_from_text(chunk['text'], chunk['chunk_id'])\n",
        "                \n",
        "                # å¤„ç†æ¦‚å¿µ\n",
        "                for concept in result.get('concepts', []):\n",
        "                    all_concepts.append({\n",
        "                        'entity': concept['name'],\n",
        "                        'category': concept.get('category', 'unknown'),\n",
        "                        'importance': concept.get('importance', 1),\n",
        "                        'chunk_id': chunk['chunk_id'],\n",
        "                        'source_pdf': chunk['source_pdf']\n",
        "                    })\n",
        "                \n",
        "                # å¤„ç†å…³ç³»\n",
        "                for relation in result.get('relationships', []):\n",
        "                    all_relationships.append({\n",
        "                        'node_1': relation['head'],\n",
        "                        'node_2': relation['tail'],\n",
        "                        'edge': relation['relation'],\n",
        "                        'weight': relation.get('confidence', 0.8),\n",
        "                        'source': 'llm',\n",
        "                        'chunk_id': chunk['chunk_id']\n",
        "                    })\n",
        "                \n",
        "                self.stats['successful_requests'] += 1\n",
        "                self.stats['total_concepts'] += len(result.get('concepts', []))\n",
        "                self.stats['total_relationships'] += len(result.get('relationships', []))\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"å¤„ç†å— {chunk['chunk_id']} å¤±è´¥: {e}\")\n",
        "                self.stats['failed_requests'] += 1\n",
        "                continue\n",
        "            \n",
        "            self.stats['total_requests'] += 1\n",
        "        \n",
        "        logger.info(f\"æå–å®Œæˆ: {len(all_concepts)} ä¸ªæ¦‚å¿µ, {len(all_relationships)} ä¸ªå…³ç³»\")\n",
        "        \n",
        "        return pd.DataFrame(all_concepts), pd.DataFrame(all_relationships)\n",
        "    \n",
        "    def _extract_from_text(self, text: str, chunk_id: str) -> Dict:\n",
        "        \"\"\"ä½¿ç”¨ GPT ä»å•ä¸ªæ–‡æœ¬å—æå–æ¦‚å¿µ\"\"\"\n",
        "        # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼ˆæ§åˆ¶æˆæœ¬ï¼‰\n",
        "        text = text[:1500]\n",
        "        \n",
        "        system_prompt = \"\"\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©å­¦çŸ¥è¯†æå–åŠ©æ‰‹ï¼Œä¸“æ³¨äºæ¾æçº¿è™«ç—…é¢†åŸŸã€‚\n",
        "è¯·ä»ç»™å®šæ–‡æœ¬ä¸­æå–å…³é”®æ¦‚å¿µå’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚\n",
        "\n",
        "æ¦‚å¿µç±»åˆ«åŒ…æ‹¬ï¼š\n",
        "- ç—…åŸä½“ï¼ˆå¦‚ï¼šæ¾æçº¿è™«ï¼‰\n",
        "- å®¿ä¸»ï¼ˆå¦‚ï¼šæ¾æ ‘ã€é»‘æ¾ï¼‰\n",
        "- è½½ä½“ï¼ˆå¦‚ï¼šæ¾å¢¨å¤©ç‰›ï¼‰\n",
        "- ç—‡çŠ¶ï¼ˆå¦‚ï¼šé’ˆå¶å˜è‰²ã€æ ‘è„‚å‡å°‘ï¼‰\n",
        "- é˜²æ²»æ–¹æ³•ï¼ˆå¦‚ï¼šåŒ–å­¦é˜²æ²»ã€ç”Ÿç‰©é˜²æ²»ï¼‰\n",
        "- åœ°åŒºï¼ˆå¦‚ï¼šæ—¥æœ¬ã€ä¸­å›½ï¼‰\n",
        "- å…¶ä»–ç›¸å…³æ¦‚å¿µ\n",
        "\n",
        "è¯·åªè¿”å› JSON æ ¼å¼ï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ã€‚\"\"\"\n",
        "        \n",
        "        user_prompt = f\"\"\"æ–‡æœ¬ï¼š\n",
        "{text}\n",
        "\n",
        "è¯·æå–æ¦‚å¿µå’Œå…³ç³»ï¼Œè¿”å› JSONï¼š\n",
        "{{\n",
        "  \"concepts\": [\n",
        "    {{\"name\": \"æ¦‚å¿µåç§°\", \"category\": \"ç±»åˆ«\", \"importance\": 1-3}}\n",
        "  ],\n",
        "  \"relationships\": [\n",
        "    {{\"head\": \"æ¦‚å¿µ1\", \"tail\": \"æ¦‚å¿µ2\", \"relation\": \"å…³ç³»ç±»å‹\", \"confidence\": 0.0-1.0}}\n",
        "  ]\n",
        "}}\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=800,\n",
        "                timeout=self.timeout\n",
        "            )\n",
        "            \n",
        "            content = response.choices[0].message.content.strip()\n",
        "            \n",
        "            # å°è¯•æå– JSONï¼ˆå¯èƒ½åœ¨ä»£ç å—ä¸­ï¼‰\n",
        "            if '```json' in content:\n",
        "                content = content.split('```json')[1].split('```')[0].strip()\n",
        "            elif '```' in content:\n",
        "                content = content.split('```')[1].split('```')[0].strip()\n",
        "            \n",
        "            result = json.loads(content)\n",
        "            return result\n",
        "            \n",
        "        except json.JSONDecodeError as e:\n",
        "            logger.warning(f\"JSON è§£æå¤±è´¥ ({chunk_id}): {e}\")\n",
        "            logger.debug(f\"åŸå§‹å“åº”: {content[:200]}...\")\n",
        "            return {'concepts': [], 'relationships': []}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"API è°ƒç”¨å¤±è´¥ ({chunk_id}): {e}\")\n",
        "            return {'concepts': [], 'relationships': []}\n",
        "    \n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
        "        return self.stats\n",
        "\n",
        "print(\"âœ… ConceptExtractorOpenAI ç±»å®šä¹‰å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. æ‰§è¡Œæ¦‚å¿µæå–\n",
        "\n",
        "**âš ï¸ é‡è¦æç¤º**ï¼š\n",
        "- æ­¤æ­¥éª¤ä¼šæ¶ˆè€— OpenAI API é…é¢\n",
        "- å¤„ç† 30 ä¸ªå—é¢„è®¡è´¹ç”¨ $1-3\n",
        "- å¯éšæ—¶ä¸­æ–­ï¼Œå·²å¤„ç†çš„æ•°æ®ä¼šä¿ç•™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# éªŒè¯ API Key\n",
        "if not CONFIG['llm']['api_key']:\n",
        "    print(\"âŒ OpenAI API Key æœªé…ç½®ï¼\")\n",
        "    print(\"   è¯·è¿”å› 00_ç¯å¢ƒè®¾ç½®.ipynb é…ç½® API Key\")\n",
        "    raise ValueError(\"Missing OpenAI API Key\")\n",
        "\n",
        "# åˆ›å»ºæå–å™¨\n",
        "extractor = ConceptExtractorOpenAI(\n",
        "    api_key=CONFIG['llm']['api_key'],\n",
        "    model=CONFIG['llm']['model'],\n",
        "    temperature=CONFIG['llm']['temperature'],\n",
        "    timeout=CONFIG['llm']['timeout']\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… æå–å™¨å·²åˆå§‹åŒ–\")\n",
        "print(f\"   æ¨¡å‹: {CONFIG['llm']['model']}\")\n",
        "print(f\"   æ¸©åº¦: {CONFIG['llm']['temperature']}\")\n",
        "print(f\"\\nâš ï¸  å³å°†å¤„ç† {min(len(chunks), CONFIG['llm']['max_chunks'])} ä¸ªæ–‡æœ¬å—\")\n",
        "print(f\"   é¢„è®¡æ—¶é—´: 5-10 åˆ†é’Ÿ\")\n",
        "print(f\"   é¢„è®¡è´¹ç”¨: $1-3 (ä½¿ç”¨ gpt-3.5-turbo)\")\n",
        "print(\"\\næŒ‰ Ctrl+C å¯éšæ—¶ä¸­æ–­\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ‰§è¡Œæå–\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"å¼€å§‹æ¦‚å¿µæå–\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "try:\n",
        "    concepts_df, relationships_df = extractor.extract_from_chunks(\n",
        "        chunks,\n",
        "        max_chunks=CONFIG['llm']['max_chunks']\n",
        "    )\n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"æ¦‚å¿µæå–å®Œæˆ\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nè€—æ—¶: {duration}\")\n",
        "    \n",
        "    # æ˜¾ç¤ºç»Ÿè®¡\n",
        "    stats = extractor.get_stats()\n",
        "    print(f\"\\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\")\n",
        "    print(f\"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}\")\n",
        "    print(f\"   æˆåŠŸ: {stats['successful_requests']}\")\n",
        "    print(f\"   å¤±è´¥: {stats['failed_requests']}\")\n",
        "    print(f\"   æå–çš„æ¦‚å¿µ: {stats['total_concepts']}\")\n",
        "    print(f\"   æå–çš„å…³ç³»: {stats['total_relationships']}\")\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å·²å¤„ç†çš„æ•°æ®...\")\n",
        "    concepts_df = pd.DataFrame([])\n",
        "    relationships_df = pd.DataFrame([])\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ æå–è¿‡ç¨‹å‡ºé”™: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. æŸ¥çœ‹æå–ç»“æœ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not concepts_df.empty:\n",
        "    print(\"\\nğŸ“Š æ¦‚å¿µæ ·ä¾‹ï¼ˆå‰10ä¸ªï¼‰:\")\n",
        "    print(concepts_df.head(10))\n",
        "    \n",
        "    print(\"\\nğŸ“Š æ¦‚å¿µç±»åˆ«åˆ†å¸ƒ:\")\n",
        "    print(concepts_df['category'].value_counts())\n",
        "    \n",
        "    print(\"\\nğŸ“Š é‡è¦æ€§åˆ†å¸ƒ:\")\n",
        "    print(concepts_df['importance'].value_counts().sort_index())\n",
        "else:\n",
        "    print(\"âš ï¸  æœªæå–åˆ°æ¦‚å¿µ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not relationships_df.empty:\n",
        "    print(\"\\nğŸ“Š å…³ç³»æ ·ä¾‹ï¼ˆå‰10ä¸ªï¼‰:\")\n",
        "    print(relationships_df.head(10))\n",
        "    \n",
        "    print(\"\\nğŸ“Š å…³ç³»ç±»å‹åˆ†å¸ƒ:\")\n",
        "    print(relationships_df['edge'].value_counts().head(10))\n",
        "else:\n",
        "    print(\"âš ï¸  æœªæå–åˆ°å…³ç³»\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ä¿å­˜ç»“æœ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜æ¦‚å¿µå’Œå…³ç³»\n",
        "concepts_file = f\"{output_dir}/concepts_raw.csv\"\n",
        "relationships_file = f\"{output_dir}/relationships_raw.csv\"\n",
        "\n",
        "if not concepts_df.empty:\n",
        "    concepts_df.to_csv(concepts_file, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… æ¦‚å¿µå·²ä¿å­˜: {concepts_file}\")\n",
        "\n",
        "if not relationships_df.empty:\n",
        "    relationships_df.to_csv(relationships_file, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… å…³ç³»å·²ä¿å­˜: {relationships_file}\")\n",
        "\n",
        "# ä¿å­˜ç»Ÿè®¡ä¿¡æ¯\n",
        "extraction_stats = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'duration': str(duration),\n",
        "    'llm_model': CONFIG['llm']['model'],\n",
        "    'chunks_processed': min(len(chunks), CONFIG['llm']['max_chunks']),\n",
        "    'api_stats': stats,\n",
        "    'concepts_extracted': len(concepts_df),\n",
        "    'relationships_extracted': len(relationships_df)\n",
        "}\n",
        "\n",
        "stats_file = f\"{output_dir}/extraction_llm_stats.json\"\n",
        "with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(extraction_stats, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. æ¦‚å¿µæå–å®Œæˆ\n",
        "\n",
        "### âœ… å®Œæˆæƒ…å†µ\n",
        "\n",
        "- å¤„ç†çš„æ–‡æœ¬å—æ•°é‡\n",
        "- æå–çš„æ¦‚å¿µæ•°é‡\n",
        "- æå–çš„å…³ç³»æ•°é‡\n",
        "- API è°ƒç”¨ç»Ÿè®¡\n",
        "\n",
        "### ğŸ“ ä¸‹ä¸€æ­¥\n",
        "\n",
        "è¯·æ‰“å¼€ **`03_æ¦‚å¿µå»é‡.ipynb`** ç»§ç»­æ‰§è¡Œæ¦‚å¿µå»é‡\n",
        "\n",
        "### ğŸ’¡ æç¤º\n",
        "\n",
        "- åŸå§‹æå–ç»“æœå·²ä¿å­˜ï¼Œåç»­æ­¥éª¤ä¸ä¼šé‡å¤è°ƒç”¨ API\n",
        "- å¦‚éœ€é‡æ–°æå–ï¼Œåˆ é™¤è¾“å‡ºæ–‡ä»¶åé‡æ–°è¿è¡Œå³å¯\n",
        "- å¯ä»¥è°ƒæ•´ `max_chunks` å‚æ•°å¤„ç†æ›´å¤šæˆ–æ›´å°‘çš„æ–‡æœ¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ˜¾ç¤ºå®Œæˆæ€»ç»“\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ æ¦‚å¿µæå–æ­¥éª¤å®Œæˆï¼\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nğŸ“Š æå–ç»“æœ:\")\n",
        "print(f\"   - å¤„ç†å—æ•°: {extraction_stats['chunks_processed']}\")\n",
        "print(f\"   - æ¦‚å¿µæ•°: {extraction_stats['concepts_extracted']}\")\n",
        "print(f\"   - å…³ç³»æ•°: {extraction_stats['relationships_extracted']}\")\n",
        "print(f\"   - è€—æ—¶: {extraction_stats['duration']}\")\n",
        "print(f\"\\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}\")\n",
        "print(f\"\\nâ¡ï¸  ä¸‹ä¸€æ­¥: æ‰“å¼€ 03_æ¦‚å¿µå»é‡.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
