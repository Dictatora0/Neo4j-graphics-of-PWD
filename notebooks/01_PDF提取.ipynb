{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“„ æ­¥éª¤ 1ï¼šPDF æ–‡æœ¬æå–\n",
        "\n",
        "è¿™ä¸ª notebook è´Ÿè´£ï¼š\n",
        "1. ä» PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬\n",
        "2. æ¸…æ´—å’Œè§„èŒƒåŒ–æ–‡æœ¬\n",
        "3. åˆ†å—å¤„ç†ï¼ˆä¸º LLM å‡†å¤‡ï¼‰\n",
        "4. ç¼“å­˜ç»“æœï¼ˆé¿å…é‡å¤æå–ï¼‰\n",
        "\n",
        "**â±ï¸ é¢„è®¡æ—¶é—´**ï¼šå–å†³äº PDF æ•°é‡å’Œå¤§å°ï¼Œé€šå¸¸ 2-10 åˆ†é’Ÿ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. åŠ è½½ç¯å¢ƒé…ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# è¯»å–ç¯å¢ƒå˜é‡\n",
        "if os.path.exists('/content/PWD_Project/.env'):\n",
        "    with open('/content/PWD_Project/.env', 'r') as f:\n",
        "        for line in f:\n",
        "            if '=' in line:\n",
        "                key, value = line.strip().split('=', 1)\n",
        "                os.environ[key] = value\n",
        "\n",
        "PROJECT_ROOT = os.environ.get('PROJECT_ROOT', '/content/PWD_Project')\n",
        "CONFIG_PATH = os.environ.get('CONFIG_PATH', f'{PROJECT_ROOT}/config/config_colab.yaml')\n",
        "\n",
        "# åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, f'{PROJECT_ROOT}/modules')\n",
        "\n",
        "# åŠ è½½é…ç½®\n",
        "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
        "    CONFIG = yaml.safe_load(f)\n",
        "\n",
        "print(f\"âœ… é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}\")\n",
        "print(f\"âœ… é…ç½®æ–‡ä»¶: {CONFIG_PATH}\")\n",
        "print(f\"âœ… PDF ç›®å½•: {CONFIG['pdf']['input_directory']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. å¯¼å…¥å¿…è¦çš„åº“"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List\n",
        "from datetime import datetime\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# é…ç½®æ—¥å¿—\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('PDF_Extractor')\n",
        "\n",
        "print(\"âœ… åº“å¯¼å…¥å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PDF æå–å™¨ç±»"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PDFExtractor:\n",
        "    \"\"\"PDF æ–‡æœ¬æå–å™¨ï¼ˆColab ç‰ˆæœ¬ï¼‰\"\"\"\n",
        "    \n",
        "    def __init__(self, cache_dir: str = None):\n",
        "        self.cache_dir = cache_dir or f\"{PROJECT_ROOT}/cache\"\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "    \n",
        "    def extract_from_directory(self, pdf_dir: str) -> Dict[str, str]:\n",
        "        \"\"\"ä»ç›®å½•æå–æ‰€æœ‰ PDF æ–‡æœ¬\"\"\"\n",
        "        if not os.path.exists(pdf_dir):\n",
        "            logger.error(f\"PDF ç›®å½•ä¸å­˜åœ¨: {pdf_dir}\")\n",
        "            return {}\n",
        "        \n",
        "        pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]\n",
        "        \n",
        "        if not pdf_files:\n",
        "            logger.warning(f\"æœªæ‰¾åˆ° PDF æ–‡ä»¶: {pdf_dir}\")\n",
        "            return {}\n",
        "        \n",
        "        logger.info(f\"æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶\")\n",
        "        \n",
        "        pdf_texts = {}\n",
        "        for pdf_file in tqdm(pdf_files, desc=\"æå– PDF\"):\n",
        "            pdf_path = os.path.join(pdf_dir, pdf_file)\n",
        "            text = self._extract_text(pdf_path, pdf_file)\n",
        "            if text:\n",
        "                pdf_texts[pdf_file] = text\n",
        "        \n",
        "        logger.info(f\"æˆåŠŸæå– {len(pdf_texts)} ä¸ª PDF\")\n",
        "        return pdf_texts\n",
        "    \n",
        "    def _extract_text(self, pdf_path: str, pdf_name: str) -> str:\n",
        "        \"\"\"æå–å•ä¸ª PDF çš„æ–‡æœ¬\"\"\"\n",
        "        # æ£€æŸ¥ç¼“å­˜\n",
        "        cache_file = os.path.join(self.cache_dir, f\"{Path(pdf_name).stem}.txt\")\n",
        "        if os.path.exists(cache_file):\n",
        "            logger.info(f\"ä»ç¼“å­˜è¯»å–: {pdf_name}\")\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        \n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            text_parts = []\n",
        "            \n",
        "            for page_num, page in enumerate(doc, 1):\n",
        "                text = page.get_text()\n",
        "                if text.strip():\n",
        "                    text_parts.append(text)\n",
        "            \n",
        "            doc.close()\n",
        "            \n",
        "            full_text = '\\n'.join(text_parts)\n",
        "            full_text = self._clean_text(full_text)\n",
        "            \n",
        "            # ä¿å­˜åˆ°ç¼“å­˜\n",
        "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(full_text)\n",
        "            \n",
        "            logger.info(f\"æå–å®Œæˆ: {pdf_name} ({len(full_text)} å­—ç¬¦)\")\n",
        "            return full_text\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"æå–å¤±è´¥ {pdf_name}: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"æ¸…æ´—æ–‡æœ¬\"\"\"\n",
        "        # ç§»é™¤å¤šä½™ç©ºç™½\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # ç§»é™¤æ§åˆ¶å­—ç¬¦\n",
        "        text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
        "        # ç§»é™¤é¡µç ï¼ˆå¯é€‰ï¼‰\n",
        "        text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
        "        return text.strip()\n",
        "\n",
        "print(\"âœ… PDFExtractor ç±»å®šä¹‰å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. æ‰§è¡Œ PDF æ–‡æœ¬æå–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºæå–å™¨\n",
        "extractor = PDFExtractor(cache_dir=CONFIG['output']['cache_directory'])\n",
        "\n",
        "# æå–æ–‡æœ¬\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"å¼€å§‹æå– PDF æ–‡æœ¬\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "pdf_texts = extractor.extract_from_directory(CONFIG['pdf']['input_directory'])\n",
        "\n",
        "end_time = datetime.now()\n",
        "duration = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PDF æå–å®Œæˆ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nè€—æ—¶: {duration}\")\n",
        "print(f\"æå–çš„ PDF æ•°é‡: {len(pdf_texts)}\")\n",
        "print(f\"\\næ€»å­—ç¬¦æ•°: {sum(len(text) for text in pdf_texts.values()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. æ–‡æœ¬åˆ†å—\n",
        "\n",
        "å°†é•¿æ–‡æœ¬åˆ†å‰²æˆé€‚åˆ LLM å¤„ç†çš„å—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_chunks(pdf_texts: Dict[str, str], \n",
        "                  chunk_size: int = 2000, \n",
        "                  overlap: int = 200) -> List[Dict]:\n",
        "    \"\"\"å°†æ–‡æœ¬åˆ†å—\"\"\"\n",
        "    chunks = []\n",
        "    \n",
        "    for pdf_name, text in pdf_texts.items():\n",
        "        # æŒ‰å—å¤§å°åˆ‡åˆ†\n",
        "        for i in range(0, len(text), chunk_size - overlap):\n",
        "            chunk_text = text[i:i + chunk_size]\n",
        "            \n",
        "            # è·³è¿‡è¿‡çŸ­çš„å—\n",
        "            if len(chunk_text.strip()) < 100:\n",
        "                continue\n",
        "            \n",
        "            chunks.append({\n",
        "                'chunk_id': f\"{Path(pdf_name).stem}_{len(chunks)}\",\n",
        "                'source_pdf': pdf_name,\n",
        "                'text': chunk_text,\n",
        "                'concepts': []  # ä¸ºåç»­æ­¥éª¤å‡†å¤‡\n",
        "            })\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# æ‰§è¡Œåˆ†å—\n",
        "chunks = create_chunks(pdf_texts, chunk_size=2000, overlap=200)\n",
        "\n",
        "print(f\"\\nâœ… åˆ›å»ºäº† {len(chunks)} ä¸ªæ–‡æœ¬å—\")\n",
        "print(f\"\\nå¹³å‡å—å¤§å°: {sum(len(c['text']) for c in chunks) // len(chunks)} å­—ç¬¦\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. æŸ¥çœ‹æ ·ä¾‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æœ¬å—çš„æ ·ä¾‹\n",
        "if chunks:\n",
        "    sample = chunks[0]\n",
        "    print(\"\\nğŸ“„ æ ·ä¾‹æ–‡æœ¬å—:\")\n",
        "    print(f\"\\nChunk ID: {sample['chunk_id']}\")\n",
        "    print(f\"æ¥æº PDF: {sample['source_pdf']}\")\n",
        "    print(f\"æ–‡æœ¬é•¿åº¦: {len(sample['text'])} å­—ç¬¦\")\n",
        "    print(\"\\næ–‡æœ¬é¢„è§ˆ:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(sample['text'][:500] + \"...\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ä¿å­˜ç»“æœ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜æ–‡æœ¬å’Œå—ä¿¡æ¯\n",
        "output_dir = CONFIG['output']['base_directory']\n",
        "\n",
        "# ä¿å­˜ PDF æ–‡æœ¬\n",
        "texts_file = f\"{output_dir}/pdf_texts.json\"\n",
        "with open(texts_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(pdf_texts, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ä¿å­˜æ–‡æœ¬å—\n",
        "chunks_file = f\"{output_dir}/text_chunks.json\"\n",
        "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ä¿å­˜ç»Ÿè®¡ä¿¡æ¯\n",
        "stats = {\n",
        "    'pdf_count': len(pdf_texts),\n",
        "    'chunk_count': len(chunks),\n",
        "    'total_chars': sum(len(text) for text in pdf_texts.values()),\n",
        "    'avg_chunk_size': sum(len(c['text']) for c in chunks) // len(chunks) if chunks else 0,\n",
        "    'extraction_time': str(duration),\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "stats_file = f\"{output_dir}/extraction_stats.json\"\n",
        "with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(stats, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… ç»“æœå·²ä¿å­˜:\")\n",
        "print(f\"   ğŸ“„ PDF æ–‡æœ¬: {texts_file}\")\n",
        "print(f\"   ğŸ“¦ æ–‡æœ¬å—: {chunks_file}\")\n",
        "print(f\"   ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. PDF æå–å®Œæˆ\n",
        "\n",
        "### âœ… å®Œæˆæƒ…å†µ\n",
        "\n",
        "- æå–çš„ PDF æ•°é‡\n",
        "- ç”Ÿæˆçš„æ–‡æœ¬å—æ•°é‡\n",
        "- ç¼“å­˜æ–‡ä»¶ä½ç½®\n",
        "\n",
        "### ğŸ“ ä¸‹ä¸€æ­¥\n",
        "\n",
        "è¯·æ‰“å¼€ **`02_æ¦‚å¿µæå–.ipynb`** ç»§ç»­æ‰§è¡Œæ¦‚å¿µæå–\n",
        "\n",
        "### ğŸ’¡ æç¤º\n",
        "\n",
        "- æå–çš„æ–‡æœ¬å·²ç¼“å­˜ï¼Œé‡æ–°è¿è¡Œä¸ä¼šé‡å¤æå–\n",
        "- å¦‚éœ€é‡æ–°æå–ï¼Œåˆ é™¤ `cache/` ç›®å½•ä¸‹çš„ `.txt` æ–‡ä»¶\n",
        "- å¯ä»¥éšæ—¶æ·»åŠ æ–°çš„ PDF æ–‡ä»¶åˆ° PDF ç›®å½•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ˜¾ç¤ºä¸‹ä¸€æ­¥æç¤º\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ PDF æå–æ­¥éª¤å®Œæˆï¼\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nğŸ“Š ç»Ÿè®¡:\")\n",
        "print(f\"   - PDF æ–‡ä»¶: {stats['pdf_count']} ä¸ª\")\n",
        "print(f\"   - æ–‡æœ¬å—: {stats['chunk_count']} ä¸ª\")\n",
        "print(f\"   - æ€»å­—ç¬¦æ•°: {stats['total_chars']:,} ä¸ª\")\n",
        "print(f\"   - è€—æ—¶: {stats['extraction_time']}\")\n",
        "print(f\"\\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}\")\n",
        "print(f\"\\nâ¡ï¸  ä¸‹ä¸€æ­¥: æ‰“å¼€ 02_æ¦‚å¿µæå–.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
