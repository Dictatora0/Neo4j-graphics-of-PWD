{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ”„ æ­¥éª¤ 3ï¼šæ¦‚å¿µå»é‡ä¸å…³ç³»åˆå¹¶\n",
        "\n",
        "è¿™ä¸ª notebook è´Ÿè´£ï¼š\n",
        "1. ä½¿ç”¨è¯­ä¹‰åµŒå…¥è¯†åˆ«ç›¸ä¼¼æ¦‚å¿µ\n",
        "2. åˆå¹¶é‡å¤æ¦‚å¿µ\n",
        "3. æ›´æ–°å…³ç³»ä¸­çš„æ¦‚å¿µå¼•ç”¨\n",
        "4. è¿‡æ»¤ä½è´¨é‡æ¦‚å¿µ\n",
        "5. ç”Ÿæˆæœ€ç»ˆçš„çŸ¥è¯†å›¾è°±æ•°æ®\n",
        "\n",
        "**â±ï¸ é¢„è®¡æ—¶é—´**ï¼š2-5 åˆ†é’Ÿï¼ˆå–å†³äºæ¦‚å¿µæ•°é‡ï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. åŠ è½½ç¯å¢ƒå’Œé…ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import yaml\n",
        "import logging\n",
        "from typing import Dict, List, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# åŠ è½½ç¯å¢ƒå˜é‡\n",
        "if os.path.exists('/content/PWD_Project/.env'):\n",
        "    with open('/content/PWD_Project/.env', 'r') as f:\n",
        "        for line in f:\n",
        "            if '=' in line:\n",
        "                key, value = line.strip().split('=', 1)\n",
        "                os.environ[key] = value\n",
        "\n",
        "PROJECT_ROOT = os.environ.get('PROJECT_ROOT', '/content/PWD_Project')\n",
        "CONFIG_PATH = os.environ.get('CONFIG_PATH', f'{PROJECT_ROOT}/config/config_colab.yaml')\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# åŠ è½½é…ç½®\n",
        "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
        "    CONFIG = yaml.safe_load(f)\n",
        "\n",
        "# é…ç½®æ—¥å¿—\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('Deduplicator')\n",
        "\n",
        "print(f\"âœ… é…ç½®å·²åŠ è½½\")\n",
        "print(f\"   ç›¸ä¼¼åº¦é˜ˆå€¼: {CONFIG['deduplication']['similarity_threshold']}\")\n",
        "print(f\"   åµŒå…¥æ¨¡å‹: {CONFIG['deduplication']['embedding_model']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. åŠ è½½ä¸Šä¸€æ­¥çš„æå–ç»“æœ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = CONFIG['output']['base_directory']\n",
        "\n",
        "# åŠ è½½æ¦‚å¿µ\n",
        "concepts_file = f\"{output_dir}/concepts_raw.csv\"\n",
        "if not os.path.exists(concepts_file):\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ°æ¦‚å¿µæ–‡ä»¶: {concepts_file}\")\n",
        "    print(\"   è¯·å…ˆè¿è¡Œ 02_æ¦‚å¿µæå–.ipynb\")\n",
        "    raise FileNotFoundError(concepts_file)\n",
        "\n",
        "concepts_df = pd.read_csv(concepts_file)\n",
        "print(f\"âœ… åŠ è½½äº† {len(concepts_df)} ä¸ªåŸå§‹æ¦‚å¿µ\")\n",
        "\n",
        "# åŠ è½½å…³ç³»\n",
        "relationships_file = f\"{output_dir}/relationships_raw.csv\"\n",
        "if os.path.exists(relationships_file):\n",
        "    relationships_df = pd.read_csv(relationships_file)\n",
        "    print(f\"âœ… åŠ è½½äº† {len(relationships_df)} ä¸ªåŸå§‹å…³ç³»\")\n",
        "else:\n",
        "    relationships_df = pd.DataFrame()\n",
        "    print(\"âš ï¸  æœªæ‰¾åˆ°å…³ç³»æ–‡ä»¶\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. æ¦‚å¿µå»é‡å™¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConceptDeduplicator:\n",
        "    \"\"\"ä½¿ç”¨è¯­ä¹‰åµŒå…¥è¿›è¡Œæ¦‚å¿µå»é‡\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, similarity_threshold: float = 0.85):\n",
        "        self.model_name = model_name\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        \n",
        "        logger.info(f\"åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        logger.info(\"æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
        "    \n",
        "    def deduplicate_concepts(self, concepts_df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
        "        \"\"\"å»é‡æ¦‚å¿µå¹¶è¿”å›æ˜ å°„å…³ç³»\"\"\"\n",
        "        if concepts_df.empty:\n",
        "            return concepts_df, {}\n",
        "        \n",
        "        # è·å–å”¯ä¸€æ¦‚å¿µ\n",
        "        unique_concepts = concepts_df['entity'].unique().tolist()\n",
        "        logger.info(f\"å”¯ä¸€æ¦‚å¿µæ•°: {len(unique_concepts)}\")\n",
        "        \n",
        "        # è®¡ç®—åµŒå…¥\n",
        "        logger.info(\"è®¡ç®—æ¦‚å¿µåµŒå…¥...\")\n",
        "        embeddings = self.model.encode(unique_concepts, show_progress_bar=True)\n",
        "        \n",
        "        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\n",
        "        logger.info(\"è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...\")\n",
        "        similarity_matrix = cosine_similarity(embeddings)\n",
        "        \n",
        "        # æ‰¾åˆ°ç›¸ä¼¼æ¦‚å¿µå¹¶å»ºç«‹æ˜ å°„\n",
        "        logger.info(\"è¯†åˆ«ç›¸ä¼¼æ¦‚å¿µ...\")\n",
        "        concept_mapping = {}  # æ—§æ¦‚å¿µ -> è§„èŒƒæ¦‚å¿µ\n",
        "        used_indices = set()\n",
        "        \n",
        "        for i in range(len(unique_concepts)):\n",
        "            if i in used_indices:\n",
        "                continue\n",
        "            \n",
        "            # é€‰æ‹©ç¬¬ä¸€ä¸ªä½œä¸ºè§„èŒƒæ¦‚å¿µ\n",
        "            canonical = unique_concepts[i]\n",
        "            concept_mapping[canonical] = canonical\n",
        "            \n",
        "            # æŸ¥æ‰¾ç›¸ä¼¼çš„æ¦‚å¿µ\n",
        "            for j in range(i + 1, len(unique_concepts)):\n",
        "                if j in used_indices:\n",
        "                    continue\n",
        "                \n",
        "                if similarity_matrix[i][j] >= self.similarity_threshold:\n",
        "                    # æ˜ å°„åˆ°è§„èŒƒæ¦‚å¿µ\n",
        "                    concept_mapping[unique_concepts[j]] = canonical\n",
        "                    used_indices.add(j)\n",
        "        \n",
        "        logger.info(f\"è¯†åˆ«å‡º {len(used_indices)} ä¸ªé‡å¤æ¦‚å¿µ\")\n",
        "        \n",
        "        # åº”ç”¨æ˜ å°„\n",
        "        concepts_df['entity'] = concepts_df['entity'].map(\n",
        "            lambda x: concept_mapping.get(x, x)\n",
        "        )\n",
        "        \n",
        "        # èšåˆé‡å¤æ¦‚å¿µçš„å±æ€§\n",
        "        deduplicated_df = concepts_df.groupby('entity').agg({\n",
        "            'category': 'first',\n",
        "            'importance': 'max',  # å–æœ€é«˜é‡è¦æ€§\n",
        "            'chunk_id': 'first',\n",
        "            'source_pdf': 'first'\n",
        "        }).reset_index()\n",
        "        \n",
        "        logger.info(f\"å»é‡å: {len(unique_concepts)} -> {len(deduplicated_df)} ä¸ªæ¦‚å¿µ\")\n",
        "        \n",
        "        return deduplicated_df, concept_mapping\n",
        "\n",
        "print(\"âœ… ConceptDeduplicator ç±»å®šä¹‰å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. æ‰§è¡Œæ¦‚å¿µå»é‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"å¼€å§‹æ¦‚å¿µå»é‡\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# åˆ›å»ºå»é‡å™¨\n",
        "deduplicator = ConceptDeduplicator(\n",
        "    model_name=CONFIG['deduplication']['embedding_model'],\n",
        "    similarity_threshold=CONFIG['deduplication']['similarity_threshold']\n",
        ")\n",
        "\n",
        "# æ‰§è¡Œå»é‡\n",
        "deduplicated_concepts, concept_mapping = deduplicator.deduplicate_concepts(concepts_df)\n",
        "\n",
        "end_time = datetime.now()\n",
        "duration = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"æ¦‚å¿µå»é‡å®Œæˆ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nè€—æ—¶: {duration}\")\n",
        "print(f\"åŸå§‹æ¦‚å¿µ: {len(concepts_df['entity'].unique())}\")\n",
        "print(f\"å»é‡å: {len(deduplicated_concepts)}\")\n",
        "print(f\"å»é™¤é‡å¤: {len(concepts_df['entity'].unique()) - len(deduplicated_concepts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. æ›´æ–°å…³ç³»ä¸­çš„æ¦‚å¿µå¼•ç”¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not relationships_df.empty:\n",
        "    print(\"\\næ›´æ–°å…³ç³»ä¸­çš„æ¦‚å¿µå¼•ç”¨...\")\n",
        "    \n",
        "    # åº”ç”¨æ¦‚å¿µæ˜ å°„åˆ°å…³ç³»\n",
        "    relationships_df['node_1'] = relationships_df['node_1'].map(\n",
        "        lambda x: concept_mapping.get(x, x)\n",
        "    )\n",
        "    relationships_df['node_2'] = relationships_df['node_2'].map(\n",
        "        lambda x: concept_mapping.get(x, x)\n",
        "    )\n",
        "    \n",
        "    # ç§»é™¤è‡ªç¯ï¼ˆæ¦‚å¿µæŒ‡å‘è‡ªå·±ï¼‰\n",
        "    before_count = len(relationships_df)\n",
        "    relationships_df = relationships_df[relationships_df['node_1'] != relationships_df['node_2']]\n",
        "    removed_self_loops = before_count - len(relationships_df)\n",
        "    \n",
        "    # ç§»é™¤é‡å¤å…³ç³»ï¼ˆä¿ç•™æƒé‡æœ€é«˜çš„ï¼‰\n",
        "    relationships_df = relationships_df.sort_values('weight', ascending=False)\n",
        "    relationships_df = relationships_df.drop_duplicates(\n",
        "        subset=['node_1', 'node_2', 'edge'], \n",
        "        keep='first'\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… å…³ç³»æ›´æ–°å®Œæˆ\")\n",
        "    print(f\"   ç§»é™¤è‡ªç¯: {removed_self_loops}\")\n",
        "    print(f\"   æœ€ç»ˆå…³ç³»æ•°: {len(relationships_df)}\")\n",
        "else:\n",
        "    print(\"âš ï¸  æ²¡æœ‰å…³ç³»éœ€è¦æ›´æ–°\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. è¿‡æ»¤ä½è´¨é‡æ¦‚å¿µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"è¿‡æ»¤ä½è´¨é‡æ¦‚å¿µ\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "min_importance = CONFIG['filtering']['min_importance']\n",
        "min_connections = CONFIG['filtering']['min_connections']\n",
        "\n",
        "# è®¡ç®—æ¯ä¸ªæ¦‚å¿µçš„è¿æ¥æ•°\n",
        "if not relationships_df.empty:\n",
        "    concept_connections = pd.concat([\n",
        "        relationships_df['node_1'].value_counts(),\n",
        "        relationships_df['node_2'].value_counts()\n",
        "    ], axis=1).fillna(0).sum(axis=1)\n",
        "    \n",
        "    deduplicated_concepts['connections'] = deduplicated_concepts['entity'].map(\n",
        "        lambda x: concept_connections.get(x, 0)\n",
        "    ).astype(int)\n",
        "else:\n",
        "    deduplicated_concepts['connections'] = 0\n",
        "\n",
        "# åº”ç”¨è¿‡æ»¤\n",
        "before_filter = len(deduplicated_concepts)\n",
        "\n",
        "filtered_concepts = deduplicated_concepts[\n",
        "    (deduplicated_concepts['importance'] >= min_importance) |\n",
        "    (deduplicated_concepts['connections'] >= min_connections)\n",
        "]\n",
        "\n",
        "print(f\"è¿‡æ»¤å‰: {before_filter} ä¸ªæ¦‚å¿µ\")\n",
        "print(f\"è¿‡æ»¤å: {len(filtered_concepts)} ä¸ªæ¦‚å¿µ\")\n",
        "print(f\"ç§»é™¤: {before_filter - len(filtered_concepts)} ä¸ªä½è´¨é‡æ¦‚å¿µ\")\n",
        "\n",
        "# è¿‡æ»¤å…³ç³»ï¼ˆåªä¿ç•™æœ‰æ•ˆæ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼‰\n",
        "if not relationships_df.empty:\n",
        "    valid_concepts = set(filtered_concepts['entity'].unique())\n",
        "    \n",
        "    before_filter_rel = len(relationships_df)\n",
        "    filtered_relationships = relationships_df[\n",
        "        (relationships_df['node_1'].isin(valid_concepts)) &\n",
        "        (relationships_df['node_2'].isin(valid_concepts))\n",
        "    ]\n",
        "    \n",
        "    print(f\"\\nå…³ç³»è¿‡æ»¤:\")\n",
        "    print(f\"   è¿‡æ»¤å‰: {before_filter_rel}\")\n",
        "    print(f\"   è¿‡æ»¤å: {len(filtered_relationships)}\")\n",
        "else:\n",
        "    filtered_relationships = relationships_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. æŸ¥çœ‹æœ€ç»ˆç»“æœ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nğŸ“Š æœ€ç»ˆæ¦‚å¿µæ ·ä¾‹ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰:\")\n",
        "print(filtered_concepts.nlargest(15, 'importance')[['entity', 'category', 'importance', 'connections']])\n",
        "\n",
        "print(\"\\nğŸ“Š æ¦‚å¿µç±»åˆ«åˆ†å¸ƒ:\")\n",
        "print(filtered_concepts['category'].value_counts())\n",
        "\n",
        "print(\"\\nğŸ“Š é‡è¦æ€§åˆ†å¸ƒ:\")\n",
        "print(filtered_concepts['importance'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not filtered_relationships.empty:\n",
        "    print(\"\\nğŸ“Š æœ€ç»ˆå…³ç³»æ ·ä¾‹ï¼ˆæŒ‰æƒé‡æ’åºï¼‰:\")\n",
        "    print(filtered_relationships.nlargest(15, 'weight')[['node_1', 'node_2', 'edge', 'weight']])\n",
        "    \n",
        "    print(\"\\nğŸ“Š å…³ç³»ç±»å‹åˆ†å¸ƒ:\")\n",
        "    print(filtered_relationships['edge'].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ä¿å­˜æœ€ç»ˆç»“æœ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜å»é‡åçš„æ¦‚å¿µ\n",
        "final_concepts_file = f\"{output_dir}/concepts_final.csv\"\n",
        "filtered_concepts.to_csv(final_concepts_file, index=False, encoding='utf-8-sig')\n",
        "print(f\"âœ… æœ€ç»ˆæ¦‚å¿µå·²ä¿å­˜: {final_concepts_file}\")\n",
        "\n",
        "# ä¿å­˜å»é‡åçš„å…³ç³»\n",
        "if not filtered_relationships.empty:\n",
        "    final_relationships_file = f\"{output_dir}/relationships_final.csv\"\n",
        "    filtered_relationships.to_csv(final_relationships_file, index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… æœ€ç»ˆå…³ç³»å·²ä¿å­˜: {final_relationships_file}\")\n",
        "\n",
        "# ä¿å­˜æ¦‚å¿µæ˜ å°„ï¼ˆç”¨äºè¿½æº¯ï¼‰\n",
        "mapping_file = f\"{output_dir}/concept_mapping.json\"\n",
        "with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(concept_mapping, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… æ¦‚å¿µæ˜ å°„å·²ä¿å­˜: {mapping_file}\")\n",
        "\n",
        "# ä¿å­˜ç»Ÿè®¡ä¿¡æ¯\n",
        "dedup_stats = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'duration': str(duration),\n",
        "    'original_concepts': len(concepts_df['entity'].unique()),\n",
        "    'deduplicated_concepts': len(deduplicated_concepts),\n",
        "    'filtered_concepts': len(filtered_concepts),\n",
        "    'duplicates_removed': len(concepts_df['entity'].unique()) - len(deduplicated_concepts),\n",
        "    'low_quality_removed': len(deduplicated_concepts) - len(filtered_concepts),\n",
        "    'original_relationships': len(relationships_df) if not relationships_df.empty else 0,\n",
        "    'final_relationships': len(filtered_relationships),\n",
        "    'similarity_threshold': CONFIG['deduplication']['similarity_threshold'],\n",
        "    'min_importance': min_importance,\n",
        "    'min_connections': min_connections\n",
        "}\n",
        "\n",
        "stats_file = f\"{output_dir}/deduplication_stats.json\"\n",
        "with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(dedup_stats, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. å»é‡å®Œæˆ\n",
        "\n",
        "### âœ… å®Œæˆæƒ…å†µ\n",
        "\n",
        "- åŸå§‹æ¦‚å¿µæ•°é‡ â†’ å»é‡åæ•°é‡\n",
        "- ç§»é™¤çš„é‡å¤æ¦‚å¿µæ•°é‡\n",
        "- è¿‡æ»¤çš„ä½è´¨é‡æ¦‚å¿µæ•°é‡\n",
        "- æœ€ç»ˆçŸ¥è¯†å›¾è°±è§„æ¨¡\n",
        "\n",
        "### ğŸ“ ä¸‹ä¸€æ­¥\n",
        "\n",
        "è¯·æ‰“å¼€ **`04_Neo4jå¯¼å…¥.ipynb`** å°†çŸ¥è¯†å›¾è°±å¯¼å…¥æ•°æ®åº“\n",
        "\n",
        "### ğŸ’¡ æç¤º\n",
        "\n",
        "- æœ€ç»ˆæ•°æ®å·²å‡†å¤‡å¥½å¯¼å…¥ Neo4j\n",
        "- å¯ä»¥è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼é‡æ–°å»é‡\n",
        "- æ¦‚å¿µæ˜ å°„æ–‡ä»¶å¯ç”¨äºè¿½æº¯åŸå§‹æ¦‚å¿µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ˜¾ç¤ºå®Œæˆæ€»ç»“\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ æ¦‚å¿µå»é‡æ­¥éª¤å®Œæˆï¼\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nğŸ“Š å¤„ç†ç»“æœ:\")\n",
        "print(f\"   - åŸå§‹æ¦‚å¿µ: {dedup_stats['original_concepts']}\")\n",
        "print(f\"   - å»é™¤é‡å¤: {dedup_stats['duplicates_removed']}\")\n",
        "print(f\"   - å»é™¤ä½è´¨é‡: {dedup_stats['low_quality_removed']}\")\n",
        "print(f\"   - æœ€ç»ˆæ¦‚å¿µ: {dedup_stats['filtered_concepts']}\")\n",
        "print(f\"   - æœ€ç»ˆå…³ç³»: {dedup_stats['final_relationships']}\")\n",
        "print(f\"   - è€—æ—¶: {dedup_stats['duration']}\")\n",
        "print(f\"\\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}\")\n",
        "print(f\"\\nâ¡ï¸  ä¸‹ä¸€æ­¥: æ‰“å¼€ 04_Neo4jå¯¼å…¥.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
