#!/usr/bin/env python3
"""
LLM-based Concept Extraction Module
Uses Ollama with Mistral/Zephyr for extracting concepts and semantic relationships
"""

import json
import logging
from typing import List, Dict, Optional, Tuple
import requests
from tqdm import tqdm
import pandas as pd

logger = logging.getLogger(__name__)


class ConceptExtractor:
    """Extract concepts and relationships using LLM (Ollama)"""
    
    def __init__(self, model: str = "mistral", ollama_host: str = "http://localhost:11434"):
        """
        Initialize concept extractor
        
        Args:
            model: Model name (mistral, zephyr, neural-chat, etc.)
            ollama_host: Ollama server URL
        """
        self.model = model
        self.ollama_host = ollama_host
        self.api_endpoint = f"{ollama_host}/api/generate"
        self._verify_ollama_connection()
    
    def _verify_ollama_connection(self):
        """Verify Ollama server is running"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                logger.info(f"‚úì Connected to Ollama at {self.ollama_host}")
                models = response.json().get('models', [])
                model_names = [m.get('name', '').split(':')[0] for m in models]
                logger.info(f"  Available models: {', '.join(model_names)}")
            else:
                raise ConnectionError(f"Ollama returned status {response.status_code}")
        except Exception as e:
            logger.error(f"‚úó Cannot connect to Ollama: {e}")
            logger.error("  Please ensure Ollama is running: ollama serve")
            raise
    
    def _call_ollama(self, prompt: str, system_prompt: str = "", temperature: float = 0.3, max_retries: int = 3) -> Optional[str]:
        """
        Call Ollama API with retry mechanism
        
        Args:
            prompt: User prompt
            system_prompt: System prompt for context
            temperature: Temperature for generation (0.0-1.0)
            max_retries: Maximum number of retries on timeout
        
        Returns:
            Generated text or None if failed
        """
        for attempt in range(max_retries):
            try:
                payload = {
                    "model": self.model,
                    "prompt": prompt,
                    "system": system_prompt,
                    "stream": False,
                    "temperature": temperature,
                    "top_p": 0.9,
                    "top_k": 40,
                }
                
                # Ë∂ÖÊó∂Êó∂Èó¥ 120 Áßí
                response = requests.post(self.api_endpoint, json=payload, timeout=120)
                response.raise_for_status()
                
                result = response.json()
                return result.get('response', '').strip()
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    logger.warning(f"Ollama timeout (attempt {attempt + 1}/{max_retries}), retrying...")
                    continue
                else:
                    logger.error(f"Ollama API timeout after {max_retries} attempts")
                    return None
            except Exception as e:
                logger.error(f"Ollama API error (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    continue
                return None
    
    def extract_concepts(self, text: str, chunk_id: str = "") -> Optional[List[Dict]]:
        """
        Extract concepts from text chunk
        
        Args:
            text: Text chunk to extract concepts from
            chunk_id: Unique identifier for the chunk
        
        Returns:
            List of concept dictionaries with format:
            {
                "entity": str,
                "importance": int (1-5),
                "category": str,
                "chunk_id": str
            }
        """
        # È¢ÜÂüüÁâπÂÆöÁöÑÊèêÁ§∫ËØç
        system_prompt = """‰Ω†ÊòØÊùæÊùêÁ∫øËô´ÁóÖÁü•ËØÜÂõæË∞±ÊûÑÂª∫‰∏ìÂÆ∂„ÄÇ‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÂÖ∑‰ΩìÁöÑÈ¢ÜÂüüÊ¶ÇÂøµ„ÄÇ

ÈáçÁÇπÂÖ≥Ê≥®:
- ÁóÖÂéü‰Ωì: ÊùæÊùêÁ∫øËô´„ÄÅ‰º¥ÁîüÁªÜËèå„ÄÅÁ∫øËô´ÁßçÁ±ª
- ÂØÑ‰∏ªÊ§çÁâ©: ÊùæÊ†ë„ÄÅÈ©¨Â∞æÊùæ„ÄÅÈªëÊùæ„ÄÅÊπøÂú∞Êùæ
- Â™í‰ªãÊòÜËô´: ÊùæË§êÂ§©Áâõ„ÄÅ‰∫ëÊùâËä±Â¢®Â§©Áâõ
- ÁóÖÂÆ≥ÁóáÁä∂: ËêéËî´„ÄÅÊûØÊ≠ª„ÄÅÂèòËâ≤„ÄÅÈíàÂè∂ËÑ±ËêΩ
- Èò≤Ê≤ªÊé™ÊñΩ: ËçØÂâÇ„ÄÅËØ±ÊçïÂô®„ÄÅÁîüÁâ©Èò≤Ê≤ª
- ÁéØÂ¢ÉÂõ†Â≠ê: Ê∏©Â∫¶„ÄÅÊπøÂ∫¶„ÄÅÊµ∑Êãî„ÄÅÊ∞îÂÄô
- Âú∞ÁêÜ‰ΩçÁΩÆ: Áñ´Âå∫„ÄÅÂàÜÂ∏ÉÂå∫„ÄÅÁúÅ‰ªΩ

Á±ªÂà´: pathogen(ÁóÖÂéü), host(ÂØÑ‰∏ª), vector(Â™í‰ªã), symptom(ÁóáÁä∂), treatment(Èò≤Ê≤ª), environment(ÁéØÂ¢É), location(Âú∞ÁÇπ), mechanism(Êú∫Âà∂), compound(ÂåñÂêàÁâ©)

ÈÅøÂÖç: Ëøá‰∫éÈÄöÁî®ÁöÑËØç(Âõ†Á¥†„ÄÅËøáÁ®ã„ÄÅÊú∫Âà∂„ÄÅÊñπÊ≥ï)
Âè™ËøîÂõûJSONÊï∞ÁªÑÔºåÊó†ÂÖ∂‰ªñÊñáÂ≠ó„ÄÇ"""
        
        user_prompt = (
            f"‰ªé‰ª•‰∏ãÊñáÊú¨ÊèêÂèñÊùæÊùêÁ∫øËô´ÁóÖÁõ∏ÂÖ≥Ê¶ÇÂøµ:\n{text}\n\n"
            "JSONÊ†ºÂºè: [{'entity': 'Ê¶ÇÂøµÂêç', 'importance': 1-5, 'category': 'Á±ªÂà´'}, ...]"
        )
        
        response = self._call_ollama(user_prompt, system_prompt, temperature=0.1)
        
        if not response:
            return None
        
        try:
            # Try to extract JSON from response
            concepts = json.loads(response)
            
            # Validate and normalize
            valid_concepts = []
            for concept in concepts:
                if isinstance(concept, dict) and 'entity' in concept:
                    valid_concepts.append({
                        'entity': str(concept.get('entity', '')).lower().strip(),
                        'importance': min(5, max(1, int(concept.get('importance', 3)))),
                        'category': str(concept.get('category', 'misc')).lower(),
                        'chunk_id': chunk_id,
                        'type': 'concept'
                    })
            
            return valid_concepts if valid_concepts else None
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse JSON response for chunk {chunk_id}")
            return None
    
    def extract_relationships(self, text: str, chunk_id: str = "") -> Optional[List[Dict]]:
        """
        Extract semantic relationships between concepts
        
        Args:
            text: Text chunk to extract relationships from
            chunk_id: Unique identifier for the chunk
        
        Returns:
            List of relationship dictionaries with format:
            {
                "node_1": str,
                "node_2": str,
                "edge": str (relationship description),
                "weight": float (W1 - LLM-extracted relationship weight),
                "chunk_id": str
            }
        """
        # È¢ÜÂüüÁâπÂÆöÂÖ≥Á≥ªÊèêÂèñÊèêÁ§∫ËØç
        system_prompt = """ÊèêÂèñÊùæÊùêÁ∫øËô´ÁóÖÁõ∏ÂÖ≥ÂÆû‰ΩìÈó¥ÁöÑÂÖ∑‰ΩìÂÖ≥Á≥ª„ÄÇ

ÂÖ≥Á≥ªÁ±ªÂûã:
- ÂºïËµ∑/ÂØºËá¥: ÁóÖÂéüÂºïËµ∑ÁóáÁä∂„ÄÅÁéØÂ¢ÉÂØºËá¥ÂèëÁóÖ
- ‰º†Êí≠: Â™í‰ªã‰º†Êí≠ÁóÖÂéü
- ÂØÑÁîü‰∫é: ÁóÖÂéüÂØÑÁîü‰∫éÂØÑ‰∏ª
- ÊÑüÊüì: ÁóÖÂéüÊÑüÊüìÂØÑ‰∏ª
- Èò≤Ê≤ª: ËçØÂâÇÈò≤Ê≤ªÁóÖÂÆ≥„ÄÅÊé™ÊñΩÊéßÂà∂Â™í‰ªã
- ÂΩ±Âìç: ÁéØÂ¢ÉÂΩ±ÂìçÂèëÁóÖ„ÄÅÂõ†Á¥†ÂΩ±Âìç‰º†Êí≠
- ÂàÜÂ∏É‰∫é: ÁóÖÂÆ≥ÂàÜÂ∏É‰∫éÂú∞Âå∫
- Êê∫Â∏¶: Â™í‰ªãÊê∫Â∏¶ÁóÖÂéü
- ÊäëÂà∂: ËçØÂâÇÊäëÂà∂ÁóÖÂéü

Âè™ÊèêÂèñÊòéÁ°ÆÁöÑÂõ†Êûú„ÄÅÂäüËÉΩÂÖ≥Á≥ªÔºåÈÅøÂÖçÊ®°Á≥äÁöÑÂÖ±Áé∞ÂÖ≥Á≥ª„ÄÇ
Âè™ËøîÂõûJSONÊï∞ÁªÑ„ÄÇ"""
        
        user_prompt = (
            f"‰ªé‰ª•‰∏ãÊñáÊú¨ÊèêÂèñÂÆû‰ΩìÂÖ≥Á≥ª:\n{text}\n\n"
            "JSONÊ†ºÂºè: [{'node_1': 'ÂÆû‰Ωì1', 'node_2': 'ÂÆû‰Ωì2', 'edge': 'ÂÖ≥Á≥ªÁ±ªÂûã'}, ...]"
        )
        
        response = self._call_ollama(user_prompt, system_prompt, temperature=0.2)
        
        if not response:
            return None
        
        try:
            relationships = json.loads(response)
            
            valid_relationships = []
            for rel in relationships:
                if isinstance(rel, dict) and 'node_1' in rel and 'node_2' in rel:
                    valid_relationships.append({
                        'node_1': str(rel.get('node_1', '')).lower().strip(),
                        'node_2': str(rel.get('node_2', '')).lower().strip(),
                        'edge': str(rel.get('edge', 'related to')).strip(),
                        'weight': 0.8,  # W1 weight for LLM-extracted relationships
                        'chunk_id': chunk_id,
                        'source': 'llm'
                    })
            
            return valid_relationships if valid_relationships else None
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse relationships JSON for chunk {chunk_id}")
            return None
    
    def extract_concepts_and_relationships(self, text: str, chunk_id: str = "") -> Tuple[Optional[List[Dict]], Optional[List[Dict]]]:
        """
        Extract both concepts and relationships in a single LLM call (FASTER)
        
        Args:
            text: Text chunk to extract from
            chunk_id: Unique identifier for the chunk
        
        Returns:
            Tuple of (concepts_list, relationships_list)
        """
        system_prompt = """‰Ω†ÊòØÊùæÊùêÁ∫øËô´ÁóÖÁü•ËØÜÂõæË∞±ÊûÑÂª∫‰∏ìÂÆ∂„ÄÇ‰ªéÊñáÊú¨‰∏≠ÂêåÊó∂ÊèêÂèñÊ¶ÇÂøµÂíåÂÖ≥Á≥ª„ÄÇ

ÈáçÁÇπÂÖ≥Ê≥®:
- ÁóÖÂéü‰Ωì: ÊùæÊùêÁ∫øËô´„ÄÅ‰º¥ÁîüÁªÜËèå
- ÂØÑ‰∏ª: È©¨Â∞æÊùæ„ÄÅÈªëÊùæ„ÄÅÊπøÂú∞Êùæ
- Â™í‰ªã: ÊùæË§êÂ§©Áâõ„ÄÅ‰∫ëÊùâËä±Â¢®Â§©Áâõ
- ÁóáÁä∂: ËêéËî´„ÄÅÊûØÊ≠ª„ÄÅÂèòËâ≤
- Èò≤Ê≤ª: ËçØÂâÇ„ÄÅËØ±ÊçïÂô®„ÄÅÁîüÁâ©Èò≤Ê≤ª
- ÁéØÂ¢É: Ê∏©Â∫¶„ÄÅÊπøÂ∫¶„ÄÅÊµ∑Êãî
- Âú∞ÁÇπ: Áñ´Âå∫„ÄÅÂàÜÂ∏ÉÂå∫

ÂÖ≥Á≥ªÁ±ªÂûã: ÂºïËµ∑„ÄÅ‰º†Êí≠„ÄÅÂØÑÁîü‰∫é„ÄÅÊÑüÊüì„ÄÅÈò≤Ê≤ª„ÄÅÂΩ±Âìç„ÄÅÂàÜÂ∏É‰∫é„ÄÅÊê∫Â∏¶„ÄÅÊäëÂà∂

Âè™ËøîÂõûJSONÔºåÊó†ÂÖ∂‰ªñÊñáÂ≠ó„ÄÇ"""
        
        user_prompt = f"""‰ªé‰ª•‰∏ãÊñáÊú¨ÊèêÂèñÊ¶ÇÂøµÂíåÂÖ≥Á≥ª:
{text}

ËøîÂõûÊ†ºÂºè:
{{
  "concepts": [{{"entity": "ÂêçÁß∞", "importance": 1-5, "category": "Á±ªÂà´"}}],
  "relationships": [{{"node_1": "ÂÆû‰Ωì1", "node_2": "ÂÆû‰Ωì2", "edge": "ÂÖ≥Á≥ª"}}]
}}"""
        
        response = self._call_ollama(user_prompt, system_prompt, temperature=0.1)
        
        if not response:
            return None, None
        
        try:
            data = json.loads(response)
            
            # Parse concepts
            concepts = []
            for c in data.get('concepts', []):
                if isinstance(c, dict) and 'entity' in c:
                    concepts.append({
                        'entity': str(c.get('entity', '')).lower().strip(),
                        'importance': min(5, max(1, int(c.get('importance', 3)))),
                        'category': str(c.get('category', 'misc')).lower(),
                        'chunk_id': chunk_id,
                        'type': 'concept'
                    })
            
            # Parse relationships
            relationships = []
            for r in data.get('relationships', []):
                if isinstance(r, dict) and 'node_1' in r and 'node_2' in r:
                    relationships.append({
                        'node_1': str(r.get('node_1', '')).lower().strip(),
                        'node_2': str(r.get('node_2', '')).lower().strip(),
                        'edge': str(r.get('edge', 'related to')).strip(),
                        'weight': 0.8,
                        'chunk_id': chunk_id,
                        'source': 'llm'
                    })
            
            return (concepts if concepts else None, 
                    relationships if relationships else None)
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse JSON for chunk {chunk_id}")
            return None, None
    
    def extract_from_chunks(self, chunks: List[Dict], max_chunks: int = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Extract concepts and relationships from multiple chunks
        
        Args:
            chunks: List of chunk dictionaries with 'text' and 'chunk_id' keys
            max_chunks: Maximum number of chunks to process (None = all chunks)
        
        Returns:
            Tuple of (concepts_df, relationships_df)
        """
        # Limit chunks if max_chunks is specified
        if max_chunks and len(chunks) > max_chunks:
            logger.warning(f"‚ö†Ô∏è  Limiting processing to {max_chunks} chunks (out of {len(chunks)})")
            logger.warning(f"    Set max_chunks=None in code to process all chunks")
            chunks = chunks[:max_chunks]
        
        all_concepts = []
        all_relationships = []
        
        logger.info(f"üöÄ Extracting concepts and relationships from {len(chunks)} chunks...")
        logger.info(f"‚ö° OPTIMIZED: Single LLM call per chunk (2x faster)")
        logger.info(f"‚è±Ô∏è  Timeout: 120 seconds per request")
        logger.info(f"üîÑ Retry: 3 attempts per chunk")
        logger.info(f"üí° Estimated time: ~{len(chunks) * 15} seconds (15s per chunk)")
        
        successful_chunks = 0
        failed_chunks = 0
        
        for i, chunk in enumerate(tqdm(chunks, desc="Processing chunks"), 1):
            text = chunk.get('text', '')
            chunk_id = chunk.get('chunk_id', '')
            
            if not text or len(text.strip()) < 20:
                continue
            
            logger.debug(f"[{i}/{len(chunks)}] Processing chunk: {chunk_id}")
            
            # Extract both concepts and relationships in ONE call
            concepts, relationships = self.extract_concepts_and_relationships(text, chunk_id)
            
            if concepts:
                all_concepts.extend(concepts)
                logger.debug(f"  ‚úì Extracted {len(concepts)} concepts")
            else:
                logger.debug(f"  ‚ö† No concepts extracted")
                failed_chunks += 1
                continue
            
            if relationships:
                all_relationships.extend(relationships)
                logger.debug(f"  ‚úì Extracted {len(relationships)} relationships")
            
            successful_chunks += 1
        
        logger.info(f"‚úì Extraction complete: {successful_chunks} successful, {failed_chunks} failed")
        logger.info(f"  Total concepts: {len(all_concepts)}")
        logger.info(f"  Total relationships: {len(all_relationships)}")
        
        concepts_df = pd.DataFrame(all_concepts) if all_concepts else pd.DataFrame()
        relationships_df = pd.DataFrame(all_relationships) if all_relationships else pd.DataFrame()
        
        logger.info(f"Extracted {len(concepts_df)} concepts and {len(relationships_df)} relationships")
        
        return concepts_df, relationships_df


class ContextualProximityAnalyzer:
    """Analyze contextual proximity between concepts in chunks"""
    
    @staticmethod
    def extract_proximity_relationships(chunks: List[Dict]) -> pd.DataFrame:
        """
        Extract relationships based on contextual proximity (W2 weight)
        
        Concepts that appear in the same chunk are considered related by proximity
        
        Args:
            chunks: List of chunk dictionaries with extracted concepts
        
        Returns:
            DataFrame of proximity-based relationships
        """
        proximity_relationships = []
        
        for chunk in chunks:
            concepts = chunk.get('concepts', [])
            chunk_id = chunk.get('chunk_id', '')
            
            # Create pairwise relationships for all concepts in the chunk
            for i, concept1 in enumerate(concepts):
                for concept2 in concepts[i+1:]:
                    if concept1 != concept2:
                        proximity_relationships.append({
                            'node_1': concept1.lower(),
                            'node_2': concept2.lower(),
                            'edge': 'co-occurs in',
                            'weight': 0.5,  # W2 weight for contextual proximity
                            'chunk_id': chunk_id,
                            'source': 'proximity'
                        })
        
        return pd.DataFrame(proximity_relationships) if proximity_relationships else pd.DataFrame()
    
    @staticmethod
    def merge_relationships(llm_relationships: pd.DataFrame, 
                          proximity_relationships: pd.DataFrame) -> pd.DataFrame:
        """
        Merge LLM-extracted and proximity-based relationships
        
        Rules:
        - Group by (node_1, node_2) pairs
        - Sum weights
        - Concatenate relationship descriptions
        
        Args:
            llm_relationships: DataFrame from LLM extraction
            proximity_relationships: DataFrame from proximity analysis
        
        Returns:
            Merged and aggregated relationships
        """
        if llm_relationships.empty and proximity_relationships.empty:
            return pd.DataFrame()
        
        # Combine both relationship types
        all_relationships = pd.concat(
            [llm_relationships, proximity_relationships],
            ignore_index=True
        )
        
        # Group by node pairs and aggregate
        merged = all_relationships.groupby(['node_1', 'node_2']).agg({
            'weight': 'sum',
            'edge': lambda x: ' | '.join(x.unique()),
            'chunk_id': lambda x: ','.join(x.unique()),
            'source': lambda x: ','.join(x.unique())
        }).reset_index()
        
        # Normalize weights to 0-1 range
        max_weight = merged['weight'].max()
        if max_weight > 0:
            merged['weight'] = merged['weight'] / max_weight
        
        return merged
