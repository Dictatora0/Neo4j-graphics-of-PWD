#!/usr/bin/env python3
"""
çŸ¥è¯†å›¾è°±æ•°æ®æ¸…æ´—å’Œä¼˜åŒ–
ä¿®å¤ï¼šä¹±ç å®ä½“ã€ç©ºå®ä½“ã€å¤æ‚å…³ç³»ç±»å‹ã€ç±»åˆ«åˆ†å¸ƒä¸å‡
"""
import pandas as pd
import re
from collections import Counter

print("="*80)
print("çŸ¥è¯†å›¾è°±æ•°æ®æ¸…æ´—å’Œä¼˜åŒ–")
print("="*80)

# ============================================================================
# 1. è¯»å–åŸå§‹æ•°æ®
# ============================================================================
print("\nğŸ“– è¯»å–åŸå§‹æ•°æ®...")
concepts_df = pd.read_csv('output/concepts.csv')
relationships_df = pd.read_csv('output/relationships.csv')

print(f"  åŸå§‹æ¦‚å¿µæ•°: {len(concepts_df)}")
print(f"  åŸå§‹å…³ç³»æ•°: {len(relationships_df)}")

# ============================================================================
# 2. æ¸…ç†å®ä½“
# ============================================================================
print("\nğŸ§¹ æ¸…ç†å®ä½“...")

# 2.1 ç§»é™¤ç©ºå®ä½“
empty_mask = concepts_df['entity'].isna() | (concepts_df['entity'].astype(str).str.strip() == '') | (concepts_df['entity'].astype(str) == 'nan')
empty_count = empty_mask.sum()
concepts_df = concepts_df[~empty_mask]
print(f"  âœ“ ç§»é™¤ç©ºå®ä½“: {empty_count} ä¸ª")

# 2.2 ç§»é™¤ä¹±ç å®ä½“ï¼ˆåŒ…å«\uè½¬ä¹‰åºåˆ—ï¼‰
garbled_mask = concepts_df['entity'].astype(str).str.contains(r'\\u[0-9a-f]{4}', regex=True, na=False)
garbled_entities = concepts_df[garbled_mask]['entity'].tolist()
garbled_count = garbled_mask.sum()
concepts_df = concepts_df[~garbled_mask]
print(f"  âœ“ ç§»é™¤ä¹±ç å®ä½“: {garbled_count} ä¸ª")
if garbled_entities:
    print(f"    ç¤ºä¾‹: {garbled_entities[:3]}")

# 2.3 ç§»é™¤è¿‡çŸ­å®ä½“ï¼ˆ<2å­—ç¬¦ï¼‰
short_mask = concepts_df['entity'].astype(str).str.len() < 2
short_count = short_mask.sum()
concepts_df = concepts_df[~short_mask]
print(f"  âœ“ ç§»é™¤è¿‡çŸ­å®ä½“: {short_count} ä¸ª")

# 2.4 ç§»é™¤é‡å¤å®ä½“
dup_count = concepts_df.duplicated(subset=['entity']).sum()
concepts_df = concepts_df.drop_duplicates(subset=['entity'], keep='first')
print(f"  âœ“ ç§»é™¤é‡å¤å®ä½“: {dup_count} ä¸ª")

# 2.5 æ”¹è¿›ç±»åˆ«åˆ†ç±»
print("\nğŸ·ï¸  æ”¹è¿›å®ä½“ç±»åˆ«...")

# å®šä¹‰ç±»åˆ«æ˜ å°„è§„åˆ™
category_rules = {
    'ç–¾ç—…': ['ç—…', 'ç—…å®³', 'disease', 'pwd', 'çº¿è™«ç—…'],
    'ç—…åŸä½“': ['çº¿è™«', 'nematode', 'xylophilus', 'bursaphelenchus', 'ç»†èŒ', 'bacteria'],
    'åª’ä»‹': ['å¤©ç‰›', 'beetle', 'monochamus', 'alternatus', 'å¢¨å¤©ç‰›', 'è¤æ¢—'],
    'å¯„ä¸»': ['æ¾', 'pine', 'pinus', 'é©¬å°¾æ¾', 'é»‘æ¾', 'æ¹¿åœ°æ¾', 'thunbergii', 'massoniana'],
    'ç—‡çŠ¶': ['å¶ç‰‡', 'æ¯è', 'å˜è‰²', 'èè”«', 'é’ˆå¶', 'leaf', 'symptom'],
    'é˜²æ²»': ['é˜²æ²»', 'è¯±æ•', 'ç”Ÿç‰©é˜²æ²»', 'è¯å‰‚', 'control', 'treatment', 'æ²»ç–—'],
    'ç¯å¢ƒ': ['æ¸©åº¦', 'æ¹¿åº¦', 'æµ·æ‹”', 'æ°”å€™', 'temperature', 'climate', 'é™æ°´'],
    'åœ°ç‚¹': ['ç–«åŒº', 'åˆ†å¸ƒåŒº', 'é£æ™¯åŒº', 'å¿', 'å¸‚', 'çœ', 'area', 'region'],
    'æŠ€æœ¯': ['å…‰è°±', 'é¥æ„Ÿ', 'ç›‘æµ‹', 'spectral', 'sentinel', 'detection', 'æ•°æ®'],
}

def classify_entity(entity_name, current_category):
    """æ ¹æ®å®ä½“åç§°é‡æ–°åˆ†ç±»"""
    entity_lower = str(entity_name).lower()
    
    # å¦‚æœå½“å‰ç±»åˆ«ä¸æ˜¯"å…¶ä»–"ï¼Œä¿æŒåŸåˆ†ç±»
    if current_category not in ['å…¶ä»–', 'misc', 'other']:
        return current_category
    
    # æ ¹æ®è§„åˆ™é‡æ–°åˆ†ç±»
    for category, keywords in category_rules.items():
        for keyword in keywords:
            if keyword in entity_lower:
                return category
    
    return 'å…¶ä»–'

concepts_df['category'] = concepts_df.apply(
    lambda row: classify_entity(row['entity'], row['category']), 
    axis=1
)

# ç»Ÿè®¡æ–°çš„ç±»åˆ«åˆ†å¸ƒ
new_category_dist = concepts_df['category'].value_counts()
print("  æ–°ç±»åˆ«åˆ†å¸ƒ:")
for cat, count in new_category_dist.items():
    pct = count / len(concepts_df) * 100
    print(f"    {cat:15s}: {count:3d} ({pct:5.1f}%)")

# ============================================================================
# 3. æ¸…ç†å…³ç³»
# ============================================================================
print("\nğŸ”— æ¸…ç†å…³ç³»...")

# 3.1 ç§»é™¤æ¶‰åŠå·²åˆ é™¤å®ä½“çš„å…³ç³»
valid_entities = set(concepts_df['entity'].astype(str))
before_rel_count = len(relationships_df)

relationships_df = relationships_df[
    relationships_df['node_1'].astype(str).isin(valid_entities) &
    relationships_df['node_2'].astype(str).isin(valid_entities)
]
removed_rel_count = before_rel_count - len(relationships_df)
print(f"  âœ“ ç§»é™¤æ— æ•ˆå…³ç³»: {removed_rel_count} ä¸ª")

# 3.2 ç®€åŒ–å¤æ‚å…³ç³»ç±»å‹
print("\nğŸ¯ ç®€åŒ–å…³ç³»ç±»å‹...")

def simplify_edge(edge_str):
    """ç®€åŒ–å¤æ‚çš„å…³ç³»ç±»å‹"""
    edge = str(edge_str)
    
    # å¦‚æœå…³ç³»ç±»å‹è¿‡é•¿æˆ–åŒ…å«å¤šä¸ª"|"ï¼Œæå–ä¸»è¦å…³ç³»
    if len(edge) > 50 or edge.count('|') > 2:
        # æå–å…³é”®å…³ç³»è¯
        parts = [p.strip() for p in edge.split('|')]
        
        # ä¼˜å…ˆçº§å…³ç³»è¯
        priority_relations = ['å¯„ç”Ÿäº', 'ä¼ æ’­', 'å¼•èµ·', 'æ„ŸæŸ“', 'åª’ä»‹', 'é˜²æ²»', 'å½±å“']
        
        for rel in priority_relations:
            if rel in parts:
                return rel
        
        # å¦‚æœæ²¡æœ‰ä¼˜å…ˆå…³ç³»ï¼Œè¿”å›ç¬¬ä¸€ä¸ªéco-occursçš„å…³ç³»
        for part in parts:
            if part != 'co-occurs in' and part.strip():
                return part.strip()
        
        return 'co-occurs in'
    
    return edge

relationships_df['edge_original'] = relationships_df['edge']
relationships_df['edge'] = relationships_df['edge'].apply(simplify_edge)

# ç»Ÿè®¡ç®€åŒ–æ•ˆæœ
simplified_count = (relationships_df['edge'] != relationships_df['edge_original']).sum()
print(f"  âœ“ ç®€åŒ–å¤æ‚å…³ç³»: {simplified_count} ä¸ª")

# 3.3 åˆå¹¶é‡å¤å…³ç³»ï¼ˆä¿ç•™æœ€é«˜æƒé‡ï¼‰
print("\nğŸ”„ åˆå¹¶é‡å¤å…³ç³»...")
before_merge = len(relationships_df)

relationships_df = relationships_df.groupby(['node_1', 'node_2', 'edge'], as_index=False).agg({
    'weight': 'max',
    'source': lambda x: ','.join(sorted(set(','.join(x).split(',')))),
    'chunk_id': 'first'
})

merged_count = before_merge - len(relationships_df)
print(f"  âœ“ åˆå¹¶é‡å¤å…³ç³»: {merged_count} ä¸ª")

# ============================================================================
# 4. ç»Ÿè®¡æ¸…æ´—ç»“æœ
# ============================================================================
print("\nğŸ“Š æ¸…æ´—ç»“æœç»Ÿè®¡")
print("="*80)

print(f"\nå®ä½“:")
print(f"  æ¸…æ´—å‰: {len(pd.read_csv('output/concepts.csv'))} ä¸ª")
print(f"  æ¸…æ´—å: {len(concepts_df)} ä¸ª")
print(f"  ç§»é™¤: {len(pd.read_csv('output/concepts.csv')) - len(concepts_df)} ä¸ª")

print(f"\nå…³ç³»:")
print(f"  æ¸…æ´—å‰: {len(pd.read_csv('output/relationships.csv'))} ä¸ª")
print(f"  æ¸…æ´—å: {len(relationships_df)} ä¸ª")
print(f"  ç§»é™¤/åˆå¹¶: {len(pd.read_csv('output/relationships.csv')) - len(relationships_df)} ä¸ª")

# å…³ç³»ç±»å‹åˆ†å¸ƒ
print(f"\nå…³ç³»ç±»å‹åˆ†å¸ƒï¼ˆå‰10ï¼‰:")
edge_counts = relationships_df['edge'].value_counts()
for edge, count in edge_counts.head(10).items():
    pct = count / len(relationships_df) * 100
    edge_display = edge if len(edge) <= 30 else edge[:27] + "..."
    print(f"  {edge_display:32s}: {count:3d} ({pct:5.1f}%)")

# å…³ç³»æ¥æºåˆ†å¸ƒ
print(f"\nå…³ç³»æ¥æºåˆ†å¸ƒ:")
source_counts = relationships_df['source'].value_counts()
for source, count in source_counts.items():
    pct = count / len(relationships_df) * 100
    print(f"  {source:25s}: {count:3d} ({pct:5.1f}%)")

# ============================================================================
# 5. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
# ============================================================================
print("\nğŸ’¾ ä¿å­˜æ¸…æ´—åçš„æ•°æ®...")

# ä¿å­˜åˆ°æ–°æ–‡ä»¶
concepts_df.to_csv('output/concepts_cleaned.csv', index=False, encoding='utf-8-sig')
relationships_df.to_csv('output/relationships_cleaned.csv', index=False, encoding='utf-8-sig')

print(f"  âœ“ å·²ä¿å­˜: output/concepts_cleaned.csv")
print(f"  âœ“ å·²ä¿å­˜: output/relationships_cleaned.csv")

# åŒæ—¶æ›´æ–°Neo4jå¯¼å…¥æ–‡ä»¶
print("\nğŸ”„ æ›´æ–°Neo4jå¯¼å…¥æ–‡ä»¶...")

# ç”Ÿæˆnodes.csv
nodes_df = pd.DataFrame({
    'id': range(len(concepts_df)),
    'name': concepts_df['entity'],
    'label': concepts_df['category']
})
nodes_df.to_csv('output/neo4j_import/nodes_cleaned.csv', index=False, encoding='utf-8-sig')

# ç”Ÿæˆrelations.csv
relations_df = pd.DataFrame({
    'start_id': relationships_df['node_1'],
    'relation': relationships_df['edge'],
    'end_id': relationships_df['node_2'],
    'confidence': relationships_df['weight']
})
relations_df.to_csv('output/neo4j_import/relations_cleaned.csv', index=False, encoding='utf-8-sig')

print(f"  âœ“ å·²ä¿å­˜: output/neo4j_import/nodes_cleaned.csv")
print(f"  âœ“ å·²ä¿å­˜: output/neo4j_import/relations_cleaned.csv")

print("\n" + "="*80)
print("âœ“ æ•°æ®æ¸…æ´—å®Œæˆï¼")
print("="*80)

print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
print("  1. æŸ¥çœ‹æ¸…æ´—åçš„æ•°æ®: output/concepts_cleaned.csv")
print("  2. é‡æ–°å¯¼å…¥Neo4j: python3 import_graph_direct.py")
print("     (ä¿®æ”¹è„šæœ¬ä½¿ç”¨ *_cleaned.csv æ–‡ä»¶)")
print("  3. æˆ–è¿è¡Œ: python3 reimport_cleaned_graph.py")
