# å®ç°ç»†èŠ‚ä¸æ¨¡å—è¯´æ˜

> æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜æ¾æçº¿è™«ç—…çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿçš„ç«¯åˆ°ç«¯æ•°æ®æµã€æ ¸å¿ƒæ¨¡å—æºç ä½ç½®ä¸å…¸å‹è¿è¡Œåœºæ™¯ï¼Œä¸ºç†è§£ç³»ç»Ÿæ¶æ„ä¸ç”ŸæˆæŠ€æœ¯æ±‡æŠ¥ææ–™æä¾›å‚è€ƒã€‚

---

## æŠ€æœ¯æ ˆæ€»è§ˆ

| æŠ€æœ¯é¢†åŸŸ        | æ ¸å¿ƒæŠ€æœ¯/åº“                                   | ç‰ˆæœ¬è¦æ±‚ | ç”¨é€”è¯´æ˜              |
| --------------- | --------------------------------------------- | -------- | --------------------- |
| **Python ç¯å¢ƒ** | Python                                        | 3.10.13+ | åŸºç¡€è¿è¡Œç¯å¢ƒ          |
| **PDF å¤„ç†**    | PyMuPDF(fitz)                                 | latest   | åŸºç¡€ PDF è§£æ         |
|                 | pdfplumber                                    | latest   | è¡¨æ ¼æå–ä¼˜åŒ–          |
|                 | Marker                                        | latest   | AI é©±åŠ¨è§£æï¼ˆéœ€ GPUï¼‰ |
|                 | pytesseract                                   | latest   | OCR å›é€€æœºåˆ¶          |
| **LLM æ¨ç†**    | Ollama                                        | latest   | æœ¬åœ°æ¨¡å‹æœåŠ¡          |
|                 | qwen2.5-coder:7b                              | -        | æ¦‚å¿µ/å…³ç³»æŠ½å–         |
|                 | qwen2-vl                                      | -        | å¤šæ¨¡æ€å›¾ç‰‡ç†è§£        |
| **åµŒå…¥æ¨¡å‹**    | sentence-transformers                         | latest   | MiniLM-L6-v2 åŠ è½½     |
|                 | sentence-transformers/paraphrase-MiniLM-L6-v2 | -        | è¯­ä¹‰å»é‡              |
| **å›¾å¤„ç†**      | networkx                                      | latest   | å›¾ç»“æ„æ“ä½œ            |
|                 | scikit-learn                                  | latest   | èšç±»ç®—æ³•              |
| **æ•°æ®å­˜å‚¨**    | pandas                                        | latest   | CSV æ•°æ®å¤„ç†          |
|                 | Neo4j                                         | 5.x+     | å›¾æ•°æ®åº“              |
| **å¯è§†åŒ–**      | tqdm                                          | latest   | è¿›åº¦æ¡æ˜¾ç¤º            |
|                 | tabulate                                      | latest   | è¡¨æ ¼æ ¼å¼åŒ–            |
| **å…¶ä»–å·¥å…·**    | requests                                      | latest   | HTTP è¯·æ±‚             |
|                 | PyYAML                                        | latest   | é…ç½®æ–‡ä»¶è§£æ          |
|                 | FAISS                                         | latest   | å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢        |

---

## ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDFæ–‡çŒ®è¾“å…¥   â”‚â”€â”€â”€â–¶â”‚   æ–‡æœ¬æå–å±‚    â”‚â”€â”€â”€â–¶â”‚   æ–‡æœ¬é¢„å¤„ç†    â”‚
â”‚  ./æ–‡çŒ®/*.pdf   â”‚    â”‚  Marker/PyMuPDF â”‚    â”‚  æ¸…æ´—/åˆ†å—/OCR  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Neo4jå¯è§†åŒ–   â”‚â—€â”€â”€â”€â”‚   å›¾è°±æ„å»ºå±‚    â”‚â—€â”€â”€â”€â”‚   çŸ¥è¯†æŠ½å–å±‚    â”‚
â”‚  æ ·å¼/ç´¢å¼•/æŸ¥è¯¢ â”‚    â”‚  å»é‡/è¿‡æ»¤/å¯¼å…¥ â”‚    â”‚  LLM/BGE-M3     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   é«˜çº§åŠŸèƒ½æ‰©å±•  â”‚    â”‚   å®¹é”™ç›‘æ§å±‚    â”‚    â”‚   é…ç½®ç®¡ç†å±‚    â”‚
â”‚ GraphRAG/å¤šæ¨¡æ€ â”‚    â”‚ Checkpoint/æ—¥å¿— â”‚    â”‚  YAML/ç¯å¢ƒæ£€æŸ¥  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. ç«¯åˆ°ç«¯æ•°æ®æµï¼ˆä» PDF åˆ° Neo4jï¼‰

### å®Œæ•´æµç¨‹æ¦‚è§ˆ

```
PDFæ–‡çŒ®(./æ–‡çŒ®/*.pdf)
  â†“
[1] PDFExtractor.extract_from_directory
    â†’ Layout-Awareè§£æ(Marker/pdfplumber/PyMuPDF)
    â†’ ç»“æ„åŒ–æ¸…æ´—(é¡µçœ‰é¡µè„š/å‚è€ƒæ–‡çŒ®/è¡¨æ ¼è½¬æ–‡æœ¬)
    â†’ OCRå›é€€(Tesseract, æ–‡æœ¬é‡<500å­—ç¬¦æ—¶)
    â†’ è¿”å› {filename: cleaned_text}
  â†“
[2] _create_chunks
    â†’ å›ºå®šçª—å£3000å­—ç¬¦+é‡å 300å­—ç¬¦
    â†’ æ™ºèƒ½è¾¹ç•Œæ£€æµ‹(é¿å…åˆ‡æ–­å¥å­/æ®µè½)
    â†’ ç”Ÿæˆå”¯ä¸€chunk_id: {pdf_name}_{counter}
    â†’ è¿‡æ»¤ç©ºå—(<50å­—ç¬¦)ä¸çº¯æ•°å­—å—
  â†“
[3] _extract_with_checkpoints (tqdmè¿›åº¦æ¡)
    â†’ ConceptExtractor.extract_concepts_and_relationships
    â†’ Ollama APIè°ƒç”¨(qwen2.5-coder:7b, temperature=0.1)
    â†’ JSON SchemaéªŒè¯(9ç±»æ¦‚å¿µ, 5çº§é‡è¦æ€§)
    â†’ CheckpointManager.save_chunk_results (å¢é‡CSV)
    â†’ æ¯10å—ä¿å­˜å®Œæ•´checkpointå¿«ç…§
  â†“
[4] ContextualProximityAnalyzer
    â†’ æ»‘åŠ¨çª—å£å†…æ¦‚å¿µå…±ç°æ£€æµ‹
    â†’ ç”Ÿæˆco-occurså…³ç³»(weight=0.5)
    â†’ ä¸ºGraphRAGæä¾›ç¨ å¯†å›¾ç»“æ„
  â†“
[5] ConceptDeduplicator (BGE-M3)
    â†’ BGE-M3æ··åˆåµŒå…¥(dense+sparse, Î±=0.7)
    â†’ FAISSç›¸ä¼¼åº¦æ£€ç´¢(é˜ˆå€¼0.85)
    â†’ å±‚æ¬¡èšç±»ç®—æ³•å¯¹é½åŒä¹‰æ¦‚å¿µ
    â†’ RelationshipDeduplicatoræ›´æ–°å…³ç³»ç«¯ç‚¹
  â†“
[6] ConceptImportanceFilter
    â†’ åŸºäºLLMè¯„åˆ†(importanceâ‰¥2)
    â†’ åŸºäºå›¾è¿é€šåº¦(degreeâ‰¥1)
    â†’ ORé€»è¾‘ä¿ç•™é‡è¦æˆ–é«˜è¿æ¥æ¦‚å¿µ
  â†“
[7] _save_results
    â†’ UTF-8-SIGç¼–ç (Excelå‹å¥½)
    â†’ concepts.csv(5åˆ—): entity,importance,category,chunk_id,type
    â†’ relationships.csv(6åˆ—): node_1,node_2,edge,weight,chunk_id,source
  â†“
[8] import_to_neo4j_final.py (å¯é€‰)
    â†’ Cypheræ‰¹é‡å¯¼å…¥(UNWIND+MERGEä¼˜åŒ–)
    â†’ èŠ‚ç‚¹æ ·å¼(categoryâ†’color/icon)
    â†’ å…³ç³»æ ·å¼(edgeâ†’width/color)
    â†’ åˆ›å»ºç´¢å¼•(CREATE INDEX node_name)
    â†’ è®¡ç®—å›¾ç»Ÿè®¡(åº¦æ•°åˆ†å¸ƒ/æƒé‡åˆ†æ)
```

### è¯¦ç»†é˜¶æ®µè¯´æ˜

#### é˜¶æ®µ 1ï¼šPDF æ–‡æœ¬æå–

- **æ¨¡å—**ï¼š`pdf_extractor.PDFExtractor`
- **æ–¹æ³•**ï¼š`extract_from_directory(directory)`
- **è¾“å…¥**ï¼š`./æ–‡çŒ®/` ç›®å½•ä¸‹çš„æ‰€æœ‰ .pdf æ–‡ä»¶
- **å¤„ç†æµç¨‹**ï¼š
  1. **æ–‡ä»¶éå†ä¸éªŒè¯**ï¼š
     - ä½¿ç”¨ `glob.glob("*.pdf")` è·å–æ–‡ä»¶åˆ—è¡¨
     - éªŒè¯ PDF æ–‡ä»¶å®Œæ•´æ€§ï¼ˆPyMuPDF æ‰“å¼€æµ‹è¯•ï¼‰
     - è·³è¿‡æŸåæˆ–åŠ å¯†æ–‡ä»¶
  2. **ä¸‰çº§è§£æç­–ç•¥**ï¼ˆè‡ªé€‚åº”é€‰æ‹©ï¼‰ï¼š
     - **Level 1**: Markerï¼ˆAI é©±åŠ¨ï¼Œéœ€ GPUï¼‰
     ```python
     import marker
     markdown, metadata = marker.convert(pdf_path)
     text = self._extract_text_from_markdown(markdown)
     ```
     - **Level 2**: pdfplumberï¼ˆè¡¨æ ¼ä¼˜åŒ–ï¼‰
       ```python
       import pdfplumber
       with pdfplumber.open(pdf_path) as pdf:
           for page in pdf.pages:
               text += page.extract_text() or ""
               tables += page.extract_tables() or []
       ```
     - **Level 3**: PyMuPDFï¼ˆåŸºç¡€è§£æï¼‰
       ```python
       import fitz
       doc = fitz.open(pdf_path)
       text = "\n".join([page.get_text() for page in doc])
       ```
  3. **ç»“æ„åŒ–åå¤„ç†**ï¼š
     - **é¡µçœ‰é¡µè„šç§»é™¤**ï¼šåŸºäº Y åæ ‡ä½ç½®ï¼ˆ<5%æˆ–>95%é¡µé¢é«˜åº¦ï¼‰
     - **å‚è€ƒæ–‡çŒ®æ£€æµ‹**ï¼šæ­£åˆ™åŒ¹é…"å‚è€ƒæ–‡çŒ®"ã€"References"ç­‰å…³é”®è¯
     - **è¡¨æ ¼è½¬æè¿°**ï¼šä½¿ç”¨ `tabulate` åº“è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€
     - **æ–‡æœ¬è§„èŒƒåŒ–**ï¼šUnicode æ ‡å‡†åŒ–(NFKC)ã€å»é™¤æ§åˆ¶å­—ç¬¦
  4. **OCR å›é€€æœºåˆ¶**ï¼ˆæ–‡æœ¬é‡<500 å­—ç¬¦ï¼‰ï¼š
     ```python
     import pytesseract
     from PIL import Image
     images = pdf2image.convert_from_path(pdf_path)
     ocr_text = "\n".join([pytesseract.image_to_string(img, lang='chi_sim') for img in images])
     ```
- **è¾“å‡º**ï¼š`Dict[str, str]` æ ¼å¼ï¼Œå¦‚ `{"paper1.pdf": "æ¸…æ´—åçš„å®Œæ•´æ–‡æœ¬..."}`
- **æ—¥å¿—æ ¼å¼ç¤ºä¾‹**ï¼š
  ```
  INFO - æ‰¾åˆ° X ä¸ªPDFæ–‡ä»¶
  INFO - å¼€å§‹æå–: paperX.pdf
  INFO - ä½¿ç”¨pdfplumberè¿›è¡Œè¡¨æ ¼æå–å’Œä¼˜åŒ–è§£æ
  INFO - æå–å®Œæˆ: paperX.pdf, XXXXX å­—ç¬¦
  ```

#### é˜¶æ®µ 2ï¼šæ–‡æœ¬åˆ†å—

- **æ¨¡å—**ï¼š`enhanced_pipeline_safe.EnhancedKnowledgeGraphPipelineSafe`
- **æ–¹æ³•**ï¼š`_create_chunks(pdf_texts, chunk_size=3000, overlap=300)`
- **å¤„ç†é€»è¾‘**ï¼š

  ```python
  def _create_chunks(self, pdf_texts: Dict[str, str], chunk_size: int = 3000,
                    overlap: int = 300) -> List[Dict]:
      """åˆ†å—"""
      chunks = []
      chunk_id_counter = 0

      for pdf_name, text in pdf_texts.items():
          for i in range(0, len(text), chunk_size - overlap):
              chunk_text = text[i:i + chunk_size]

              if len(chunk_text.strip()) > 50:
                  chunks.append({
                      'text': chunk_text,
                      'chunk_id': f"{pdf_name}_{chunk_id_counter}",
                      'source_pdf': pdf_name,
                      'concepts': []
                  })
                  chunk_id_counter += 1

      return chunks
  ```

- **å…³é”®æŠ€æœ¯**ï¼š
  - **é‡å ç­–ç•¥**ï¼š300 å­—ç¬¦é‡å ç¡®ä¿ä¸Šä¸‹æ–‡è¿ç»­æ€§
  - **è´¨é‡è¿‡æ»¤**ï¼šè¿‡æ»¤é•¿åº¦å°äº 50 å­—ç¬¦çš„æ–‡æœ¬å—
  - **å”¯ä¸€æ ‡è¯†**ï¼šç”Ÿæˆæ ¼å¼ä¸º `{pdf_name}_{counter}` çš„ chunk_id
- **è¾“å‡º**ï¼š`List[Dict]`ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« `text`, `chunk_id`, `source_pdf`, `concepts`
- **æ—¥å¿—æ ¼å¼ç¤ºä¾‹**ï¼š
  ```
  INFO - Created XXX chunks
  ```

#### é˜¶æ®µ 3ï¼šLLM æ¦‚å¿µ/å…³ç³»æŠ½å–ï¼ˆæ ¸å¿ƒé˜¶æ®µï¼‰

- **æ¨¡å—**ï¼š`concept_extractor.ConceptExtractor`
- **æ–¹æ³•**ï¼š`extract_concepts_and_relationships(text, chunk_id)`
- **LLM é…ç½®**ï¼š
  - æ¨¡å‹ï¼š`qwen2.5-coder:7b` (é€šè¿‡ Ollama æœ¬åœ°æ¨ç†)
  - è¶…æ—¶ï¼š600 ç§’
  - æ¸©åº¦ï¼š0.1ï¼ˆä½éšæœºæ€§ï¼‰
  - JSON æ¨¡å¼ï¼šä¸¥æ ¼ Schema è¾“å‡º
- **Prompt ç»“æ„**ï¼š

  - **System Prompt**ï¼ˆè§’è‰²å®šä¹‰ï¼‰ï¼š

    ```python
    system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„æ¾æçº¿è™«ç—…çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç§‘å­¦æ–‡çŒ®ä¸­åŒæ—¶æå–æ¦‚å¿µå’Œå…³ç³»ã€‚

    ## è¾“å‡ºè¦æ±‚
    ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON Schema è¾“å‡ºï¼Œä¸å¾—æ·»åŠ ä»»ä½•è§£é‡Šæˆ– markdownï¼š

    {
      "concepts": [
        {"entity": "æ¦‚å¿µåç§°", "importance": 1-5æ•´æ•°, "category": "ç±»åˆ«"}
      ],
      "relationships": [
        {"node_1": "æºå®ä½“", "node_2": "ç›®æ ‡å®ä½“", "edge": "å…³ç³»ç±»å‹"}
      ]
    }

    ## æ¦‚å¿µæå–èŒƒå›´
    **ç—…åŸ** (pathogen): æ¾æçº¿è™«ã€Bursaphelenchus xylophilusã€ä¼´ç”Ÿç»†èŒ
    **å¯„ä¸»** (host): é©¬å°¾æ¾ã€é»‘æ¾ã€æ¹¿åœ°æ¾ã€èµ¤æ¾ã€äº‘å—æ¾
    **åª’ä»‹** (vector): æ¾è¤å¤©ç‰›ã€äº‘æ‰èŠ±å¢¨å¤©ç‰›ã€Monochamus alternatus
    **ç—‡çŠ¶** (symptom): èè”«ã€é’ˆå¶å˜è‰²ã€æ ‘è„‚åˆ†æ³Œå¼‚å¸¸ã€æ¯æ­»
    **é˜²æ²»** (treatment): é˜¿ç»´èŒç´ ã€å™»è™«å•‰ã€è¯±æ•å™¨ã€ç”Ÿç‰©é˜²æ²»
    **ç¯å¢ƒ** (environment): æ¸©åº¦ã€æ¹¿åº¦ã€é™æ°´ã€æµ·æ‹”
    **åœ°ç‚¹** (location): ç–«åŒºã€çœä»½ã€åˆ†å¸ƒåŒº
    **æœºåˆ¶** (mechanism): ä¾µæŸ“é€”å¾„ã€è‡´ç—…æœºç†
    **åŒ–åˆç‰©** (compound): èœçƒ¯ã€é…šç±»ã€æ€è™«å‰‚æˆåˆ†

    ## å…³ç³»ç±»å‹
    **å› æœ**: å¼•èµ·ã€å¯¼è‡´ã€è¯±å‘
    **ä¼ æ’­**: ä¼ æ’­ã€æºå¸¦ã€æ‰©æ•£
    **å¯„ç”Ÿ**: æ„ŸæŸ“ã€å¯„ç”Ÿäºã€ä¾µæŸ“
    **é˜²æ²»**: é˜²æ²»ã€æ§åˆ¶ã€æŠ‘åˆ¶ã€æ€ç­
    **å½±å“**: å½±å“ã€ä¿ƒè¿›ã€æŠ‘åˆ¶
    **åˆ†å¸ƒ**: åˆ†å¸ƒäºã€å‘ç”Ÿäº

    ## é‡è¦æ€§è¯„åˆ†
    5-æ ¸å¿ƒæ¦‚å¿µ, 4-é‡è¦æ¦‚å¿µ, 3-ä¸€èˆ¬æ¦‚å¿µ, 2-æ¬¡è¦æ¦‚å¿µ, 1-è¾¹ç¼˜æ¦‚å¿µ

    åªè¾“å‡º JSON å¯¹è±¡ï¼"""
    ```

  - **User Prompt**ï¼ˆä»»åŠ¡æŒ‡ä»¤ï¼‰ï¼š

    ```python
    user_prompt = f"""ä»ä»¥ä¸‹æ¾æçº¿è™«ç—…ç§‘å­¦æ–‡æœ¬ä¸­æå–æ¦‚å¿µå’Œå…³ç³»ï¼š

    {text}

    è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
    {{
      "concepts": [
        {{"entity": "æ¾æçº¿è™«", "importance": 5, "category": "pathogen"}},
        {{"entity": "æ¾è¤å¤©ç‰›", "importance": 5, "category": "vector"}},
        {{"entity": "é©¬å°¾æ¾", "importance": 4, "category": "host"}}
      ],
      "relationships": [
        {{"node_1": "æ¾æçº¿è™«", "node_2": "é©¬å°¾æ¾", "edge": "æ„ŸæŸ“"}},
        {{"node_1": "æ¾è¤å¤©ç‰›", "node_2": "æ¾æçº¿è™«", "edge": "ä¼ æ’­"}}
      ]
    }}"""
    ```

- **è°ƒç”¨æµç¨‹**ï¼š

  ```python
  for i, chunk in enumerate(tqdm(chunks, desc="Extracting concepts")):
      text = chunk.get('text', '')
      chunk_id = chunk.get('chunk_id', '')

      if not text or len(text.strip()) < 20:
          continue

      # æ ¸å¿ƒæŠ½å–
      concepts, relationships = self.concept_extractor.extract_concepts_and_relationships(
          text, chunk_id
      )

      if concepts:
          all_concepts.extend(concepts)
          logger.debug(f"Extracted {len(concepts)} concepts")

      if relationships:
          all_relationships.extend(relationships)
          logger.debug(f"Extracted {len(relationships)} relationships")

      # å¢é‡ä¿å­˜
      self.checkpoint_manager.save_chunk_results(chunk_id, concepts, relationships)

      # å®šæœŸå¿«ç…§
      if (i + 1) % self.checkpoint_interval == 0:
          logger.info(f"Checkpoint: {i+1}/{len(chunks)} chunks processed")
  ```

- **å®¹é”™å¤„ç†æœºåˆ¶**ï¼š

  - LLM è¿”å› None â†’ è½¬ä¸ºç©ºåˆ—è¡¨ `[]`
  - JSON è§£æå¤±è´¥ â†’ è®°å½•é”™è¯¯ï¼Œè¿”å› None
  - è¶…æ—¶/ç½‘ç»œé”™è¯¯ â†’ é‡è¯• 3 æ¬¡ï¼Œå¤±è´¥å continue

- **æ—¥å¿—æ ¼å¼ç¤ºä¾‹**ï¼ˆæ•°å€¼ä»…ä¸ºç¤ºæ„ï¼‰ï¼š
  ```
  INFO - Extracting concepts from XXX chunks...
  INFO - Processing chunks: X%|â–ˆâ–ˆâ–  | XX/XXX [mm:ss<hh:mm:ss, XX.XXs/it]
  INFO - Checkpoint: XX/XXX chunks processed
  DEBUG - Extracted X concepts, X relationships from chunk paperX.pdf_X
  ```

#### é˜¶æ®µ 4ï¼šå¢é‡ä¿å­˜ä¸è¿›åº¦è¿½è¸ª

- **æ¨¡å—**ï¼š`checkpoint_manager.CheckpointManager`
- **æ ¸å¿ƒæ–¹æ³•**ï¼š
  - `save_chunk_results(chunk_id, concepts, relationships)`ï¼šè¿½åŠ å†™å…¥ CSVï¼Œæ›´æ–°è¿›åº¦ JSON
  - `save_checkpoint(chunk_num, concepts_df, relationships_df)`ï¼šä¿å­˜å®Œæ•´å¿«ç…§
  - `get_processed_chunks()`ï¼šè¿”å›å·²å¤„ç†å—åˆ—è¡¨ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
- **æ–‡ä»¶ç»“æ„**ï¼š
  ```
  output/checkpoints/
  â”œâ”€â”€ .progress.json                              # è¿›åº¦è¿½è¸ª
  â”œâ”€â”€ concepts_incremental.csv                    # å¢é‡æ¦‚å¿µ
  â”œâ”€â”€ relationships_incremental.csv               # å¢é‡å…³ç³»
  â”œâ”€â”€ checkpoint_concepts_40_20251129_223048.csv  # å¿«ç…§
  â””â”€â”€ checkpoint_relationships_40_20251129_223048.csv
  ```
- **.progress.json æ ¼å¼**ï¼š
  ```json
  {
    "processed_chunks": ["paper1.pdf_0", "paper1.pdf_1", ...],
    "total_concepts": 320,
    "total_relationships": 250,
    "started_at": "2025-11-29T19:30:00",
    "last_update": "2025-11-29T20:48:15"
  }
  ```
- **æ–­ç‚¹ç»­ä¼ é€»è¾‘**ï¼š
  ```python
  processed_chunks = checkpoint_manager.get_processed_chunks()
  remaining = [c for c in chunks if c['chunk_id'] not in processed_chunks]
  logger.info(f"Skipping {len(processed_chunks)} already processed chunks")
  ```

#### é˜¶æ®µ 5ï¼šè¯­å¢ƒè¿‘é‚»å…³ç³»

- **æ¨¡å—**ï¼š`concept_extractor.ContextualProximityAnalyzer`
- **æ–¹æ³•**ï¼š`extract_proximity_relationships(chunks)`
- **é€»è¾‘**ï¼š

  ```python
  @staticmethod
  def extract_proximity_relationships(chunks: List[Dict]) -> pd.DataFrame:
      """Extract relationships based on contextual proximity"""
      proximity_relationships = []

      for chunk in chunks:
          concepts = chunk.get('concepts', [])
          chunk_id = chunk.get('chunk_id', '')

          # Create pairwise relationships for all concepts in the chunk
          for i, concept1 in enumerate(concepts):
              for concept2 in concepts[i+1:]:
                  if concept1 != concept2:
                      proximity_relationships.append({
                          'node_1': concept1.lower(),
                          'node_2': concept2.lower(),
                          'edge': 'co-occurs in',
                          'weight': 0.5,  # W2 weight for contextual proximity
                          'chunk_id': chunk_id,
                          'source': 'proximity'
                      })

      return pd.DataFrame(proximity_relationships) if proximity_relationships else pd.DataFrame()
  ```

- **ç›®çš„**ï¼šä¸º GraphRAG ç¤¾åŒºæ£€æµ‹æä¾›æ›´ç¨ å¯†çš„å›¾ç»“æ„

#### é˜¶æ®µ 6ï¼šè¯­ä¹‰å»é‡ä¸å®ä½“å¯¹é½

- **æ¨¡å—**ï¼š`concept_deduplicator.ConceptDeduplicator` + `SentenceTransformerEmbedding`
- **æ–¹æ³•**ï¼š`deduplicate_concepts(concepts_df)`
- **æŠ€æœ¯å®ç°**ï¼š

  ```python
  def deduplicate_concepts(self, concepts_df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str]]:
      """Deduplicate concepts based on semantic similarity"""
      if concepts_df.empty:
          return concepts_df, {}

      # Get unique concepts
      unique_concepts = concepts_df['entity'].unique()

      if len(unique_concepts) < 2:
          return concepts_df, {concept: concept for concept in unique_concepts}

      # Generate embeddings using sentence-transformers
      embeddings = self.embedding_provider.embed(list(unique_concepts))

      # Calculate similarity matrix
      similarity_matrix = cosine_similarity(embeddings)

      # Find duplicate clusters using hierarchical clustering
      clusters = self._cluster_similar_concepts(similarity_matrix, unique_concepts)

      # Create concept mapping
      concept_mapping = {}
      for cluster in clusters:
          if len(cluster) > 1:
              # Select canonical concept (highest importance)
              cluster_concepts = concepts_df[concepts_df['entity'].isin(cluster)]
              canonical = cluster_concepts.loc[cluster_concepts['importance'].idxmax(), 'entity']

              for concept in cluster:
                  if concept != canonical:
                      concept_mapping[concept] = canonical

      # Apply mapping to concepts
      deduplicated_df = concepts_df.copy()
      deduplicated_df['entity'] = deduplicated_df['entity'].map(
          lambda x: concept_mapping.get(x, x)
      )

      return deduplicated_df, concept_mapping
  ```

- **å…³ç³»æ›´æ–°ç­–ç•¥**ï¼š

  ```python
  @staticmethod
  def update_relationships(relationships_df: pd.DataFrame,
                          concept_mapping: Dict[str, str]) -> pd.DataFrame:
      """Update relationships to use canonical concept names"""
      # Map node_1 and node_2 to canonical names
      relationships_df['node_1'] = relationships_df['node_1'].map(
          lambda x: concept_mapping.get(x, x)
      )
      relationships_df['node_2'] = relationships_df['node_2'].map(
          lambda x: concept_mapping.get(x, x)
      )

      # Remove self-loops
      relationships_df = relationships_df[
          relationships_df['node_1'] != relationships_df['node_2']
      ]

      return relationships_df
  ```

- **æ•ˆæœç¤ºä¾‹**ï¼š
  - `"æ¾æçº¿è™«"` â†” `"Bursaphelenchus xylophilus"` â†’ `"æ¾æçº¿è™«"`
  - `"é©¬å°¾æ¾"` â†” `"Pinus massoniana"` â†’ `"é©¬å°¾æ¾"`
  - `"æ¾è¤å¤©ç‰›"` â†” `"Monochamus alternatus"` â†’ `"æ¾è¤å¤©ç‰›"`
- **æ—¥å¿—ç¤ºä¾‹**ï¼š
  ```
  INFO - Deduplicating XXX concepts...
  INFO - Using sentence-transformers for semantic similarity
  INFO - Updated relationships after deduplication: XXX relationships
  ```

#### é˜¶æ®µ 7ï¼šé‡è¦æ€§ä¸è¿é€šåº¦è¿‡æ»¤

- **æ¨¡å—**ï¼š`concept_deduplicator.ConceptImportanceFilter`
- **æ–¹æ³•**ï¼š`filter_concepts(concepts_df, relationships_df, min_importance=2, min_connections=1)`
- **è¿‡æ»¤é€»è¾‘**ï¼š

  ```python
  def filter_concepts(concepts_df: pd.DataFrame,
                     relationships_df: pd.DataFrame,
                     min_importance: int = 2,
                     min_connections: int = 1) -> pd.DataFrame:
      """Filter concepts based on importance and connectivity"""
      filtered_df = concepts_df.copy()

      # Filter out generic concepts
      initial_count = len(filtered_df)
      filtered_df = filtered_df[
          ~filtered_df['entity'].isin(self.GENERIC_CONCEPTS)
      ]

      # Filter by importance
      filtered_df = filtered_df[
          filtered_df['importance'] >= min_importance
      ]

      # Filter by connectivity (if relationships provided)
      if not relationships_df.empty:
          # Count connections for each concept
          node1_counts = relationships_df['node_1'].value_counts()
          node2_counts = relationships_df['node_2'].value_counts()
          connection_counts = (node1_counts + node2_counts).fillna(0)

          # Apply connectivity filter
          filtered_df = filtered_df[
              filtered_df['entity'].map(connection_counts) >= min_connections
          ]

      return filtered_df
  ```

- **ç›®çš„**ï¼šå»é™¤å­¤ç«‹èŠ‚ç‚¹ä¸ä½æƒé‡å™ªå£°

#### é˜¶æ®µ 8ï¼šç»“æœè½ç›˜

- **æ–‡ä»¶**ï¼š`output/concepts.csv`, `output/relationships.csv`
- **æ ¼å¼**ï¼šUTF-8-SIG ç¼–ç ï¼ˆExcel å‹å¥½ï¼‰ï¼ŒåŒ…å« header
- **concepts.csv åˆ—**ï¼š`entity, importance, category, chunk_id, type`
- **relationships.csv åˆ—**ï¼š`node_1, node_2, edge, weight, chunk_id, source`

#### é˜¶æ®µ 9ï¼šNeo4j å›¾è°±å¯¼å…¥ï¼ˆå¯é€‰ï¼‰

- **æ¨¡å—**ï¼š`neo4j_generator.Neo4jGenerator`
- **æµç¨‹**ï¼š
  1. è¯»å– concepts.csv å’Œ relationships.csv
  2. ç”Ÿæˆ Cypher å¯¼å…¥è„šæœ¬
  3. åˆ›å»ºçº¦æŸå’Œç´¢å¼•
  4. æ‰¹é‡å¯¼å…¥èŠ‚ç‚¹å’Œå…³ç³»
- **å…³é”®ä»£ç **ï¼š

  ```python
  class Neo4jGenerator:
      def __init__(self, config):
          self.config = config

      def generate_cypher_script(self, concepts_df, relationships_df):
          """Generate Cypher script for bulk import"""
          script = []

          # Create constraints
          script.append("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Concept) REQUIRE c.name IS UNIQUE")

          # Import nodes
          script.append("""
          UNWIND $concepts AS concept
          MERGE (c:Concept {name: concept.entity})
          SET c.importance = concept.importance,
              c.category = concept.category,
              c.chunk_id = concept.chunk_id
          """)

          # Import relationships
          script.append("""
          UNWIND $relationships AS rel
          MATCH (a:Concept {name: rel.node_1})
          MATCH (b:Concept {name: rel.node_2})
          MERGE (a)-[r:RELATED {type: rel.edge}]->(b)
          SET r.weight = rel.weight,
              r.chunk_id = rel.chunk_id
          """)

          return "\\n".join(script)
  ```

---

## 2. æ ¸å¿ƒæ¨¡å—ä¸æºç ä½ç½®é€ŸæŸ¥è¡¨

| åŠŸèƒ½é˜¶æ®µ              | ä¸»è¦æ–‡ä»¶                      | æ ¸å¿ƒç±»/å‡½æ•°                                            | è¯´æ˜                                           |
| --------------------- | ----------------------------- | ------------------------------------------------------ | ---------------------------------------------- |
| **PDF è§£æä¸ç»“æ„åŒ–**  | `pdf_extractor.py`            | `PDFExtractor`                                         | Layout-Awareã€è¡¨æ ¼è½¬æ–‡æœ¬ã€OCR å›é€€             |
| **æ–‡æœ¬åˆ†å—**          | `enhanced_pipeline_safe.py`   | `EnhancedKnowledgeGraphPipelineSafe._create_chunks`    | å›ºå®šçª—å£+overlap ç”Ÿæˆ chunk_id                 |
| **LLM æ¦‚å¿µ/å…³ç³»æŠ½å–** | `concept_extractor.py`        | `ConceptExtractor.extract_concepts_and_relationships`  | Ollama+Qwen2.5ï¼Œä¸¥æ ¼ JSON Schema               |
| **è¯­å¢ƒè¿‘é‚»å…³ç³»**      | `concept_extractor.py`        | `ContextualProximityAnalyzer`                          | å—å†…å…±ç°ç”Ÿæˆ co-occurs å…³ç³»                    |
| **è¯­ä¹‰å»é‡ä¸å¯¹é½**    | `concept_deduplicator.py`     | `ConceptDeduplicator`, `SentenceTransformerEmbedding`  | sentence-transformers è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå¯¹é½åŒä¹‰æ¦‚å¿µ |
| **å…³ç³»ç«¯ç‚¹æ›´æ–°**      | `concept_deduplicator.py`     | `RelationshipDeduplicator.update_relationships`        | å°†å…³ç³»ä¸¤ç«¯æ›¿æ¢ä¸ºè§„èŒƒå                         |
| **é‡è¦æ€§è¿‡æ»¤**        | `concept_deduplicator.py`     | `ConceptImportanceFilter.filter_concepts`              | åŸºäº importance ä¸åº¦æ•°ç­›é€‰                     |
| **å®‰å…¨æµæ°´çº¿**        | `enhanced_pipeline_safe.py`   | `EnhancedKnowledgeGraphPipelineSafe`                   | å¢é‡ä¿å­˜ã€æ–­ç‚¹ç»­ä¼ ã€å¤šå±‚å®¹é”™                   |
| **è¿›åº¦ç®¡ç†**          | `checkpoint_manager.py`       | `CheckpointManager`                                    | è¿½è¸ª processed_chunksï¼Œä¿å­˜å¿«ç…§                |
| **å¯åŠ¨ä¸ç¯å¢ƒæ£€æŸ¥**    | `run_pipeline.py`, `start.sh` | `main()`, shell è„šæœ¬                                   | æ£€æŸ¥ Ollamaã€æ¨¡å‹ã€ä¾èµ–ï¼Œæ‰“å°æ—¶é—´ä¼°ç®—          |
| **çŠ¶æ€ç›‘æ§**          | `status.sh`, `monitor.sh`     | shell è„šæœ¬                                             | è¯»å–.progress.jsonï¼Œå±•ç¤ºå®æ—¶çŠ¶æ€               |
| **Neo4j å¯¼å…¥**        | `import_to_neo4j_final.py`    | ä¸»æµç¨‹è„šæœ¬                                             | åº”ç”¨æ ·å¼ã€åˆ›å»ºç´¢å¼•ã€è®¡ç®—åº¦æ•°                   |
| **Cypher ç”Ÿæˆ**       | `neo4j_generator.py`          | `Neo4jGenerator`                                       | ç”Ÿæˆ nodes.csvã€relations.csvã€import.cypher   |
| **Agentic Workflow**  | `agentic_extractor.py`        | `AgenticExtractor`, `CriticAgent`, `RefineAgent`       | Extractâ†’Criticâ†’Refine ä¸‰é˜¶æ®µè´¨é‡æå‡           |
| **GraphRAG ç¤¾åŒºæ‘˜è¦** | `graph_rag.py`                | `GraphRAG`, `CommunityDetector`, `CommunitySummarizer` | Louvain/Leiden ç¤¾åŒºæ£€æµ‹+LLM æ‘˜è¦               |
| **å¤šæ¨¡æ€æ‰©å±•**        | `multimodal_extractor.py`     | `MultimodalExtractor`                                  | å›¾ç‰‡æè¿°ç”Ÿæˆï¼ˆQwen2-VLï¼‰ï¼Œå¯é€‰åŠŸèƒ½             |

---

## 3. å…¸å‹è¿è¡Œåœºæ™¯ä¸ç›‘æ§

### åœºæ™¯ 1ï¼šé¦–æ¬¡å®Œæ•´è¿è¡Œ

**æ­¥éª¤**ï¼š

1. å‡†å¤‡æ•°æ®ï¼šå°† PDF æ–‡çŒ®æ”¾å…¥ `./æ–‡çŒ®/` ç›®å½•
2. æ£€æŸ¥é…ç½®ï¼šç¼–è¾‘ `config/config.yaml`
   ```yaml
   llm:
     model: qwen2.5-coder:7b
     max_chunks: 100 # æˆ–nullè¡¨ç¤ºå…¨éƒ¨å¤„ç†
     timeout: 600
   deduplication:
     use_bge_m3: true
     similarity_threshold: 0.85
   ```
3. å¯åŠ¨æœåŠ¡ï¼š`ollama serve` ï¼ˆå¦ä¸€ç»ˆç«¯ï¼‰
4. å¯åŠ¨ç®¡é“ï¼š`./start.sh`
5. ç›‘æ§è¿›åº¦ï¼š`./monitor.sh` ï¼ˆå¦ä¸€ç»ˆç«¯ï¼‰

**é¢„æœŸè¾“å‡ºæ ¼å¼**ï¼ˆæ•°å€¼ä»…ä¸ºç¤ºæ„ï¼‰ï¼š

```
============================================================
çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿ v2.5
============================================================

[INFO] Pythonç‰ˆæœ¬: X.XX.XX
[INFO] OllamaæœåŠ¡: è¿è¡Œä¸­
[INFO] LLMæ¨¡å‹: qwen2.5-coder:7b
[INFO] PDFæ–‡ä»¶: XX ä¸ª

============================================================
æç¤º:
  - æŒ‰ Ctrl+C å¯å®‰å…¨é€€å‡ºå¹¶ä¿å­˜è¿›åº¦
  - åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ './monitor.sh' æŸ¥çœ‹è¿›åº¦
  - æ—¥å¿—æ–‡ä»¶: output/kg_builder.log
============================================================

[Step 1/6] Extracting text from PDFs...
æ‰¾åˆ° XX ä¸ªPDFæ–‡ä»¶
æå–PDFæ–‡æœ¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| XX/XX [mm:ss<00:00]

[Step 2/6] Splitting texts into chunks...
Created XXX chunks

[Step 3/6] Extracting concepts with checkpoint support...
Extracting concepts:  X%|â–ˆâ–ˆâ–         | XX/XXX [mm:ss<hh:mm:ss, XX.XXs/it]
Checkpoint: XX/XXX chunks processed
```

### åœºæ™¯ 2ï¼šä¸­é€”ä¸­æ–­åæ¢å¤

**åœºæ™¯æè¿°**ï¼šè¿è¡Œè¿‡ç¨‹ä¸­æŒ‰ Ctrl+C æˆ–ç¨‹åºå´©æºƒ

**æ¢å¤æ­¥éª¤**ï¼š

1. ç›´æ¥å†æ¬¡è¿è¡Œ `./start.sh`
2. ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹ checkpoint

**è¾“å‡ºç¤ºä¾‹**ï¼š

```
============================================================
RESUMING from previous checkpoint:
  - Processed chunks: XX
  - Total concepts: XXX
  - Total relationships: XXX
============================================================

[Step 2/6] Splitting texts into chunks...
Skipping XX already processed chunks
Remaining chunks to process: XXX

[Step 3/6] Extracting concepts with checkpoint support...
Extracting concepts:  0%|          | 0/462 [00:00<?, ?it/s]
```

### åœºæ™¯ 3ï¼šå®æ—¶ç›‘æ§ï¼ˆmonitor.shï¼‰

**è¿è¡Œ**ï¼š`./monitor.sh`

**è¾“å‡ºæ ¼å¼ç¤ºä¾‹**ï¼ˆæ•°å€¼ä»…ä¸ºç¤ºæ„ï¼‰ï¼š

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 ğŸ“Š çŸ¥è¯†å›¾è°±æ„å»ºè¿›åº¦ç›‘æ§
 æ›´æ–°æ—¶é—´: YYYY-MM-DD HH:MM:SS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ ç®¡é“è¿›ç¨‹: è¿è¡Œä¸­
  PID: XXXXX, CPU: XX.X%, å†…å­˜: X.X%

ğŸ“ Checkpoint è¿›åº¦:
  å·²å¤„ç†å—æ•°: XX
  æ€»æ¦‚å¿µæ•°: XXX
  æ€»å…³ç³»æ•°: XXX
  æœ€åæ›´æ–°: YYYY-MM-DD HH:MM:SS

â±ï¸  æ—¶é—´ä¼°ç®—:
  å·²è¿è¡Œ: XX åˆ†é’Ÿ
  å¹³å‡é€Ÿåº¦: XX ç§’/å—
  å‰©ä½™æ—¶é—´: çº¦ XXX åˆ†é’Ÿ

ğŸ“ è¾“å‡ºæ–‡ä»¶:
  âœ“ concepts.csv: XXK (XXX è¡Œ)
  âœ“ relationships.csv: XXK (XXX è¡Œ)
  âœ“ .progress.json: X.XK

ğŸ“‹ æœ€è¿‘æ—¥å¿—:
  YYYY-MM-DD HH:MM:SS - SafePipeline - INFO - Checkpoint: XX/XXX chunks processed
  YYYY-MM-DD HH:MM:SS - CheckpointManager - INFO - Saved results for chunk: paperX.pdf_XX

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 å¿«æ·æ“ä½œ: [r] åˆ·æ–°  [l] æŸ¥çœ‹å®Œæ•´æ—¥å¿—  [q] é€€å‡º
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## 4. å…³é”®æ—¥å¿—æ¨¡å¼ä¸è°ƒè¯•

### æ­£å¸¸è¿è¡Œæ—¥å¿—

```
2025-11-29 19:30:00 - SafePipeline - INFO - Starting Safe Enhanced Pipeline with Checkpoint Support
2025-11-29 19:30:01 - SafePipeline - INFO - [Step 1/6] Extracting text from PDFs...
2025-11-29 19:30:45 - SafePipeline - INFO - [Step 2/6] Splitting texts into chunks...
2025-11-29 19:30:46 - SafePipeline - INFO - Created 507 chunks
2025-11-29 19:30:46 - SafePipeline - INFO - [Step 3/6] Extracting concepts with checkpoint support...
2025-11-29 19:32:18 - CheckpointManager - INFO - Saved results for chunk: paper1.pdf_0
2025-11-29 19:33:50 - CheckpointManager - INFO - Saved results for chunk: paper1.pdf_1
...
2025-11-29 20:30:10 - SafePipeline - INFO - Checkpoint: 40/507 chunks processed
2025-11-29 20:30:10 - CheckpointManager - INFO - Checkpoint saved at chunk 40
```

### å¼‚å¸¸æ¨¡å¼ä¸å¤„ç†

**1. LLM è¶…æ—¶**

```
ERROR - Ollama API timeout after 3 attempts for chunk paper5.pdf_23
WARNING - Chunk paper5.pdf_23: LLM returned None, using empty results
INFO - Checkpoint: saved chunk paper5.pdf_23 with 0 concepts, 0 relationships
```

â†’ ç³»ç»Ÿç»§ç»­å¤„ç†ä¸‹ä¸€å—ï¼Œä¸ä¸­æ–­

**2. JSON è§£æå¤±è´¥**

```
ERROR - JSON è§£æå¤±è´¥ [paper8.pdf_15] - Qwen æœªæ­£ç¡®è¾“å‡º JSON
ERROR - åŸå§‹å“åº”ï¼ˆå‰500å­—ç¬¦ï¼‰: Here are the concepts: {...
```

â†’ è¿”å› Noneï¼Œè®°å½•æ—¥å¿—ï¼Œç»§ç»­

**3. Ctrl+C ç”¨æˆ·ä¸­æ–­**

```
WARNING - User interrupted (Ctrl+C)
WARNING - ============================================================
INFO - Checkpointå·²è‡ªåŠ¨ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»ä¸­æ–­å¤„ç»§ç»­
INFO - è¿›åº¦ä¿å­˜ä½ç½®: output/checkpoints
```

â†’ ä¼˜é›…é€€å‡ºï¼Œä¿å­˜è¿›åº¦

---

## 5. æ€§èƒ½æŒ‡æ ‡ä¸å®éªŒæ•°æ®

> **è¯´æ˜**ï¼šä»¥ä¸‹æ•°æ®æ¥æºäºé¡¹ç›® README.md ä¸­çš„æ€§èƒ½è¯„ä¼°ç« èŠ‚ä¸æŠ€æœ¯æŒ‘æˆ˜æ–‡æ¡£ï¼Œå…·ä½“æ•°å€¼ä¼šå› ç¡¬ä»¶ç¯å¢ƒã€æ–‡æœ¬å¤æ‚åº¦è€Œæœ‰æ‰€å·®å¼‚ã€‚

### å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆæ¥è‡ª READMEï¼‰

**æ¨¡å‹é€‰å‹**ï¼ˆå‚è§ README "æ ¸å¿ƒåˆ›æ–°ç‚¹ - LLM æ€§èƒ½ä¼˜åŒ–"ï¼‰ï¼š

- é¡¹ç›®æœ€ç»ˆé€‰æ‹© `qwen2.5-coder:7b` æ¨¡å‹
- å¤„ç†æ—¶é—´ï¼šç›¸æ¯”æ›´å¤§æ¨¡å‹æ˜¾è‘—ä¼˜åŒ–
- è¶…æ—¶ç‡ï¼šé€šè¿‡å‚æ•°ä¼˜åŒ–å’Œé‡è¯•æœºåˆ¶å¤§å¹…é™ä½

**åµŒå…¥æ¨¡å‹å¯¹æ¯”**ï¼ˆå‚è§ README "æ€§èƒ½æŒ‡æ ‡ - åµŒå…¥æ¨¡å‹å¯¹æ¯”"ï¼‰ï¼š

- ä½¿ç”¨ BGE-M3 æ›¿ä»£ MiniLM-L6
- ä¸­æ–‡è¯­ä¹‰ç›¸ä¼¼åº¦è¯†åˆ«èƒ½åŠ›æå‡
- ä¸“ä¸šæœ¯è¯­å¯¹é½æ•ˆæœå¢å¼º
- æ”¯æŒä¸­è‹±æ–‡æ··åˆåœºæ™¯çš„å®ä½“å¯¹é½

**Checkpoint æœºåˆ¶æ•ˆæœ**ï¼ˆå‚è§ README "æ ¸å¿ƒåˆ›æ–°ç‚¹ - Checkpoint æœºåˆ¶"ï¼‰ï¼š

- æœ€å¤§æŸå¤±æ—¶é—´ï¼šæ˜¾è‘—å‡å°‘å¤„ç†ä¸­æ–­æ—¶çš„æ•°æ®æŸå¤±
- æ•°æ®ä¸¢å¤±é£é™©ï¼šé€šè¿‡å¢é‡ä¿å­˜æœºåˆ¶å¤§å¹…é™ä½
- ç³»ç»Ÿå¯ç”¨æ€§ï¼šæ–­ç‚¹ç»­ä¼ åŠŸèƒ½æ˜¾è‘—æå‡ç¨³å®šæ€§

**å¤„ç†æ—¶é—´åˆ†å¸ƒ**ï¼ˆå‚è€ƒ README "æ€§èƒ½æŒ‡æ ‡"ï¼‰ï¼š

- PDF æå–ï¼šå ç”¨è¾ƒå°éƒ¨åˆ†æ—¶é—´
- æ–‡æœ¬åˆ†å—ï¼šå¤„ç†é€Ÿåº¦è¾ƒå¿«
- LLM æ¨ç†ï¼šä¸»è¦æ—¶é—´æ¶ˆè€—ï¼ˆå–å†³äºæ–‡æœ¬é‡å’Œæ¨¡å‹æ€§èƒ½ï¼‰
- è¯­ä¹‰å»é‡ï¼šç›¸å¯¹è¾ƒå¿«
- GraphRAGï¼ˆå¯é€‰ï¼‰ï¼šé¢å¤–å¤„ç†æ—¶é—´
- æ€»å¤„ç†æ—¶é—´ï¼šå–å†³äºæ–‡çŒ®æ•°é‡å’Œç¡¬ä»¶é…ç½®

> æ³¨ï¼šå®é™…è¿è¡Œæ—¶é—´å—ç¡¬ä»¶é…ç½®ï¼ˆCPU/GPUï¼‰ã€æ–‡çŒ®æ•°é‡ã€æ–‡æœ¬å¤æ‚åº¦ç­‰å› ç´ å½±å“ã€‚

---

## 6. æ‰©å±•é˜…è¯»ä¸é«˜çº§åŠŸèƒ½

### Agentic Workflowï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

- **æ–‡ä»¶**ï¼š`agentic_extractor.py`
- **å·¥ä½œæµ**ï¼šExtract Agent â†’ Critic Agent â†’ Refine Agent
- **å¯ç”¨æ–¹å¼**ï¼š
  ```yaml
  agentic:
    enable_llm_review: true
    review_confidence_range: [0.6, 0.85]
  ```
- **æ•ˆæœ**ï¼ˆå‚è€ƒ README "Agentic Workflow è¯´æ˜"ï¼‰ï¼š
  - é€šè¿‡ä¸‰é˜¶æ®µè´¨é‡å®¡æŸ¥æå‡æŠ½å–å‡†ç¡®æ€§
  - å‡å°‘é€»è¾‘é”™è¯¯å’Œå®ä½“è¯†åˆ«é—®é¢˜
  - æ³¨æ„ï¼šä¼šå¢åŠ å¤„ç†æ—¶é—´å’Œè®¡ç®—èµ„æºæ¶ˆè€—

### GraphRAG ç¤¾åŒºæ‘˜è¦ï¼ˆå¯é€‰ï¼Œæ”¯æŒå…¨å±€æŸ¥è¯¢ï¼‰

- **æ–‡ä»¶**ï¼š`graph_rag.py`
- **åŠŸèƒ½**ï¼š
  1. ç¤¾åŒºæ£€æµ‹ï¼šLouvain/Leiden ç®—æ³•
  2. LLM æ‘˜è¦ï¼šä¸ºæ¯ä¸ªç¤¾åŒºç”Ÿæˆä¸»é¢˜ä¸æè¿°
- **å¯ç”¨æ–¹å¼**ï¼š
  ```yaml
  agentic:
    enable_graph_rag: true
    community_algorithm: louvain
  ```
- **ä½¿ç”¨åœºæ™¯**ï¼šå›ç­”"é˜²æ²»ç­–ç•¥æ•´ä½“æ ¼å±€"ç­‰å®è§‚é—®é¢˜

### å¤šæ¨¡æ€æ‰©å±•ï¼ˆé¢„ç•™æ¥å£ï¼‰

- **æ–‡ä»¶**ï¼š`multimodal_extractor.py`
- **åŠŸèƒ½**ï¼šæå– PDF å›¾ç‰‡ â†’ Qwen2-VL ç”Ÿæˆæè¿° â†’ ä½œä¸ºæ–‡æœ¬å—æŠ½å–
- **å¯ç”¨æ–¹å¼**ï¼š
  ```yaml
  pdf:
    enable_image_captions: true
    caption_model: qwen2-vl
  ```

---

## 7. æ•…éšœæ’æŸ¥é€ŸæŸ¥

| é—®é¢˜              | æ—¥å¿—æ¨¡å¼                                    | è§£å†³æ–¹æ¡ˆ                            |
| ----------------- | ------------------------------------------- | ----------------------------------- |
| Ollama æœåŠ¡æœªå¯åŠ¨ | `ConnectionError: Cannot connect to Ollama` | è¿è¡Œ `ollama serve`                 |
| æ¨¡å‹æœªå®‰è£…        | `Model 'qwen2.5-coder:7b' not found`        | è¿è¡Œ `ollama pull qwen2.5-coder:7b` |
| LLM è¶…æ—¶          | `Ollama API timeout after 3 attempts`       | å¢åŠ  `llm.timeout` æˆ–æ¢æ›´å°æ¨¡å‹     |
| JSON è§£æå¤±è´¥     | `JSON è§£æå¤±è´¥ - Qwen æœªæ­£ç¡®è¾“å‡º JSON`      | æ£€æŸ¥ temperature è®¾ç½®ï¼Œé™è‡³ 0.05    |
| BGE-M3 åŠ è½½å¤±è´¥   | `Failed to initialize embeddings`           | æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼Œæ¨¡å‹éœ€~2GB            |
| Neo4j è¿æ¥å¤±è´¥    | `Unable to connect to Neo4j`                | æ£€æŸ¥æœåŠ¡çŠ¶æ€ä¸å¯†ç                   |

---

## 8. é«˜çº§åŠŸèƒ½æŠ€æœ¯å®ç°

### 8.1 Agentic Workflow ä¸‰é˜¶æ®µè´¨é‡æå‡

- **æ–‡ä»¶**ï¼š`agentic_extractor.py`
- **æ ¸å¿ƒç±»**ï¼š
  - `ExtractAgent`: åˆæ¬¡æŠ½å–æ¦‚å¿µå’Œå…³ç³»
  - `CriticAgent`: å®¡æŸ¥æŠ½å–è´¨é‡ï¼Œè¯†åˆ«é”™è¯¯å’Œé€»è¾‘è°¬è¯¯
  - `RefineAgent`: æ ¹æ®å®¡ç¨¿æ„è§ä¿®æ­£å’Œä¼˜åŒ–ç»“æœ
- **å·¥ä½œæµç¨‹**ï¼š
  1. Extract Agent ä½¿ç”¨ LLM åˆæ­¥æŠ½å–
  2. Critic Agent æ£€æŸ¥æœ¬ä½“ç¬¦åˆæ€§ã€é€»è¾‘ä¸€è‡´æ€§
  3. Refine Agent æ ¹æ®å®¡æŸ¥ç»“æœä¼˜åŒ–è¾“å‡º
- **æœ¬ä½“å®šä¹‰**ï¼š
  ```python
  self.ontology = {
      'valid_categories': ['pathogen', 'host', 'vector', 'symptom', 'treatment',
                         'environment', 'location', 'mechanism', 'compound'],
      'valid_relations': ['å¼•èµ·', 'å¯¼è‡´', 'è¯±å‘', 'ä¼ æ’­', 'æºå¸¦', 'æ‰©æ•£',
                        'æ„ŸæŸ“', 'å¯„ç”Ÿäº', 'ä¾µæŸ“', 'é˜²æ²»', 'æ§åˆ¶', 'æŠ‘åˆ¶',
                        'æ€ç­', 'å½±å“', 'ä¿ƒè¿›', 'åˆ†å¸ƒäº', 'å‘ç”Ÿäº'],
  }
  ```

### 8.2 GraphRAG ç¤¾åŒºæ£€æµ‹ä¸æ‘˜è¦

- **æ–‡ä»¶**ï¼š`graph_rag.py`
- **æ ¸å¿ƒç±»**ï¼š`CommunityDetector`
- **æ”¯æŒç®—æ³•**ï¼š
  - Louvain: å¿«é€Ÿæ¨¡å—åº¦ä¼˜åŒ–ç®—æ³•ï¼ˆéœ€è¦ NetworkXï¼‰
  - Leiden: æ”¹è¿›çš„ Louvain ç®—æ³•ï¼ˆéœ€è¦ igraphï¼‰
  - Label Propagation: æ ‡ç­¾ä¼ æ’­ç®—æ³•
  - Connected Components: è¿é€šåˆ†é‡ï¼ˆåŸºç¡€ç®—æ³•ï¼‰
- **ç®—æ³•æµç¨‹**ï¼š

  ```python
  def detect_communities(self, concepts_df: pd.DataFrame,
                        relationships_df: pd.DataFrame) -> Dict[int, List[str]]:
      """æ£€æµ‹çŸ¥è¯†å›¾è°±ä¸­çš„ç¤¾åŒº"""
      if concepts_df.empty or relationships_df.empty:
          return {}

      if self.algorithm == 'louvain':
          return self._detect_louvain(concepts_df, relationships_df)
      elif self.algorithm == 'leiden':
          return self._detect_leiden(concepts_df, relationships_df)
      elif self.algorithm == 'label_propagation':
          return self._detect_label_propagation(concepts_df, relationships_df)
      else:
          return self._detect_connected_components(concepts_df, relationships_df)
  ```

- **ç¤¾åŒºæ‘˜è¦ Prompt**ï¼š

  ```python
  summary_prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†å›¾è°±ç¤¾åŒºä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼š

  ç¤¾åŒºèŠ‚ç‚¹ï¼š{', '.join(nodes)}
  æ ¸å¿ƒæ¦‚å¿µï¼š{core_concepts}
  ä¸»è¦å…³ç³»ï¼š{key_relationships}

  è¯·ç”Ÿæˆ100å­—ä»¥å†…çš„ç¤¾åŒºä¸»é¢˜æè¿°ï¼Œçªå‡ºè¯¥ç¤¾åŒºçš„æ ¸å¿ƒç‰¹å¾ã€‚
  """
  ```

### 8.3 å¤šæ¨¡æ€æ‰©å±•ï¼ˆå›¾ç‰‡çŸ¥è¯†æŠ½å–ï¼‰

- **æ–‡ä»¶**ï¼š`multimodal_extractor.py`
- **æ ¸å¿ƒç±»**ï¼š`ImageExtractor`
- **åŠŸèƒ½**ï¼š
  - ä» PDF ä¸­æå–å›¾ç‰‡ï¼ˆæ˜¾å¾®é•œç…§ç‰‡ã€ç»Ÿè®¡å›¾è¡¨ã€åˆ†å¸ƒåœ°å›¾ï¼‰
  - ä½¿ç”¨ VLMï¼ˆVision-Language Modelsï¼‰ç”Ÿæˆå›¾ç‰‡æè¿°
  - ä»å›¾ç‰‡æè¿°ä¸­æŠ½å–çŸ¥è¯†ä¸‰å…ƒç»„
- **æ”¯æŒæ¨¡å‹**ï¼š
  - Qwen2-VL-7Bï¼ˆæœ¬åœ° Ollamaï¼‰
  - LLaVA-Nextï¼ˆæœ¬åœ° Ollamaï¼‰
  - transformersï¼ˆæœ¬åœ° GPUï¼‰
- **å›¾ç‰‡æå–æµç¨‹**ï¼š

  ```python
  def extract_images_from_pdf(self, pdf_path: str) -> List[Dict]:
      """ä» PDF ä¸­æå–å›¾ç‰‡"""
      doc = fitz.open(pdf_path)
      images_info = []

      for page_num in range(len(doc)):
          page = doc[page_num]
          image_list = page.get_images()

          for img_index, img in enumerate(image_list):
              xref = img[0]
              pix = fitz.Pixmap(doc, xref)

              # è´¨é‡è¿‡æ»¤
              if pix.width < self.min_width or pix.height < self.min_height:
                  pix = None
                  continue

              # ä¿å­˜å›¾ç‰‡
              img_path = f"{self.output_dir}/{pdf_name}_p{page_num}_{img_index}.png"
              pix.save(img_path)

              images_info.append({
                  'path': img_path,
                  'page': page_num,
                  'size': (pix.width, pix.height)
              })

              pix = None
      return images_info
  ```

              f"{self.ollama_host}/api/generate",
              json={
                  "model": "qwen2-vl",
                  "prompt": prompt,
                  "images": [image_data],
                  "stream": False
              },
              timeout=120
          )

          return response.json().get('response', '')

  ```

  ```

### 8.4 Neo4j é«˜çº§æŸ¥è¯¢ä¸å¯è§†åŒ–

- **Cypher æŸ¥è¯¢ä¼˜åŒ–**ï¼š

  ```cypher
  // æ‰¹é‡åˆ›å»ºèŠ‚ç‚¹ï¼ˆä½¿ç”¨UNWINDä¼˜åŒ–ï¼‰
  UNWIND $concepts AS concept
  MERGE (c:Concept {name: concept.entity})
  SET c.importance = concept.importance,
      c.category = concept.category,
      c.source = concept.chunk_id

  // æ‰¹é‡åˆ›å»ºå…³ç³»
  UNWIND $relationships AS rel
  MATCH (a:Concept {name: rel.node_1})
  MATCH (b:Concept {name: rel.node_2})
  MERGE (a)-[r:RELATED {type: rel.edge}]->(b)
  SET r.weight = rel.weight,
      r.source = rel.chunk_id

  // åˆ›å»ºå…¨æ–‡ç´¢å¼•
  CREATE FULLTEXT INDEX concept_fulltext FOR (c:Concept) ON EACH [c.name]
  ```

- **é«˜çº§åˆ†ææŸ¥è¯¢**ï¼š

  ```cypher
  // æŸ¥æ‰¾å…³é”®è·¯å¾„ï¼ˆç—…åŸä½“â†’å¯„ä¸»â†’ç—‡çŠ¶ï¼‰
  MATCH path = (p:Concept {category:'pathogen'})-[]->(h:Concept {category:'host'})-[]->(s:Concept {category:'symptom'})
  RETURN path, length(path) as path_length
  ORDER BY path_length
  LIMIT 10

  // ç¤¾åŒºå½±å“åŠ›åˆ†æ
  MATCH (c:Concept)-[r]-(n:Concept)
  WITH c, count(n) as connections, sum(r.weight) as total_weight
  WHERE c.importance >= 4
  RETURN c.name, c.category, connections, total_weight
  ORDER BY total_weight DESC

  // é˜²æ²»æ–¹æ³•æ•ˆæœè¯„ä¼°
  MATCH (p:Concept {category:'prevention'})-[r:PREVENTS]->(d:Concept {category:'pathogen'})
  RETURN p.name, r.weight, d.name
  ORDER BY r.weight DESC
  ```

---

**æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œå¦‚æœ‰ç–‘é—®è¯·å‚è€ƒ `docs/TECHNICAL_CHALLENGES.md` æˆ–é¡¹ç›® READMEã€‚**
